{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emulating realistically bad-for-cosmology SN Ia samples from PLAsTiCC data\n",
    "\n",
    "_Alex I. Malz (GCCL@RUB)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import gzip\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "\n",
    "rando = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import proclam\n",
    "from proclam.metrics.util import *\n",
    "from proclam.metrics.util import RateMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classes we care about\n",
    "\n",
    "| `true_target`=`type` | `code` |\n",
    "| -------------------- | ------ |\n",
    "| 90 | SNIa |\n",
    "| 67 | SNIa-91bg |\n",
    "| 52 | SNIax |\n",
    "| 42 | SNII |\n",
    "| 62 | SNIbc |\n",
    "| 95 | SLSN-I |\n",
    "| 88 | AGN |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maybe_sn_classes = {90: 'SNIa', \n",
    "                    67: 'SNIa-91bg', \n",
    "                    52: 'SNIax', \n",
    "                    42: 'SNII', \n",
    "                    62: 'SNIbc', \n",
    "                    95: 'SLSN-I', \n",
    "                    88: 'AGN'}\n",
    "maybe_sn_classes[15] = 'TDE'\n",
    "maybe_sn_classes[64] = 'KN'\n",
    "\n",
    "sel_class = 90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gather all available lightcurves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '/media/RESSPECT/data/PLAsTiCC/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "other than intermediate data products, work in `/media/RESSPECT/data/PLAsTiCC/for_metrics/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lcs = pd.read_csv(datapath+'PLAsTiCC_zenodo/plasticc_test_metadata.csv')\n",
    "all_lcs = all_lcs.rename(columns={\"object_id\": \"id\", \"true_z\": \"redshift\", \"true_target\": \"code\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_info0 = {}\n",
    "for field in ['ddf', 'wfd']:\n",
    "    field_info0[field] = {}\n",
    "    field_info0[field]['true_cat'] = all_lcs.loc[all_lcs['ddf_bool'] == (field == 'ddf')][['id', 'redshift', 'code']]\n",
    "    field_info0[field]['n_tot_cat'] = len(field_info0[field]['true_cat'])\n",
    "    field_info0[field]['n_each_cat'] = dict(field_info0[field]['true_cat'].groupby('code').count()['id'])\n",
    "    print(field_info0[field]['n_each_cat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_class_pos = 3000\n",
    "\n",
    "def gen_nums(field_info, field, n_class_pos=3000, sel_class=90, classmap=maybe_sn_classes):\n",
    "\n",
    "    field_info[field]['pos'] = field_info[field]['n_each_cat'][sel_class]\n",
    "    field_info[field]['glob'] = sum([field_info[field]['n_each_cat'][classid] for classid in classmap])\n",
    "    field_info[field]['neg'] = field_info[field]['glob'] - field_info[field]['pos']\n",
    "    \n",
    "    n_class_glob = n_class_pos * field_info[field]['glob'] / field_info[field]['pos']\n",
    "    field_info[field]['class_all'] = {classid: int(round(n_class_glob * field_info[field]['n_each_cat'][classid] / field_info[field]['glob'])) for classid in classmap}\n",
    "    field_info[field]['class_glob'] = sum(field_info[field]['class_all'].values())\n",
    "    print(field_info[field]['class_all'])\n",
    "    \n",
    "    return field_info[field].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_info1, field_info3, field_info6 = {}, {}, {}\n",
    "field_defs = [int(1e3), int(3e3), int(6e3)]\n",
    "for field in ['ddf', 'wfd']:\n",
    "    field_info1[field] = gen_nums(field_info0, field, n_class_pos=field_defs[0])\n",
    "    field_info3[field] = gen_nums(field_info0, field, n_class_pos=field_defs[1])\n",
    "    field_info6[field] = gen_nums(field_info0, field, n_class_pos=field_defs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: sample from those that survived but use ratios from overall, SALT2 failures are class dependent/independent, cadence dependent/independent, can't disentangle (because this freebie classification criterion will not always be true with SALT3 etc., the detection ratios will depend on cadence), from our perspective contamination can't be worse, this is worst case for single contaminant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_fit_filter_ddf = pd.read_csv(datapath+'for_metrics/ddf/samples/all_objs_survived_SALT2_DDF.csv')\n",
    "lc_fit_filter_wfd = pd.read_csv(datapath+'for_metrics/wfd/samples/all_objs_survived_SALT2_WFD.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_maybe_sn_ddf = pd.merge(lc_fit_filter_ddf['id'], all_lcs[['id', 'redshift', 'code']], on=['id'])\n",
    "all_maybe_sn_wfd = pd.merge(lc_fit_filter_wfd['id'], all_lcs[['id', 'redshift', 'code']], on=['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: plot \"confusion matrix\" based on SALT2 fit success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_maybe_sn = {'ddf': all_maybe_sn_ddf, 'wfd': all_maybe_sn_wfd}\n",
    "for info in [field_info1, field_info3, field_info6]:\n",
    "    for field in ['ddf', 'wfd']:\n",
    "        count_surv = dict(all_maybe_sn[field].groupby('code').count()['id'])\n",
    "        info[field]['class_avail'] = {classid: 0 for classid in maybe_sn_classes.keys()}\n",
    "        for classid in maybe_sn_classes.keys():\n",
    "            if classid in count_surv.keys():\n",
    "                info[field]['class_avail'][classid] = count_surv[classid]\n",
    "        print(info[field]['class_avail'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~a priori all samples will be 3000 \"classified SN Ia\"~~ TODO: try with `n_class_pos`=3e4, 3e5 for the three dummy cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## subsample the classes to make new samples\n",
    "\n",
    "TODO: maybe investigate redshift distribution of sample classified as Ia?\n",
    "\n",
    "~~TODO: make a table of all test cases~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath = '/media/RESSPECT/data/PLAsTiCC/for_metrics/'\n",
    "savepaths = {}\n",
    "for field in ['ddf', 'wfd']:\n",
    "    savepaths[field] = savepath + field + '/samples/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get sample ids matching a confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the true/false positive/negative rates along the way to making the subsamples, we need a notion of negatives that would never end up in the cosmology sample.\n",
    "Let's use the DDF type ratios to figure out how many objects will be classified as negative for our samples of 3000 positive classifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`n_class_all` contains the number of objects in the true population, and the confusion matrix tells us how many will end up being classified as positive or negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save outputs as `id,redshift,type,code,orig_sample=test,queryable=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample_cat(cm, cm_indices, ntot, cat, surv=None, rando=rando,\n",
    "                  pos_key=90, where_to_save=None, save_neg=True, force=False):\n",
    "    if surv is None:\n",
    "        surv = cat.copy()\n",
    "    #     print(ntot[pos_key])\n",
    "    # normalize to number in true class\n",
    "#     print(cm)\n",
    "    pcm = (cm.T / np.sum(cm, axis=1)).T\n",
    "#     pcm = cm / np.sum(cm, axis=0)\n",
    "#     print(np.sum(pcm))\n",
    "    # want row corresponding to predicted class\n",
    "    pos_row = pcm[cm_indices[pos_key]] * ntot[pos_key]\n",
    "    pos_row = [int(round(i)) for i in pos_row]\n",
    "    pos_ids, neg_ids = pd.DataFrame(columns=cat.columns), pd.DataFrame(columns=cat.columns)\n",
    "    err = 0\n",
    "    for typeid in cm_indices.keys():\n",
    "        n_crit = len(surv[surv['code'] == typeid])\n",
    "#         print((typeid, pos_row[cm_indices[typeid]], n_pos))\n",
    "#         print(('debug', surv[surv['code'] == typeid]))\n",
    "        \n",
    "        n_pos = pos_row[cm_indices[typeid]]\n",
    "        if n_pos > ntot[typeid]:\n",
    "            print(f'cannot draw {n_pos} {typeid} from existing {ntot[typeid]} in {pos_row}')\n",
    "            n_pos = ntot[typeid]\n",
    "            err = 1\n",
    "        if n_pos > n_crit:\n",
    "            print(f'cannot draw {n_pos} {typeid} from surviving {n_crit} in {pos_row}')\n",
    "            n_pos = n_crit\n",
    "            err = 1\n",
    "            \n",
    "        n_neg = ntot[typeid] - n_pos\n",
    "        if n_neg > ntot[typeid]:\n",
    "            print(f'cannot draw {n_neg} {typeid} from negative {ntot[typeid]}')\n",
    "            n_neg = ntot[typeid]\n",
    "            err = 1\n",
    "            \n",
    "        print((n_pos, n_neg))\n",
    "\n",
    "#         print(len(cat[cat['code'] == typeid]))\n",
    "        pos = surv[surv['code'] == typeid].sample(n=n_pos, random_state=rando)\n",
    "        neg = cat[cat['code'] == typeid].sample(n=n_neg, random_state=rando, replace=True)\n",
    "#         pos = matches[:n_pos]\n",
    "#         neg = matches[n_pos:]\n",
    "        if len(pos) > 0:\n",
    "            pos_ids = pos_ids.append(pos)\n",
    "        if len(neg) > 0:\n",
    "            neg_ids = neg_ids.append(neg)\n",
    "    # special checks for edge cases on rounding errors! only matters when more than 2 classes present\n",
    "    n_err = int(round(np.sum(pos_row)) - len(pos_ids))\n",
    "    print('err=' + str(n_err))\n",
    "    if n_err > 0:\n",
    "        bonus = surv[(surv['code'] == pos_key) & (~surv.id.isin(pos.id))].sample(n=n_err, random_state=rando)\n",
    "#         print(bonus)\n",
    "        pos_ids = pos_ids.append(bonus)\n",
    "#         print((len(pos_ids),  int(round(np.sum(pos_row)))))\n",
    "#         print(pos_ids[-1 * err:])\n",
    "#         neg_ids = neg_ids[err:]\n",
    "#     print((len(pos_ids),  int(round(np.sum(pos_row)))))\n",
    "    if n_err < 0:\n",
    "        drop_indices = np.random.choice(pos_ids[pos_ids['code'] == pos_key].index, -1 * err, replace=False)\n",
    "        pos_ids = pos_ids.drop(drop_indices)\n",
    "    assert(len(pos_ids) == int(round(np.sum(pos_row))))\n",
    "#     assert(len(pos_ids) + len(neg_ids) == np.sum(np.array([ntot[typeid] for typeid in cm_indices.keys()])))\n",
    "    if where_to_save:\n",
    "#         if err == 1:\n",
    "#             where_to_save += 'fail' + str(len(pos_ids))\n",
    "        pos_ids['orig_sample'] = 'test'\n",
    "        pos_ids['queryable'] = True\n",
    "        pos_ids['type'] = None\n",
    "        pos_ids[['id','redshift','type','code','orig_sample','queryable']].to_csv(where_to_save+'.csv', index=False)\n",
    "    return pos_ids, neg_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### realistic classifier\n",
    "\n",
    "start from fiducial contamination rates from ~~a real (awful) confusion matrix at `/media/RESSPECT/data/PLAsTiCC/for_metrics/confusion_matrices`~~ Avocado\n",
    "\n",
    "~~These were just the test set lightcurves for classes (67, 88, 42(minus 7?), 90(minus 11?), 52, 62, 64, 95, 15) from ddf-only~~\n",
    "\n",
    "~~figure out classes in confusion matrix by comparing number of ddf test set-only lightcurves~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid_cm = np.loadtxt('confusion_matrix_no_galactic_kb.txt')[:, :-6]\n",
    "# with open(savepath+'confusion_matrices/confusion_matrix.npy', 'rb') as confmat:\n",
    "#     fid_cm = np.load(confmat)\n",
    "plt.imshow(fid_cm)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cm_classes = [67, 88, 42, 90, 52, 62, 64, 95, 15]\n",
    "cm_classes = [90, 67, 52, 42, 62, 95, 15, 64, 88]\n",
    "cm_indices = {}\n",
    "for classid in maybe_sn_classes.keys():\n",
    "    cm_indices[classid] = cm_classes.index(classid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fiducial sample corresponding to input confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, field_info in enumerate([field_info1, field_info3, field_info6]):\n",
    "    for field in ['ddf', 'wfd']:\n",
    "#         print(field_info[field]['class_all'])\n",
    "        fiducial = subsample_cat(fid_cm, cm_indices, field_info[field]['class_all'], field_info[field]['true_cat'], surv=all_maybe_sn[field],\n",
    "                             where_to_save=savepaths[field]+'fiducial'+str(field_defs[i]))\n",
    "        if fiducial is not None:\n",
    "            print(len(fiducial[0]))\n",
    "#     print(len(fiducial[0][fiducial[0]['code'] == sel_class])+len(fiducial[1][fiducial[1]['code'] == sel_class]))\n",
    "#     print((fiducial[0]['code'].value_counts(), fiducial[1]['code'].value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 100% SNIa sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "perf_cm = np.identity(len(cm_indices.keys()))\n",
    "for i, field_info in enumerate([field_info1, field_info3, field_info6]):\n",
    "    for field in ['ddf', 'wfd']:\n",
    "        perfect = subsample_cat(perf_cm, cm_indices, field_info[field]['class_all'], field_info[field]['true_cat'], surv=all_maybe_sn[field],\n",
    "                            where_to_save=savepaths[field]+'perfect'+str(field_defs[i]))#, rando=999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### random/guessing/uncertain classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, field_info in enumerate([field_info1, field_info3, field_info6]):\n",
    "    for field in ['ddf', 'wfd']:\n",
    "        rand_cm = np.ones((len(cm_indices.keys()), len(cm_indices.keys()))) / len(cm_indices.keys())**2\n",
    "        rand_cm *= np.array([field_info[field]['n_each_cat'][key] for (key, val) in \n",
    "                         sorted(cm_indices.items(), key=lambda x: x[1])])\n",
    "        rand_cm = rand_cm.T / np.sum(rand_cm)\n",
    "        rand_cm *= np.array([field_info[field]['class_all'][key] for (key, val) in \n",
    "                         sorted(cm_indices.items(), key=lambda x: x[1])])\n",
    "        rand_cm = rand_cm.T / np.sum(rand_cm)\n",
    "        guesser = subsample_cat(rand_cm, cm_indices, field_info[field]['class_all'], field_info[field]['true_cat'], surv=all_maybe_sn[field],\n",
    "                            where_to_save=savepaths[field]+'random'+str(field_defs[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate classification metrics on the subsamples\n",
    "\n",
    "better to do it along the way to making the subsamples, especially important for non-extreme subsamples filling the space of classification metric values\n",
    "\n",
    "first get rates using `proclam` functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_to_rate(pos_ids, neg_ids, pos_key=sel_class):\n",
    "    pos_ids['classed'] = True\n",
    "    neg_ids['classed'] = False\n",
    "    whole_samp = pd.concat((pos_ids, neg_ids))\n",
    "    whole_samp['truth'] = None\n",
    "    whole_samp['truth'][whole_samp['code'] != pos_key] = False\n",
    "    whole_samp['truth'][whole_samp['code'] == pos_key] = True\n",
    "    bin_cm = det_to_cm(whole_samp['classed'].to_numpy(), whole_samp['truth'].to_numpy())\n",
    "    rawrate = cm_to_rate(bin_cm)._asdict()\n",
    "    rel_to_sel = {key: rawrate[key][0] for key in rawrate.keys()}\n",
    "    rate = proclam.util.RateMatrix(**rel_to_sel)\n",
    "    return rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate all the metrics!\n",
    "\n",
    "TODO: put some version of this into `proclam` at some point!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class det_mets(RateMatrix):\n",
    "    \"binary classification metrics\"\n",
    "    def __init__(self, **rates):\n",
    "        \"\"\"\n",
    "        Call like `thing = det_mets(**rates._asdict())`\n",
    "        \"\"\"\n",
    "#         self.rates = rates#.asdict()\n",
    "        self._get_tots()\n",
    "        self._from_rates()\n",
    "        self._sn_mets()\n",
    "        self._translate()\n",
    "    def _get_tots(self):\n",
    "        self.CP = self.TP + self.FN\n",
    "        self.CN = self.TN + self.FP\n",
    "        self.T = self.TP + self.TN\n",
    "        self.F = self.FP + self.FN\n",
    "        self.P = self.TP + self.FP\n",
    "        self.N = self.TN + self.FN\n",
    "    def _from_rates(self):\n",
    "        self.PPV = self.TP / (self.TP + self.FP)\n",
    "        self.NPV = self.TN / (self.TN + self.FN)\n",
    "        self.PT = (np.sqrt(self.TPR * (1. - self.TNR)) + self.TNR - 1.) / (self.TPR + self.TNR - 1.)\n",
    "        self.TS = self.TP / (self.TP + self.FN + self.FP)\n",
    "        self._derived()\n",
    "    def _derived(self):\n",
    "        self.ACC = (self.TP + self.TN) / (self.CP + self.CN)\n",
    "        self.BA = (self.TPR + self.TNR) / 2,\n",
    "        self.F1S = 2. * self.PPV * self.TPR / (self.PPV + self.TPR)\n",
    "        self.MCC = (self.TP * self.TN - self.FP * self.FN) / (np.sqrt(self.P * self.CP * self.CN * self.N))\n",
    "        self.FM = np.sqrt(self.PPV * self.TPR)\n",
    "        self.BM = self.TPR + self.TNR - 1.\n",
    "        self.MK = self.PPV + self.NPV - 1.\n",
    "    def _translate(self):\n",
    "        self.positive = self.CP\n",
    "        self.negative = self.CN\n",
    "        self.sensitivity = self.TPR\n",
    "        self.recall = self.TPR\n",
    "        self.specificity = self.TNR\n",
    "        self.selectivity = self.TNR\n",
    "        self.precision = self.PPV\n",
    "        self.FDR = 1. - self.PPV\n",
    "        self.FOR = 1. - self.NPV\n",
    "        self.CSI = self.TS\n",
    "        self.accuracy = self.ACC\n",
    "        self.f1_score = self.F1S\n",
    "        self.informedness = self.BM\n",
    "        self.deltaP = self.MK\n",
    "    def _sn_mets(self):\n",
    "        self.get_efficiency()\n",
    "        self.get_purity()\n",
    "    def get_efficiency(self):\n",
    "        self.efficiency = self.TP / self.CP\n",
    "        return self.efficiency\n",
    "    def get_purity(self):\n",
    "        self.purity = self.TP / self.P\n",
    "        return self.purity\n",
    "    def get_fom(self, penalty):\n",
    "        self.pseudo_purity = self.TP / (self.TP + penalty * self.FP)\n",
    "        return self.pseudo_purity * self.efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "demonstrate on the archetypes (broken at the moment due to flagging when not enough of contaminant in \"closed universe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for field in ['ddf', 'wfd']:\n",
    "#     print(field)\n",
    "#     for cm in [cm_perfect, cm_almost, cm_noisy, cm_uncertain]:\n",
    "#         pos, neg = subsample_cat(cm, cm_indices, field_info[field]['class_all'], field_info[field]['true_cat'])\n",
    "#         rates = cat_to_rate(pos, neg)\n",
    "#         mets = det_mets(**rates._asdict())\n",
    "#         print(f'purity:{mets.purity}, efficiency:{mets.efficiency}, fom1:{mets.get_fom(1.)}, fom3:{mets.get_fom(3.)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## next, make samples corresponding to metric values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "original plan was to have these samples:\n",
    "- 100% Ia\n",
    "- Ia/Ibc\n",
    "- - 50/50\n",
    "- - 75/25\n",
    "- - 90/10\n",
    "- - 95/5\n",
    "- - 98/2\n",
    "- Ia/II\n",
    "- Ia/91bg\n",
    "- Ia/Iax\n",
    "- AGN\n",
    "- TDE \n",
    "- KN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ia_percents = np.array([50, 68, 75, 90, 95, 98, 99])\n",
    "mix_percents = 100 - ia_percents\n",
    "contaminants = maybe_sn_classes.copy()\n",
    "contaminants.pop(sel_class)\n",
    "metpaths = {field: savepath+field+'/metrics/' for field in ['ddf', 'wfd']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assume symmetry in 2-class mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# binary_ia_mets = {}\n",
    "cols = ['field', 'contaminant', 'percent', 'inloc', 'name', 'f1', 'purity', 'efficiency', 'fom1', 'fom3'] + [key for key in RateMatrix.__dict__.keys() if key[0] != '_']\n",
    "directory = pd.DataFrame(columns=cols)\n",
    "field_info = field_info3\n",
    "n_class_pos = field_defs[1]\n",
    "for field in ['ddf', 'wfd']:\n",
    "    for key, val in contaminants.items():\n",
    "        subset_indices = {sel_class: 0, key: 1}\n",
    "        crit = math.floor(field_info[field]['class_all'][key] / n_class_pos * 100)\n",
    "        print(f'cannot have more than {crit} percent of {val}')\n",
    "        for i, perc in enumerate(mix_percents):\n",
    "            print(f'seeking {perc * n_class_pos / 100} of '+str(field_info[field]['class_all'][key]))\n",
    "            if perc > crit:\n",
    "#                 perc = 100 - math.floor(field_info[field]['class_all'][key] / n_class_pos * 100.)\n",
    "                perc = crit\n",
    "#             else:\n",
    "#                 sampfn = savepaths[field]+str(ia_percents[i])+str(maybe_sn_classes[sel_class])+str(perc)+val\n",
    "#                 cm = np.array([[ia_percents[i], perc], [perc, ia_percents[i]]])\n",
    "            fn = str(100 - perc)+str(maybe_sn_classes[sel_class])+str(perc)+val\n",
    "            sampfn = savepaths[field]+fn\n",
    "            cm = np.array([[100 - perc, perc], [perc, 100 - perc]])\n",
    "            pos, neg = subsample_cat(cm, subset_indices, field_info[field]['class_all'], field_info[field]['true_cat'], surv=all_maybe_sn[field], \n",
    "                                     where_to_save=sampfn)#where_to_save=None)#\n",
    "            rates = cat_to_rate(pos, neg)\n",
    "            mets = det_mets(**rates._asdict())\n",
    "            \n",
    "            metfn = metpaths[field]+fn#f'{100-perc}_{sel_class}_{perc}_{key}'\n",
    "            prelim = [mets.f1_score, mets.purity, mets.efficiency, mets.get_fom(1.), mets.get_fom(3.)]\n",
    "            print(f'{metfn} = F1:{mets.f1_score}, purity:{mets.purity}, efficiency:{mets.efficiency}, fom1:{mets.get_fom(1.)}, fom3:{mets.get_fom(3.)}')\n",
    "            with open(metfn+'.pkl', 'wb') as metfile:\n",
    "                pkl.dump(rates._asdict(), metfile)\n",
    "                print('success for '+metfn)\n",
    "            thisloc = len(directory)\n",
    "            directory.loc[thisloc] = [field, val, perc, metfn, fn] + prelim + [rates._asdict()[key] for key in rates._asdict().keys()]\n",
    "directory = directory.drop_duplicates(ignore_index=True)\n",
    "directory.to_csv(savepath+'directory.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEXT: plot redshift distribution of all samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: consider nontrivial mixes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create new confusion matrices to tune output sample rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "consider `proclam` classifier archetypes for inspiration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M_classes = len(cm_indices)\n",
    "\n",
    "# # 'Uncertain' --> 'Random'\n",
    "# cm_uncertain = np.ones((M_classes, M_classes))\n",
    "\n",
    "# # 'Perfect'\n",
    "# cm_perfect = np.eye(M_classes) + 1.e-8\n",
    "\n",
    "# # 'Almost'\n",
    "# cm_almost = np.eye(M_classes) + 0.1 * np.ones((M_classes, M_classes))\n",
    "\n",
    "# # 'Noisy'\n",
    "# cm_noisy = np.eye(M_classes) + 0.5 * np.ones((M_classes, M_classes))\n",
    "\n",
    "# # # 'Tunnel Vision'\n",
    "# # cm = np.ones((M_classes, M_classes))\n",
    "# # cm = cm * np.asarray(0.1)[np.newaxis, np.newaxis]\n",
    "# # cm[:, chosen] = cm[:, chosen] / M_classes\n",
    "# # cm[chosen][chosen] += M_classes\n",
    "\n",
    "# # # 'Cruise Control'\n",
    "# # cm = np.eye(M_classes) + 1.e-8\n",
    "# # cm[:] = cm[chosen]\n",
    "\n",
    "# # # 'Subsuming'\n",
    "# # cm = np.eye(M_classes) + 0.1 * np.ones((M_classes, M_classes))\n",
    "# # cm[chosen] = cm[chosen-1]\n",
    "\n",
    "# # # 'Mutually Subsuming'\n",
    "# # cm = np.eye(M_classes) + 0.1 * np.ones((M_classes, M_classes))\n",
    "# # cm[chosen][chosen+1] = cm[chosen][chosen]\n",
    "# # cm[chosen+1][chosen] = cm[chosen+1][chosen+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 'Mutually Subsuming'\n",
    "# target = cm_indices[sel_class]\n",
    "# contaminant = cm_indices[62]\n",
    "# half_ibc_cm = np.eye(M_classes) + 0.1 * np.ones((M_classes, M_classes))\n",
    "# half_ibc_cm[target][contaminant] = half_ibc_cm[target][target]\n",
    "# half_ibc_cm[contaminant][target] = half_ibc_cm[contaminant][contaminant]\n",
    "# # plt.imshow(half_ibc_cm)\n",
    "# # plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make new confusion matrices as mixtures of existing ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mix_arr(inarrs, weights=None):\n",
    "#     narrs = len(inarrs)\n",
    "#     if weights is None:\n",
    "#         weights = np.ones_like((1, narrs))\n",
    "#     arrs = inarrs / np.sum(np.sum(inarrs, axis=-1), axis=-1)[:, np.newaxis, np.newaxis]\n",
    "#     normwts = weights / np.sum(weights)\n",
    "#     outarr = np.sum(arrs * normwts[:, np.newaxis, np.newaxis], axis=0)\n",
    "#     return outarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_cm = mix_arr(np.array([cm_uncertain, cm_perfect]))\n",
    "# plt.imshow(new_cm)\n",
    "# plt.colorbar()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
