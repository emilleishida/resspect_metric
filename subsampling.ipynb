{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emulating realistically bad-for-cosmology SN Ia samples from PLAsTiCC data\n",
    "\n",
    "_Alex I. Malz (GCCL@RUB)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "\n",
    "rando = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import proclam\n",
    "from proclam.metrics.util import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classes we care about\n",
    "\n",
    "| `true_target`=`type` | `code` |\n",
    "| -------------------- | ------ |\n",
    "| 90 | SNIa |\n",
    "| 67 | SNIa-91bg |\n",
    "| 52 | SNIax |\n",
    "| 42 | SNII |\n",
    "| 62 | SNIbc |\n",
    "| 95 | SLSN-I |\n",
    "| 88 | AGN |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maybe_sn_classes = {90: 'SNIa', \n",
    "                    67: 'SNIa-91bg', \n",
    "                    52: 'SNIax', \n",
    "                    42: 'SNII', \n",
    "                    62: 'SNIbc', \n",
    "                    95: 'SLSN-I', \n",
    "                    88: 'AGN'}\n",
    "maybe_sn_classes[64] = 'KN'\n",
    "maybe_sn_classes[15] = 'TDE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gather all available lightcurves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '/media/RESSPECT/data/PLAsTiCC/PLAsTiCC_zenodo/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "other than intermediate data products, work in `/media/RESSPECT/data/PLAsTiCC/for_metrics/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_maybe_sn = pd.read_csv('/media/RESSPECT/data/PLAsTiCC/PLAsTiCC_zenodo/plasticc_test_metadata.csv')\n",
    "# print(len(all_maybe_sn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_maybe_sn = all_maybe_sn.rename(columns={\"object_id\": \"id\", \"true_z\": \"redshift\", \"true_target\": \"code\"})\n",
    "print(all_maybe_sn.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_maybe_sn['orig_sample'] = 'test'\n",
    "# all_maybe_sn['queryable'] = True\n",
    "# all_maybe_sn['type'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DDF now, WFD later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_ddf = all_maybe_sn.loc[all_maybe_sn['ddf_bool'] == 1][['id', 'redshift', 'code']]\n",
    "# true_wfd = all_maybe_sn.loc[all_maybe_sn['ddf_bool'] == 0][['id', 'redshift', 'code']]\n",
    "n_ddf_tot = len(true_ddf)\n",
    "n_ddf_all = dict(true_ddf.groupby('code').count()['id'])\n",
    "# ddf_rats = dict(true_ddf.groupby('code').count()['id'] / n_ddf_tot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a priori all samples will be 3000 \"classified SN Ia\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## subsample the classes to make new samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "original plan was to have these samples:\n",
    "- 100% Ia\n",
    "- Ia/Ibc\n",
    "- - 50/50\n",
    "- - 75/25\n",
    "- - 90/10\n",
    "- - 95/5\n",
    "- - 98/2\n",
    "- Ia/II\n",
    "- Ia/91bg\n",
    "- Ia/Iax\n",
    "- AGN\n",
    "- TDE \n",
    "- KN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_class_pos = 3000\n",
    "sel_class = 90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### as in proclam, based on a confusion matrix\n",
    "\n",
    "start from fiducial contamination rates from a real (awful) confusion matrix at `/media/RESSPECT/data/PLAsTiCC/for_metrics/confusion_matrices`\n",
    "\n",
    "figure out classes in confusion matrix by comparing number of ddf test set-only lightcurves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath = '/media/RESSPECT/data/PLAsTiCC/for_metrics/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(savepath+'confusion_matrices/confusion_matrix.npy', 'rb') as confmat:\n",
    "    cm = np.load(confmat)\n",
    "n_cm = np.sum(cm)\n",
    "plt.imshow(np.log(cm), cmap='viridis_r')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`axis=0` is predicted classes, `axis=1` is true classes\n",
    "\n",
    "These were just the test set lightcurves for classes (67, 88, 42(minus 7?), 90(minus 11?), 52, 62, 64, 95, 15) from ddf-only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_classes = [67, 88, 42, 90, 52, 62, 64, 95, 15]\n",
    "cm_indices = {}\n",
    "# cm_rat = {}\n",
    "for classid in maybe_sn_classes.keys():\n",
    "    cm_indices[classid] = cm_classes.index(classid)\n",
    "#     cm_rat[classid] = sum(cm[cm_indices[classid]]) / n_cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get sample ids matching a confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save outputs as `id,redshift,type,code,orig_sample=test,queryable=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample_cm(cm, cm_indices, samp_key, cat=all_maybe_sn, where_to_save=None, rando=rando):#, ntot=n_samp):\n",
    "    cm_row = cm.T[cm_indices[samp_key]]\n",
    "    out_ids = pd.DataFrame(columns=cat.columns)\n",
    "    for typeid in cm_indices.keys():\n",
    "#         if not ntot:\n",
    "        n_to_sample = int(cm_row[cm_indices[typeid]])\n",
    "        matches = cat[cat['code'] == typeid].sample(n=n_to_sample, random_state=rando)\n",
    "        if len(matches) > 0:\n",
    "            out_ids = out_ids.append(matches)\n",
    "    out_ids['orig_sample'] = 'test'\n",
    "    out_ids['queryable'] = True\n",
    "    out_ids['type'] = None\n",
    "    if where_to_save:\n",
    "        out_ids[['id','redshift','type','code','orig_sample','queryable']].to_csv(where_to_save+'.csv', index=False)\n",
    "    return(out_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 100% SNIa sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_perfect = np.diag(cm) * np.identity(len(cm_indices.keys()))\n",
    "perfect = subsample_cm(cm_perfect, cm_indices, sel_class)#, where_to_save=savepath+'perfect_samp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"realistic\" sample\n",
    "\n",
    "fiducial sample corresponding to input confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiducial = subsample_cm(cm, cm_indices, sel_class)#, where_to_save=savepath+'fiducial_samp')\n",
    "# print(len(fiducial))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate classification metrics on the subsamples\n",
    "\n",
    "do it along the way to making the subsamples, especially important for non-extreme subsamples filling the space of classification metric values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the true/false positive/negative rates along the way to making the subsamples, we need a notion of negatives that would never end up in the cosmology sample.\n",
    "Let's use the DDF type ratios to figure out how many objects will be classified as negative for our samples of 3000 positive classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ddf_pos = n_ddf_all[sel_class]\n",
    "n_ddf_glob = sum([n_ddf_all[classid] for classid in maybe_sn_classes])\n",
    "n_ddf_neg = n_ddf_glob - n_ddf_pos\n",
    "\n",
    "n_class_glob = n_class_pos * n_ddf_glob / n_ddf_pos\n",
    "n_class_all = {classid: int(round(n_class_glob * n_ddf_all[classid] / n_ddf_glob)) for classid in maybe_sn_classes}\n",
    "n_class_glob = sum(n_class_all.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`n_class_all` contains the number of objects in the true population, and the confusion matrix tells us how many will end up being classified as positive or negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample_cat(cm, cm_indices, pos_key=sel_class, \n",
    "                  ntot=n_class_all, cat=true_ddf, \n",
    "                  where_to_save=None, rando=rando):\n",
    "    # normalize to number in true class\n",
    "    pcm = cm / np.sum(cm, axis=1)\n",
    "    # want row corresponding to predicted class\n",
    "    pos_row = pcm[cm_indices[pos_key]] * ntot[pos_key]\n",
    "    pos_ids, neg_ids = pd.DataFrame(columns=cat.columns), pd.DataFrame(columns=cat.columns)\n",
    "#     bin_cm = np.zeros((2, 2))\n",
    "    for typeid in cm_indices.keys():\n",
    "        n_pos = int(round(pos_row[cm_indices[typeid]]))\n",
    "        n_neg = ntot[typeid] - n_pos\n",
    "#         if typeid == pos_key:\n",
    "#             print((cm[cm_indices[typeid]], pos_row[cm_indices[typeid]], ntot[typeid], n_pos))\n",
    "#             bin_cm[0][0] += n_pos\n",
    "#             bin_cm[1][0] += n_neg\n",
    "#         else:\n",
    "#             bin_cm[0][1] += n_pos\n",
    "#             bin_cm[1][1] += n_neg\n",
    "        matches = cat[cat['code'] == typeid].sample(n=ntot[typeid], random_state=rando)\n",
    "        pos = matches[:n_pos]\n",
    "        neg = matches[n_pos:]\n",
    "        if len(pos) > 0:\n",
    "            pos_ids = pos_ids.append(pos)\n",
    "        if len(neg) > 0:\n",
    "            neg_ids = neg_ids.append(neg)\n",
    "    if where_to_save:\n",
    "        pos_ids['orig_sample'] = 'test'\n",
    "        pos_ids['queryable'] = True\n",
    "        pos_ids['type'] = None\n",
    "        pos_ids[['id','redshift','type','code','orig_sample','queryable']].to_csv(where_to_save+'.csv', index=False)\n",
    "    return pos_ids, neg_ids\n",
    "\n",
    "def cat_to_rate(pos_ids, neg_ids, pos_key=sel_class):\n",
    "    pos_ids['classed'] = True\n",
    "    neg_ids['classed'] = False\n",
    "    whole_samp = pd.concat((pos_ids, neg_ids))\n",
    "    whole_samp['truth'] = None\n",
    "    whole_samp['truth'][whole_samp['code'] != pos_key] = False\n",
    "    whole_samp['truth'][whole_samp['code'] == pos_key] = True\n",
    "    bin_cm = det_to_cm(whole_samp['classed'].to_numpy(), whole_samp['truth'].to_numpy())\n",
    "    rate = cm_to_rate(bin_cm)\n",
    "    return rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note to self: doing sanity checks that the rate creation bit works properly, then build up the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos, neg = subsample_cat(cm, cm_indices)\n",
    "print(len(pos))\n",
    "print(len(neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rates = cat_to_rate(pos, neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "RateMatrix = collections.namedtuple('rates', 'TPR FPR FNR TNR TP FP FN TN')\n",
    "class det_mets(RateMatrix):\n",
    "    \"binary classification metrics\"\n",
    "    def __init__(self, **rates):\n",
    "        \"\"\"\n",
    "        Call like `thing = det_mets(**rates._asdict())`\n",
    "        \"\"\"\n",
    "#         self.rates = rates#.asdict()\n",
    "        self.P = self.TP + self.FN\n",
    "        self.N = self.TN + self.FP\n",
    "#     def efficiency(self):\n",
    "#         self."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thing = det_mets(**rates._asdict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now try to get rates for the samples using proclam functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deterministic_metrics(truth, classified_pos, type_key=sel_class):\n",
    "    true_rates = truth.groupby('code').count()['id']\n",
    "    tp_plus_fp = pd.merge(classified_pos, truth, right_index=True, #on='id',\n",
    "                left_on='id', right_on='object_id')[['id', 'code']]\n",
    "    \n",
    "    cm_to_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deterministic_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## next, perturb randomly using vaguely `proclam` approach\n",
    "\n",
    "idea: mixture model of confusion matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maybe consider `proclam` classifier archetypes for inspiration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 'Uncertain'\n",
    "# cm = np.ones((M_classes, M_classes))\n",
    "\n",
    "# # 'Perfect'\n",
    "# cm = np.eye(M_classes) + 1.e-8\n",
    "\n",
    "# # 'Almost'\n",
    "# cm = np.eye(M_classes) + 0.1 * np.ones((M_classes, M_classes))\n",
    "\n",
    "# # 'Noisy'\n",
    "# cm = np.eye(M_classes) + 0.5 * np.ones((M_classes, M_classes))\n",
    "\n",
    "# # 'Tunnel Vision'\n",
    "# cm = np.ones((M_classes, M_classes))\n",
    "# cm = cm * np.asarray(0.1)[np.newaxis, np.newaxis]\n",
    "# cm[:, chosen] = cm[:, chosen] / M_classes\n",
    "# cm[chosen][chosen] += M_classes\n",
    "\n",
    "# # 'Cruise Control'\n",
    "# cm = np.eye(M_classes) + 1.e-8\n",
    "# cm[:] = cm[chosen]\n",
    "\n",
    "# # 'Subsuming'\n",
    "# cm = np.eye(M_classes) + 0.1 * np.ones((M_classes, M_classes))\n",
    "# cm[chosen] = cm[chosen-1]\n",
    "\n",
    "# # 'Mutually Subsuming'\n",
    "# cm = np.eye(M_classes) + 0.1 * np.ones((M_classes, M_classes))\n",
    "# cm[chosen][chosen+1] = cm[chosen][chosen]\n",
    "# cm[chosen+1][chosen] = cm[chosen+1][chosen+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb_subsamp(cm, cm_indices, samp_key, epsilon):\n",
    "    cm_row = cm.T[cm_indices[samp_key]]\n",
    "    for typeid in cm_indices.keys():\n",
    "        if typeid != samp_key:\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## next, make samples corresponding to metric values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start with purity and efficiency for binary classification situation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tot_in_test = np.sum(cm, axis=1)\n",
    "# print(tot_in_test)\n",
    "\n",
    "# tp_in_test = np.diag(cm)\n",
    "# tpr_in_test = tp_in_test / tot_in_test\n",
    "# # print((tp_in_test, tpr_in_test))\n",
    "# efficiency = tpr_in_test\n",
    "# print(efficiency)\n",
    "\n",
    "# contamination_raw = np.sum(cm, axis=0) - tp_in_test\n",
    "# contamination_rate = contamination_raw / tot_in_test\n",
    "# # print((contamination_raw, contamination_rate))\n",
    "# purity = tp_in_test / (tp_in_test + contamination_raw)\n",
    "# print(purity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recidivator (Python 3)",
   "language": "python",
   "name": "recidivator_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
