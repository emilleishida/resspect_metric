{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emulating realistically bad-for-cosmology SN Ia samples from PLAsTiCC data\n",
    "\n",
    "_Alex I. Malz (GCCL@RUB)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import gzip\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "\n",
    "rando = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import proclam\n",
    "from proclam.metrics.util import *\n",
    "from proclam.metrics.util import RateMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classes we care about\n",
    "\n",
    "| `true_target`=`type` | `code` |\n",
    "| -------------------- | ------ |\n",
    "| 90 | SNIa |\n",
    "| 67 | SNIa-91bg |\n",
    "| 52 | SNIax |\n",
    "| 42 | SNII |\n",
    "| 62 | SNIbc |\n",
    "| 95 | SLSN-I |\n",
    "| 88 | AGN |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maybe_sn_classes = {90: 'SNIa', \n",
    "                    67: 'SNIa-91bg', \n",
    "                    52: 'SNIax', \n",
    "                    42: 'SNII', \n",
    "                    62: 'SNIbc', \n",
    "                    95: 'SLSN-I', \n",
    "                    88: 'AGN'}\n",
    "maybe_sn_classes[15] = 'TDE'\n",
    "maybe_sn_classes[64] = 'KN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gather all available lightcurves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '/media/RESSPECT/data/PLAsTiCC/PLAsTiCC_zenodo/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "other than intermediate data products, work in `/media/RESSPECT/data/PLAsTiCC/for_metrics/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_maybe_sn = pd.read_csv(datapath+'plasticc_test_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_maybe_sn = all_maybe_sn.rename(columns={\"object_id\": \"id\", \"true_z\": \"redshift\", \"true_target\": \"code\"})\n",
    "print(all_maybe_sn.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DDF now, WFD later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_ddf = all_maybe_sn.loc[all_maybe_sn['ddf_bool'] == 1][['id', 'redshift', 'code']]\n",
    "# true_wfd = all_maybe_sn.loc[all_maybe_sn['ddf_bool'] == 0][['id', 'redshift', 'code']]\n",
    "n_ddf_tot = len(true_ddf)\n",
    "n_ddf_all = dict(true_ddf.groupby('code').count()['id'])\n",
    "# ddf_rats = dict(true_ddf.groupby('code').count()['id'] / n_ddf_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ddf_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_ddf.loc[true_ddf['code'] == 95]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a priori all samples will be 3000 \"classified SN Ia\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## subsample the classes to make new samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_class_pos = 3000\n",
    "sel_class = 90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get sample ids matching a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# class ConfMat_borken(object):\n",
    "#     def __init__(self, cm, indmap):\n",
    "#         \"\"\"\n",
    "#         `axis=0` is predicted classes, `axis=1` is true classes\n",
    "#         \"\"\"\n",
    "#         self.in_cm = cm \n",
    "#         self.cm = self.in_cm / np.sum(self.in_cm)\n",
    "#         self.class_ids = indmap\n",
    "#         self.norm = np.ones_like(self.class_ids)\n",
    "#         self._extract_cols_()\n",
    "#         self._extract_rows_()\n",
    "# #         self._check_cm_()\n",
    "#     def _check_cm_(self):\n",
    "#         # insert some consistency checks here\n",
    "#         pass\n",
    "#     def _extract_cols_(self):\n",
    "#         self.true_cols = {typeid: self.cm[self.class_ids[typeid]] for typeid in self.class_ids.keys()}\n",
    "#         return self.true_cols\n",
    "#     def _extract_rows_(self):\n",
    "#         self.pred_rows = {typeid: self.cm.T[self.class_ids[typeid]] for typeid in self.class_ids.keys()}\n",
    "#         return self.pred_rows\n",
    "#     def _proc_norm_(self, norm, n_orig):\n",
    "#         if type(norm) == int:\n",
    "#             norm = {typeid: norm for typeid in self.class_ids.keys()}\n",
    "#         self.norm = np.empty(len(self.class_ids))\n",
    "#         for classno in self.class_ids.keys():\n",
    "#             self.norm[self.class_ids[classno]] = norm[classno]\n",
    "#         self.norm[n_orig == 0] = 0\n",
    "#         self.norm[n_orig != 0] = self.norm[n_orig != 0] / n_orig[n_orig != 0]\n",
    "#         return self.norm\n",
    "#     def rescale_true(self, norm):\n",
    "#         n_true = np.sum(self.cm, axis=0)\n",
    "#         self.norm = self._proc_norm_(norm, n_true)\n",
    "#         self.cm = self.cm * self.norm\n",
    "#         self._extract_cols_()\n",
    "#         self._extract_rows_()\n",
    "# #         print(np.sum(self.cm, axis=0))\n",
    "#         return# self.cm\n",
    "#     def rescale_pred(self, norm):\n",
    "#         n_pred = np.sum(self.cm, axis=1)\n",
    "#         self.norm = self._proc_norm_(norm, n_pred)\n",
    "#         self.cm = (self.cm.T * self.norm).T\n",
    "#         self._extract_cols_()\n",
    "#         self._extract_rows_()\n",
    "# #         print(np.sum(self.cm, axis=1))\n",
    "#         return# self.cm\n",
    "#     def binarize(self, ref_class):\n",
    "#         pass\n",
    "#     def make_rates(self, ref_class=None):\n",
    "#         pass\n",
    "\n",
    "# class ConfMat(object):\n",
    "#     def __init__(self, cm, indmap):\n",
    "#         \"\"\"\n",
    "#         `axis=0` is predicted classes, `axis=1` is true classes\n",
    "#         \"\"\"\n",
    "#         self.in_cm = cm\n",
    "#         self.class_ids = indmap\n",
    "#         self.true_cols = {typeid: cm[indmap[typeid]] for typeid in indmap.keys()}\n",
    "#         self.pred_rows = {typeid: cm.T[indmap[typeid]] for typeid in indmap.keys()}\n",
    "#         self._check_cm_()\n",
    "#     def _check_cm_(self):\n",
    "#         # insert some consistency checks here\n",
    "#         pass\n",
    "#     def binarize(self, ref_class):\n",
    "#         pass\n",
    "#     def make_rates(self, ref_class=None):\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the true/false positive/negative rates along the way to making the subsamples, we need a notion of negatives that would never end up in the cosmology sample.\n",
    "Let's use the DDF type ratios to figure out how many objects will be classified as negative for our samples of 3000 positive classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ddf_pos = n_ddf_all[sel_class]\n",
    "n_ddf_glob = sum([n_ddf_all[classid] for classid in maybe_sn_classes])\n",
    "n_ddf_neg = n_ddf_glob - n_ddf_pos\n",
    "\n",
    "n_class_glob = n_class_pos * n_ddf_glob / n_ddf_pos\n",
    "n_class_all = {classid: int(round(n_class_glob * n_ddf_all[classid] / n_ddf_glob)) for classid in maybe_sn_classes}\n",
    "n_class_glob = sum(n_class_all.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`n_class_all` contains the number of objects in the true population, and the confusion matrix tells us how many will end up being classified as positive or negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save outputs as `id,redshift,type,code,orig_sample=test,queryable=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def subsample_cm_borken(cm_obj, samp_key, cat, \n",
    "#                  where_to_save=None, rando=rando):#, ntot=n_samp):\n",
    "#     cm_row = cm_obj.pred_rows[samp_key]#cm.T[cm_indices[samp_key]]\n",
    "#     print(cm_row)\n",
    "#     out_ids = pd.DataFrame(columns=cat.columns)\n",
    "#     for typekey in cm_obj.class_ids.keys():\n",
    "# #         if not ntot:\n",
    "#         print(cm_obj.class_ids[typekey])\n",
    "#         n_to_sample = int(cm_row[cm_obj.class_ids[typekey]])\n",
    "#         if n_to_sample < len(cat[cat['code'] == typekey]):\n",
    "#             matches = cat[cat['code'] == typekey].sample(n=n_to_sample, random_state=rando)\n",
    "#         else:\n",
    "#             print(n_to_sample)\n",
    "#         if n_to_sample > 0:\n",
    "#             out_ids = out_ids.append(matches)\n",
    "#     out_ids['orig_sample'] = 'test'\n",
    "#     out_ids['queryable'] = True\n",
    "#     out_ids['type'] = None\n",
    "#     if where_to_save:\n",
    "#         savecols = ['id','redshift','type','code','orig_sample','queryable']\n",
    "#         out_ids[savecols].to_csv(where_to_save+'.csv', index=False)\n",
    "#     return(out_ids)\n",
    "\n",
    "# def subsample_cm(cm_obj, samp_key, cat, \n",
    "#                  where_to_save=None, rando=rando):#, ntot=n_samp):\n",
    "#     cm_row = cm_obj.pred_rows[samp_key]#cm.T[cm_indices[samp_key]]\n",
    "#     out_ids = pd.DataFrame(columns=cat.columns)\n",
    "#     for typekey in cm_obj.class_ids.keys():\n",
    "# #         if not ntot:\n",
    "#         n_to_sample = int(cm_row[cm_obj.class_ids[typekey]])\n",
    "#         matches = cat[cat['code'] == typekey].sample(n=n_to_sample, random_state=rando)\n",
    "#         if len(matches) > 0:\n",
    "#             out_ids = out_ids.append(matches)\n",
    "#     out_ids['orig_sample'] = 'test'\n",
    "#     out_ids['queryable'] = True\n",
    "#     out_ids['type'] = None\n",
    "#     if where_to_save:\n",
    "#         savecols = ['id','redshift','type','code','orig_sample','queryable']\n",
    "#         out_ids[savecols].to_csv(where_to_save+'.csv', index=False)\n",
    "#     return(out_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample_cat(cm, cm_indices, pos_key=sel_class, \n",
    "                  ntot=n_class_all, cat=true_ddf, \n",
    "                  where_to_save=None, save_neg=True, rando=rando):\n",
    "    # normalize to number in true class\n",
    "    pcm = cm / np.sum(cm, axis=1)\n",
    "    # want row corresponding to predicted class\n",
    "    pos_row = pcm[cm_indices[pos_key]] * ntot[pos_key]\n",
    "    pos_ids, neg_ids = pd.DataFrame(columns=cat.columns), pd.DataFrame(columns=cat.columns)\n",
    "#     bin_cm = np.zeros((2, 2))\n",
    "    for typeid in cm_indices.keys():\n",
    "        n_pos = int(round(pos_row[cm_indices[typeid]]))\n",
    "        n_neg = ntot[typeid] - n_pos\n",
    "#         if typeid == pos_key:\n",
    "#             print((cm[cm_indices[typeid]], pos_row[cm_indices[typeid]], ntot[typeid], n_pos))\n",
    "#             bin_cm[0][0] += n_pos\n",
    "#             bin_cm[1][0] += n_neg\n",
    "#         else:\n",
    "#             bin_cm[0][1] += n_pos\n",
    "#             bin_cm[1][1] += n_neg\n",
    "        matches = cat[cat['code'] == typeid].sample(n=ntot[typeid], random_state=rando)\n",
    "        pos = matches[:n_pos]\n",
    "        neg = matches[n_pos:]\n",
    "        if len(pos) > 0:\n",
    "            pos_ids = pos_ids.append(pos)\n",
    "        if len(neg) > 0:\n",
    "            neg_ids = neg_ids.append(neg)\n",
    "    if where_to_save:\n",
    "        pos_ids['orig_sample'] = 'test'\n",
    "        pos_ids['queryable'] = True\n",
    "        pos_ids['type'] = None\n",
    "        pos_ids[['id','redshift','type','code','orig_sample','queryable']].to_csv(where_to_save+'.csv', index=False)\n",
    "    return pos_ids, neg_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### realistic classifier\n",
    "\n",
    "start from fiducial contamination rates from a real (awful) confusion matrix at `/media/RESSPECT/data/PLAsTiCC/for_metrics/confusion_matrices`\n",
    "\n",
    "These were just the test set lightcurves for classes (67, 88, 42(minus 7?), 90(minus 11?), 52, 62, 64, 95, 15) from ddf-only\n",
    "\n",
    "figure out classes in confusion matrix by comparing number of ddf test set-only lightcurves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath = '/media/RESSPECT/data/PLAsTiCC/for_metrics/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(savepath+'confusion_matrices/confusion_matrix.npy', 'rb') as confmat:\n",
    "    cm = np.load(confmat)\n",
    "# plt.imshow(np.log(fid_cm.in_cm), cmap='viridis_r')\n",
    "# plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_classes = [67, 88, 42, 90, 52, 62, 64, 95, 15]\n",
    "cm_indices = {}\n",
    "# cm_rat = {}\n",
    "for classid in maybe_sn_classes.keys():\n",
    "    cm_indices[classid] = cm_classes.index(classid)\n",
    "#     cm_rat[classid] = sum(cm[cm_indices[classid]]) / n_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# fid_cm = ConfMat(cm, cm_indices)\n",
    "# # fid_cm.rescale_true(n_ddf_all)\n",
    "# fid_cm.rescale_pred(n_class_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fiducial sample corresponding to input confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiducial = subsample_cat(cm, cm_indices, where_to_save=savepath+'fiducial_samp')\n",
    "# print(len(fiducial))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reopen = pd.read_csv(savepath+'fiducial_samp.csv')\n",
    "\n",
    "reopen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 100% SNIa sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_cm = np.identity(len(cm_indices.keys()))\n",
    "# perf_cm = ConfMat(cm_perfect, cm_indices)\n",
    "# perf_cm.rescale_true(n_ddf_all)\n",
    "# perf_cm.rescale_pred(n_class_pos)\n",
    "# print(perf_cm.cm)\n",
    "perfect = subsample_cat(perf_cm, cm_indices, where_to_save=savepath+'perfect_samp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create new confusion matrices to tune output sample rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "consider `proclam` classifier archetypes for inspiration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_classes = len(cm_indices)\n",
    "\n",
    "# 'Uncertain'\n",
    "cm_uncertain = np.ones((M_classes, M_classes))\n",
    "\n",
    "# 'Perfect'\n",
    "cm_perfect = np.eye(M_classes) + 1.e-8\n",
    "\n",
    "# 'Almost'\n",
    "cm_almost = np.eye(M_classes) + 0.1 * np.ones((M_classes, M_classes))\n",
    "\n",
    "# 'Noisy'\n",
    "cm_noisy = np.eye(M_classes) + 0.5 * np.ones((M_classes, M_classes))\n",
    "\n",
    "# # 'Tunnel Vision'\n",
    "# cm = np.ones((M_classes, M_classes))\n",
    "# cm = cm * np.asarray(0.1)[np.newaxis, np.newaxis]\n",
    "# cm[:, chosen] = cm[:, chosen] / M_classes\n",
    "# cm[chosen][chosen] += M_classes\n",
    "\n",
    "# # 'Cruise Control'\n",
    "# cm = np.eye(M_classes) + 1.e-8\n",
    "# cm[:] = cm[chosen]\n",
    "\n",
    "# # 'Subsuming'\n",
    "# cm = np.eye(M_classes) + 0.1 * np.ones((M_classes, M_classes))\n",
    "# cm[chosen] = cm[chosen-1]\n",
    "\n",
    "# # 'Mutually Subsuming'\n",
    "# cm = np.eye(M_classes) + 0.1 * np.ones((M_classes, M_classes))\n",
    "# cm[chosen][chosen+1] = cm[chosen][chosen]\n",
    "# cm[chosen+1][chosen] = cm[chosen+1][chosen+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Mutually Subsuming'\n",
    "target = cm_indices[sel_class]\n",
    "contaminant = cm_indices[62]\n",
    "half_ibc_cm = np.eye(M_classes) + 0.1 * np.ones((M_classes, M_classes))\n",
    "half_ibc_cm[target][contaminant] = half_ibc_cm[target][target]\n",
    "half_ibc_cm[contaminant][target] = half_ibc_cm[contaminant][contaminant]\n",
    "# plt.imshow(half_ibc_cm)\n",
    "# plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make new confusion matrices as mixtures of existing ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_arr(inarrs, weights=None):\n",
    "    narrs = len(inarrs)\n",
    "    if weights is None:\n",
    "        weights = np.ones_like((1, narrs))\n",
    "    arrs = inarrs / np.sum(np.sum(inarrs, axis=-1), axis=-1)[:, np.newaxis, np.newaxis]\n",
    "    normwts = weights / np.sum(weights)\n",
    "    outarr = np.sum(arrs * normwts[:, np.newaxis, np.newaxis], axis=0)\n",
    "    return outarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cm = mix_arr(np.array([cm_uncertain, cm_perfect]))\n",
    "# plt.imshow(new_cm)\n",
    "# plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate classification metrics on the subsamples\n",
    "\n",
    "better to do it along the way to making the subsamples, especially important for non-extreme subsamples filling the space of classification metric values\n",
    "\n",
    "first get rates using `proclam` functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_to_rate(pos_ids, neg_ids, pos_key=sel_class):\n",
    "    pos_ids['classed'] = True\n",
    "    neg_ids['classed'] = False\n",
    "    whole_samp = pd.concat((pos_ids, neg_ids))\n",
    "    whole_samp['truth'] = None\n",
    "    whole_samp['truth'][whole_samp['code'] != pos_key] = False\n",
    "    whole_samp['truth'][whole_samp['code'] == pos_key] = True\n",
    "    bin_cm = det_to_cm(whole_samp['classed'].to_numpy(), whole_samp['truth'].to_numpy())\n",
    "    rawrate = cm_to_rate(bin_cm)._asdict()\n",
    "    rel_to_sel = {key: rawrate[key][0] for key in rawrate.keys()}\n",
    "    rate = proclam.util.RateMatrix(**rel_to_sel)\n",
    "    return rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate all the metrics!\n",
    "\n",
    "and put some version of this into `proclam` at some point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class det_mets(RateMatrix):\n",
    "    \"binary classification metrics\"\n",
    "    def __init__(self, **rates):\n",
    "        \"\"\"\n",
    "        Call like `thing = det_mets(**rates._asdict())`\n",
    "        \"\"\"\n",
    "#         self.rates = rates#.asdict()\n",
    "        self._get_tots()\n",
    "        self._from_rates()\n",
    "        self._sn_mets()\n",
    "        self._translate()\n",
    "    def _get_tots(self):\n",
    "        self.CP = self.TP + self.FN\n",
    "        self.CN = self.TN + self.FP\n",
    "        self.T = self.TP + self.TN\n",
    "        self.F = self.FP + self.FN\n",
    "        self.P = self.TP + self.FP\n",
    "        self.N = self.TN + self.FN\n",
    "    def _from_rates(self):\n",
    "        self.PPV = self.TP / (self.TP + self.FP)\n",
    "        self.NPV = self.TN / (self.TN + self.FN)\n",
    "        self.PT = (np.sqrt(self.TPR * (1. - self.TNR)) + self.TNR - 1.) / (self.TPR + self.TNR - 1.)\n",
    "        self.TS = self.TP / (self.TP + self.FN + self.FP)\n",
    "        self._derived()\n",
    "    def _derived(self):\n",
    "        self.ACC = (self.TP + self.TN) / (self.CP + self.CN)\n",
    "        self.BA = (self.TPR + self.TNR) / 2,\n",
    "        self.F1S = 2. * self.PPV * self.TPR / (self.PPV + self.TPR)\n",
    "        self.MCC = (self.TP * self.TN - self.FP * self.FN) / (np.sqrt(self.P * self.CP * self.CN * self.N))\n",
    "        self.FM = np.sqrt(self.PPV * self.TPR)\n",
    "        self.BM = self.TPR + self.TNR - 1.\n",
    "        self.MK = self.PPV + self.NPV - 1.\n",
    "    def _translate(self):\n",
    "        self.positive = self.CP\n",
    "        self.negative = self.CN\n",
    "        self.sensitivity = self.TPR\n",
    "        self.recall = self.TPR\n",
    "        self.specificity = self.TNR\n",
    "        self.selectivity = self.TNR\n",
    "        self.precision = self.PPV\n",
    "        self.FDR = 1. - self.PPV\n",
    "        self.FOR = 1. - self.NPV\n",
    "        self.CSI = self.TS\n",
    "        self.accuracy = self.ACC\n",
    "        self.f1_score = self.F1S\n",
    "        self.informedness = self.BM\n",
    "        self.deltaP = self.MK\n",
    "    def _sn_mets(self):\n",
    "        self.get_efficiency()\n",
    "        self.get_purity()\n",
    "    def get_efficiency(self):\n",
    "        self.efficiency = self.TP / self.CP\n",
    "        return self.efficiency\n",
    "    def get_purity(self):\n",
    "        self.purity = self.TP / self.P\n",
    "        return self.purity\n",
    "    def get_fom(self, penalty):\n",
    "        self.pseudo_purity = self.TP / (self.TP + penalty * self.FP)\n",
    "        return self.pseudo_purity * self.efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "demonstrate on the archetypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cm in [cm_perfect, cm_almost, cm_noisy]:\n",
    "    pos, neg = subsample_cat(cm, cm_indices)\n",
    "    rates = cat_to_rate(pos, neg)\n",
    "    mets = det_mets(**rates._asdict())\n",
    "    print(f'purity:{mets.purity}, efficiency:{mets.efficiency}, fom1:{mets.get_fom(1.)}, fom3:{mets.get_fom(3.)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## next, make samples corresponding to metric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maybe_sn_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "original plan was to have these samples:\n",
    "- 100% Ia\n",
    "- Ia/Ibc\n",
    "- - 50/50\n",
    "- - 75/25\n",
    "- - 90/10\n",
    "- - 95/5\n",
    "- - 98/2\n",
    "- Ia/II\n",
    "- Ia/91bg\n",
    "- Ia/Iax\n",
    "- AGN\n",
    "- TDE \n",
    "- KN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ia_percents = np.array([50, 68, 75, 90, 95, 98, 99])\n",
    "mix_percents = 100 - ia_percents\n",
    "contaminants = maybe_sn_classes.copy()\n",
    "contaminants.pop(sel_class)\n",
    "samppath = savepath+'samples/'\n",
    "metpath = savepath+'metrics/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assume symmetry in 2-class mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# binary_ia_mets = {}\n",
    "for key, val in contaminants.items():\n",
    "# for j in [0]:\n",
    "#     key = 67\n",
    "#     val = contaminants[key]\n",
    "#     binary_ia_mets[key] = []\n",
    "    subset_indices = {sel_class: 0, key: 1}\n",
    "    for i, perc in enumerate(mix_percents):\n",
    "        sampfn = samppath+str(ia_percents[i])+str(maybe_sn_classes[sel_class])+str(perc)+val\n",
    "        print(sampfn)\n",
    "        cm = np.array([[ia_percents[i], perc], [perc, ia_percents[i]]])\n",
    "#         print(cm)\n",
    "        pos, neg = subsample_cat(cm, subset_indices, where_to_save=sampfn)\n",
    "        rates = cat_to_rate(pos, neg)\n",
    "#         mets = det_mets(**rates._asdict())\n",
    "        metfn = metpath+f'{ia_percents[i]}_{sel_class}_{perc}_{key}'\n",
    "        with open(metfn+'.pkl', 'wb') as metfile:\n",
    "            pkl.dump(rates._asdict(), metfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recidivator (Python 3)",
   "language": "python",
   "name": "recidivator_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
