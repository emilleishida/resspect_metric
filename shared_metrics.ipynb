{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating and saving all metrics\n",
    "\n",
    "_Alex I. Malz (GCCL@RUB) & Emille Ishida (LPC@Clermont-Ferrand)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "# import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import gzip\n",
    "import scipy.stats as sps\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class naming scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# types_names = {90:'Ia', 67: '91bg', 52:'Iax', 42:'II', 62:'Ibc', \n",
    "#                95: 'SLSN', 15:'TDE', 64:'KN', 88:'AGN', 92:'RRL', 65:'M-dwarf',\n",
    "#                16:'EB',53:'Mira', 6:'MicroL', 991:'MicroLB', 992:'ILOT', \n",
    "#                993:'CART', 994:'PISN',995:'MLString'}\n",
    "\n",
    "# SNANA_types = {90:11, 62:{1:3, 2:13}, 42:{1:2, 2:12, 3:14},\n",
    "#                67:41, 52:43, 64:51, 95:60, 994:61, 992:62,\n",
    "#                993:63, 15:64, 88:70, 92:80, 65:81, 16:83,\n",
    "#                53:84, 991:90, 6:{1:91, 2:93}}\n",
    "\n",
    "SNANA_names = {11: 'Ia', 3:'Ibc', 13: 'Ibc', 2:'II', 12:'II', 14:'II',\n",
    "               41: '91bg', 43:'Iax', 51:'KN', 60:'SLSN', 61:'PISN', 62:'ILOT',\n",
    "               63:'CART', 64:'TDE', 70:'AGN', 80:'RRL', 81:'M-dwarf', 83:'EB',\n",
    "               84:'Mira', 90:'MicroLB', 91:'MicroL', 93:'MicroL'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how to name saved files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are for what to name saved files\n",
    "alex = False\n",
    "redo = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which test cas to run (because one big loop would be awful, although this should really be a script that runs them in parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = ''\n",
    "field = 'DDF'\n",
    "if field == 'WFD':\n",
    "    k = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "manually remove cases that won't be in the same big plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if field == 'DDF':\n",
    "        dirname = '/media/RESSPECT/data/PLAsTiCC/for_metrics/ddf/emille_samples/'\n",
    "        cases = os.listdir(dirname)\n",
    "        cases.remove('random1500.csv')\n",
    "        cases.remove('random6000.csv')\n",
    "        cases.remove('fiducial1500.csv')\n",
    "        cases.remove('fiducial6000.csv')\n",
    "        cases.remove('perfect6000.csv')\n",
    "        cases.remove('perfect1500.csv')\n",
    "        cases.remove('all_DDF.csv')\n",
    "        cases.remove('perfect3000.csv')\n",
    "        cases.remove('random3000.csv')\n",
    "elif field == 'WFD':\n",
    "        dirname = '/media/RESSPECT/data/PLAsTiCC/for_metrics/wfd/emille_samples' + str(k) + '/'\n",
    "        cases = os.listdir(dirname)\n",
    "#         print(cases)\n",
    "        cases.remove('M0DIF')\n",
    "        cases.remove('fitres')\n",
    "        cases.remove('stan_summary')\n",
    "#         cases.remove('perfect1500.csv')\n",
    "#         cases.remove('perfect6000.csv')\n",
    "#         cases.remove('perfect3000_IX.csv')\n",
    "#         cases.remove('perfect3000_I.csv') \n",
    "#         cases.remove('perfect3000_II.csv')\n",
    "#         cases.remove('perfect3000_III.csv')\n",
    "#         cases.remove('perfect3000_IV.csv')\n",
    "#         cases.remove('perfect3000_V.csv')\n",
    "#         cases.remove('perfect3000_VI.csv')\n",
    "#         cases.remove('perfect3000_VII.csv')\n",
    "#         cases.remove('perfect3000_VIII.csv')\n",
    "#         cases.remove('perfect3000_0.csv')\n",
    "        cases.remove('perfect3000.csv')\n",
    "        cases.remove('random3000.csv')\n",
    "        cases.remove('fiducial3000.csv')\n",
    "if '.ipynb_checkpoints' in cases:\n",
    "        cases.remove('.ipynb_checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checking percentages matching names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for name in cases:    \n",
    "#     fname = dirname + name \n",
    "\n",
    "#     data = pd.read_csv(fname)\n",
    "\n",
    "#     types, freq = np.unique(data['SIM_TYPE_INDEX'].values, return_counts=True)\n",
    "        \n",
    "# #     print('\\n')\n",
    "# #     print('case: ' + name)\n",
    "# #     for i in range(len(types)):\n",
    "# #         print('perc ' + SNANA_names[types[i]] + ' : ', round(freq[i]/data.shape[0], 2))\n",
    "# #     print('Total number: ', data.shape[0])\n",
    "# #     print('Unique ids: ', np.unique(data['CID'].shape[0]))\n",
    "# #     print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## how to calculate classification metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import proclam\n",
    "from proclam.metrics.util import *\n",
    "from proclam.metrics.util import RateMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class for binary classification metrics\n",
    "\n",
    "TODO: add in checks for between 0 and 1\n",
    "\n",
    "TODO: put this in proclam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class det_mets(RateMatrix):\n",
    "    \"binary classification metrics\"\n",
    "    def __init__(self, **rates):\n",
    "        \"\"\"\n",
    "        Call like `thing = det_mets(**rates._asdict())`\n",
    "        \"\"\"\n",
    "#         self.rates = rates#.asdict()\n",
    "        self._get_tots()\n",
    "        self._from_rates()\n",
    "        self._sn_mets()\n",
    "        self._translate()\n",
    "    def _get_tots(self):\n",
    "        self.CP = self.TP + self.FN\n",
    "        self.CN = self.TN + self.FP\n",
    "        self.T = self.TP + self.TN\n",
    "        self.F = self.FP + self.FN\n",
    "        self.P = self.TP + self.FP\n",
    "        self.N = self.TN + self.FN\n",
    "    def _from_rates(self):\n",
    "        self.PPV = self.TP / (self.TP + self.FP)\n",
    "        self.NPV = self.TN / (self.TN + self.FN)\n",
    "        self.PT = (np.sqrt(self.TPR * (1. - self.TNR)) + self.TNR - 1.) / (self.TPR + self.TNR - 1.)\n",
    "        self.TS = self.TP / (self.TP + self.FN + self.FP)\n",
    "        self._derived()\n",
    "    def _derived(self):\n",
    "        self.ACC = (self.TP + self.TN) / (self.CP + self.CN)\n",
    "        self.BA = (self.TPR + self.TNR) / 2.,\n",
    "        self.F1S = 2. * self.PPV * self.TPR / (self.PPV + self.TPR)\n",
    "        self.MCC = (self.TP * self.TN - self.FP * self.FN) / (np.sqrt(self.P * self.CP * self.CN * self.N))\n",
    "        self.FM = np.sqrt(self.PPV * self.TPR)\n",
    "        self.BM = self.TPR + self.TNR - 1.\n",
    "        self.MK = self.PPV + self.NPV - 1.\n",
    "    def _translate(self):\n",
    "        self.positive = self.CP\n",
    "        self.negative = self.CN\n",
    "        self.sensitivity = self.TPR\n",
    "        self.recall = self.TPR\n",
    "        self.specificity = self.TNR\n",
    "        self.selectivity = self.TNR\n",
    "        self.precision = self.PPV\n",
    "        self.FDR = 1. - self.PPV\n",
    "        self.FOR = 1. - self.NPV\n",
    "        self.CSI = self.TS\n",
    "        self.accuracy = self.ACC\n",
    "        self.f1_score = self.F1S\n",
    "        self.informedness = self.BM\n",
    "        self.deltaP = self.MK\n",
    "    def _sn_mets(self):\n",
    "        self.get_efficiency()\n",
    "        self.get_purity()\n",
    "    def get_efficiency(self):\n",
    "        self.efficiency = self.TP / self.CP\n",
    "        return self.efficiency\n",
    "    def get_purity(self):\n",
    "        self.purity = self.TP / self.P\n",
    "        return self.purity\n",
    "    def get_fom(self, penalty):\n",
    "        self.pseudo_purity = self.TP / (self.TP + penalty * self.FP)\n",
    "        fom = self.pseudo_purity * self.efficiency\n",
    "        return fom\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make the proclam ratematrix from information we know about each test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nums_to_rate(tot, P, T, FP):\n",
    "    N = tot - P\n",
    "    TN = N - FP\n",
    "    F = tot - T\n",
    "    FN = F - TN\n",
    "    TP = P - FN\n",
    "    assert(FP + FN + TP + TN == tot)\n",
    "    \n",
    "    TPR = TP / P\n",
    "    FPR = FP / N\n",
    "    FNR = FN / P\n",
    "    TNR = TN / N\n",
    "    \n",
    "#     cm = np.array([[totIa - cont, totall - ], [, cont]])\n",
    "    rate = proclam.util.RateMatrix(TPR=TPR, FPR=FPR, FNR=FNR, TNR=TNR, TP=TP, FN=FN, TN=TN, FP=FP)\n",
    "    return rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define the true number of objects available in each sample before and after SALT2 fit as filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = {}\n",
    "total['DDF'] = {}                     # total number of objects in the sample\n",
    "total['DDF']['before_salt2'] = 4335\n",
    "total['DDF']['after_salt2'] = 3456\n",
    "total['WFD'] = {}\n",
    "total['WFD']['before_salt2'] = 5588\n",
    "total['WFD']['after_salt2'] = 3306"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate the metrics from the available numbers of objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_metrics(cont, totIa=3000, before_salt2=False, field=field):\n",
    "    \"\"\"Classification metrics for a sample of 3k SNIa.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    cont: int < totIa\n",
    "        number of contaminant objects\n",
    "    totIa: int (optional)\n",
    "        Number of Ia in the sample. Default is 3000.\n",
    "    before_salt2: bool (optional)\n",
    "        If True use total sample number before SALT2 fit.\n",
    "        Default is False.\n",
    "    field: str (optional)\n",
    "        Cadence: 'DDF' or 'WFD'. Default is DDF.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    accuracy: floatTPR=TPR, FPR=FPR, FNR=FNR, TNR=TNR, TP=TP, FN=FN, TN=TN, FP=FP\n",
    "    efficiency: float\n",
    "    purity: float\n",
    "    figure of merit (W=1): float\n",
    "    figure of merit (W=3): float\n",
    "    \"\"\"\n",
    "    \n",
    "    if totIa != 3000:\n",
    "        raise ValueError('Numbers are hard coded for 3000 SNIa.')\n",
    "#         return(np.nan, np.nan, np.nan, np.nan, np.nan)\n",
    "    \n",
    "    if before_salt2:\n",
    "        totall = total[field]['before_salt2']\n",
    "    else:\n",
    "        totall = total[field]['after_salt2']\n",
    "    \n",
    "    rate = nums_to_rate(tot=totall, P=totIa, T=totIa, FP=cont)._asdict()\n",
    "    class_mets = det_mets(**rate)\n",
    "    \n",
    "#     acc = (totall - (2* totIa * cont))/totall\n",
    "#     eff = (totIa - totIa * cont)/totIa\n",
    "#     f1 = ((totIa - totIa * cont)/totIa) * (1 - cont)\n",
    "#     c = float(cont) / float(totIa)\n",
    "#     f3 = ((1. - c) * totIa)/(((1. - c) * totIa) + 3. * ((c) * totIa))\n",
    "    f3 = class_mets.get_fom(3.)\n",
    "#     if f3 < 0. or f3 > 1.:\n",
    "#         return False\n",
    "    \n",
    "#     return acc, eff, 1 - cont, f1, f3\n",
    "    return class_mets.accuracy, class_mets.get_efficiency(), class_mets.get_purity(), class_mets.f1_score, f3#class_mets.get_fom(3.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## how to calculate KLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KLD is prone to numerical divergence when coverage between distributions is poor, so protect against overflow/underflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 2. * sys.float_info.min\n",
    "\n",
    "def safe_log(arr, threshold=eps):\n",
    "    \"\"\"\n",
    "    Takes the natural logarithm of an array that might contain zeros.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr: ndarray, float\n",
    "        array of values to be logged\n",
    "    threshold: float, optional\n",
    "        small, positive value to replace zeros and negative numbers\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    logged: ndarray\n",
    "        logged values, with small value replacing un-loggable values\n",
    "    \"\"\"\n",
    "    arr = np.asarray(arr)\n",
    "    arr[arr < threshold] = threshold\n",
    "    logged = np.log(arr)\n",
    "    return logged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how to make a grid and fit multidimensional KDE to posterior samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_grid(x, y, x_ngrid=100, y_ngrid=100):\n",
    "    x_min = x.min()#-1.2\n",
    "    x_max = x.max()#-0.8\n",
    "    y_min = y.min()#0.2\n",
    "    y_max = y.max()#0.4\n",
    "\n",
    "    x_grid, y_grid = np.mgrid[x_min:x_max:x_ngrid*1.j, y_min:y_max:y_ngrid*1.j]\n",
    "    x_vec, y_vec = x_grid[:, 0], y_grid[0, :]\n",
    "    dx = (x_max - x_min) / (x_ngrid - 1)\n",
    "    dy = (y_max - y_min) / (y_ngrid - 1)\n",
    "\n",
    "    return(((x_min, y_min), (x_max, y_max)), (x_grid, y_grid), (x_vec, y_vec), (dx, dy))\n",
    "\n",
    "def make_kde(Xgrid, Ygrid, Xsamps, Ysamps, to_log=False, save=None):\n",
    "    positions = np.vstack([Xgrid.ravel(), Ygrid.ravel()])\n",
    "    values = np.vstack([Xsamps, Ysamps])\n",
    "    kernel = sps.gaussian_kde(values, bw_method='scott')\n",
    "    Z = np.reshape(kernel(positions).T, Xgrid.shape)\n",
    "    if to_log:\n",
    "        return safe_log(Z)\n",
    "    else:\n",
    "        return Z\n",
    "    \n",
    "#     if save is not None:\n",
    "        \n",
    "# TODO: normalize up here before log!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use chippr implementation of KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stolen from chippr\n",
    "def calculate_kld(lpe, lqe, dx, from_log=False, vb=True):\n",
    "    \"\"\"\n",
    "    Calculates the Kullback-Leibler Divergence between two N-dimensional PDFs \n",
    "    evaluated on a shared, regular grid (sorry, too lazy to deal with irregular grid)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lpe: numpy.ndarray, float\n",
    "        log-probability distribution evaluated on a grid whose distance from `q`\n",
    "        will be calculated.\n",
    "    lqe: numpy.ndarray, float\n",
    "        log-probability distribution evaluated on a grid whose distance to `p` will\n",
    "        be calculated.\n",
    "    dx: numpy.ndarray, float\n",
    "        separation of grid values in each dimension\n",
    "    from_log: boolean, optional\n",
    "        if False, lpe, lqe are probability distributions, not log-probability distributions\n",
    "    vb: boolean, optional\n",
    "        report on progress to stdout?\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dpq: float\n",
    "        the value of the Kullback-Leibler Divergence from `q` to `p`\n",
    "    \"\"\"\n",
    "    # Normalize the evaluations, so that the integrals can be done\n",
    "    gridnorm = np.ones_like(lpe) * np.prod(dx)\n",
    "    if from_log:\n",
    "        pe = np.exp(lpe)\n",
    "        qe = np.exp(lqe)\n",
    "#     print(np.prod(dx))\n",
    "#     print(gridnorm)\n",
    "    else:\n",
    "        pe = lpe\n",
    "        qe = lqe\n",
    "    pi = np.sum(pe * gridnorm)\n",
    "    qi = np.sum(qe * gridnorm)\n",
    "    # (very approximately!) by simple summation:\n",
    "    pn = pe / pi\n",
    "    qn = qe / qi\n",
    "    # Compute the log of the normalized PDFs\n",
    "    logp = safe_log(pn)\n",
    "    logq = safe_log(qn)\n",
    "    # Calculate the KLD from q to p\n",
    "    Dpq = np.sum(pn * (logp - logq))\n",
    "#     if np.isnan(Dpq):\n",
    "#         return((lpe, lqe, dx))\n",
    "    return Dpq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep for the metrics calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set up for saving the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "\n",
    "pop_Ia_all = []\n",
    "pop_nIa_all = []\n",
    "perc_Ia_all = []\n",
    "perc_nIa_all = []\n",
    "\n",
    "wfit_w_all = []\n",
    "wfit_wsig_all = []\n",
    "wfit_om_all = []\n",
    "wfit_omsig_all = []\n",
    "wfit_w_all_lowz = []\n",
    "wfit_wsig_all_lowz = []\n",
    "wfit_om_all_lowz = []\n",
    "wfit_omsig_all_lowz = []\n",
    "\n",
    "stan_w_all = []\n",
    "stan_wsig_all = []\n",
    "stan_om_all = []\n",
    "stan_omsig_all = []\n",
    "\n",
    "stan_w_all_lowz = []\n",
    "stan_wsig_all_lowz = []\n",
    "stan_om_all_lowz = []\n",
    "stan_omsig_all_lowz = []\n",
    "\n",
    "other_index = []\n",
    "other_name = []\n",
    "\n",
    "wdist_median = []\n",
    "\n",
    "eff = []\n",
    "pur = []\n",
    "acc = []\n",
    "f1 = []\n",
    "f3 = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read Wasserstein distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if field == 'DDF':\n",
    "    fname = 'Wasserstein/wassersteinDistances_ddf.dat'\n",
    "elif field == 'WFD':\n",
    "    fname = 'Wasserstein/wassersteinDistances_wfd' + str(k) +'.dat'\n",
    "\n",
    "wdist = pd.read_csv(fname)\n",
    "name_flagA = np.array(['perfect3000' in name for name in wdist['FileA'].values])\n",
    "name_flagB = np.array(['perfect3000' in name for name in wdist['FileB'].values])\n",
    "name_flag = np.logical_or(name_flagA, name_flagB)\n",
    "\n",
    "if field == 'DDF':\n",
    "    wdist_ddf = wdist[name_flag]\n",
    "elif field == 'WFD':\n",
    "    wdist_wfd = wdist[name_flag]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prep KDE for reference (closest to truth) distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_pre = dirname[:46]\n",
    "path_post = 'posteriors/samples_emille'+str(k)+'/'\n",
    "refname = 'chains_perfect3000_lowz_withbias.csv.gz'\n",
    "\n",
    "# fieldstr = {'DDF': 'ddf', 'WFD': 'wfd'}\n",
    "\n",
    "fullpath = path_pre + path_post\n",
    "alloutputs = pd.DataFrame(columns=['path', 'KLD'])\n",
    "    # make reference sample\n",
    "with gzip.open(fullpath+refname) as reffn:\n",
    "    flatref = pd.read_csv(reffn)\n",
    "[w_ref, Omm_ref] = [flatref['w'], flatref['om']]\n",
    "ref_extrema, ref_grids, ref_vecs, ref_ds = make_grid(w_ref, Omm_ref)\n",
    "(w_vec, Omm_vec) = ref_vecs\n",
    "(dw, dOmm) = ref_ds\n",
    "((xmin, ymin), (xmax, ymax)) = ref_extrema\n",
    "(w_grid, Omm_grid) = ref_grids\n",
    "d_ref = {'w': dw, 'Omm': dOmm}\n",
    "grid_ref = {'w': w_grid, 'Omm': Omm_grid}\n",
    "kde_ref = make_kde(w_grid, Omm_grid, w_ref, Omm_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "diagnostic plots for the posterior samples\n",
    "\n",
    "TODO: plot 1D marginals\n",
    "\n",
    "TODO: check that $\\Omega_{m}$ dimension is the same for all cases (and flat?)\n",
    "\n",
    "TODO: check for coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_perf, xgrid, ygrid = np.histogram2d(w_ref, Omm_ref, bins=ref_vecs, density=True)\n",
    "plt.imshow(kde_ref.T, origin='lower', extent=[xmin, xmax, ymin, ymax], cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(w_ref, Omm_ref, bins=[w_vec, Omm_vec], density=True, cmap=plt.cm.Blues)\n",
    "plt.savefig('just_dist.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read in cosmology outputs and calculate classification metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for case in cases:\n",
    "\n",
    "    names.append(case[:-4])\n",
    "    \n",
    "    pop = {}\n",
    "    perc = {}\n",
    "\n",
    "    samples_dir = case[:-4] + '/'\n",
    "    \n",
    "    if field == 'DDF':\n",
    "        if alex:\n",
    "            data = pd.read_csv('/media/emille/git/COIN/RESSPECT_work/PLAsTiCC/metrics_paper/' +\\\n",
    "                           'resspect_metric/SALT2_fit/DDF_Alex/' + case)\n",
    "        else:\n",
    "            data = pd.read_csv(dirname + case)\n",
    "    else:\n",
    "        data = pd.read_csv(dirname + case)\n",
    "#     elif field == 'WFD':\n",
    "#         data = pd.read_csv('/media/RESSPECT/data/PLAsTiCC/for_metrics/wfd/emille_samples' + str(k) + '/' + case)\n",
    "            \n",
    "    stats = np.unique(data['SIM_TYPE_INDEX'].values, return_counts=True)\n",
    "    \n",
    "    if field == 'DDF':\n",
    "        if alex:\n",
    "            fname_cosmo_lowz = '/media/emille/git/COIN/RESSPECT_work/PLAsTiCC/metrics_paper/resspect_metric/' + \\\n",
    "                    'posteriors/DDF/' + case[:-4] + '/' + \\\n",
    "                    'Alex_samples/omprior_0.01_flat/results/test_salt2mu_lowz_withbias_' + case[:-4] + '.M0DIF.cospar'\n",
    "        else:\n",
    "            fname_cosmo_lowz = \\\n",
    "                      '/media/RESSPECT/data/PLAsTiCC/for_metrics/ddf/M0DIF/omprior_0.01_flat/' + \\\n",
    "                     'test_salt2mu_lowz_withbias_' + case[:-4] + '.M0DIF.cospar'\n",
    "    elif field == 'WFD':\n",
    "        fname_cosmo_lowz = '/media/RESSPECT/data/PLAsTiCC/for_metrics/wfd/emille_samples' + str(k) + '/M0DIF/' \\\n",
    "                        + '/test_salt2mu_lowz_withbias_' + case[:-4] + '.M0DIF.cospar'\n",
    "        \n",
    "    cosmofit_lowz = pd.read_csv(fname_cosmo_lowz, delim_whitespace=True,\n",
    "                          comment='#', names=['w', 'wsig_marg',  'OM',  'OM_sig',  'chi2',  \n",
    "                                              'Ndof',  'sigint', 'wran',  'OMran',  'label'])\n",
    "    wfit_w_all_lowz.append(cosmofit_lowz['w'].values[0])\n",
    "    wfit_wsig_all_lowz.append(cosmofit_lowz['wsig_marg'].values[0])\n",
    "    wfit_om_all_lowz.append(cosmofit_lowz['OM'].values[0])\n",
    "    wfit_omsig_all_lowz.append(cosmofit_lowz['OM_sig'].values[0])\n",
    "\n",
    "    if field == 'DDF':\n",
    "        if alex:\n",
    "            fname_stan = case[:-4] + '/Alex_samples/omprior_0.01_flat/results/stan_summary_' + case[:-4] + '_lowz_withbias.dat'\n",
    "        else:\n",
    "            fname_stan = '/media/RESSPECT/data/PLAsTiCC/for_metrics/ddf/stan_summary/omprior_0.01_flat/stan_summary_' + \\\n",
    "                      case[:-4] + '_lowz_withbias.dat'\n",
    "    elif field == 'WFD':\n",
    "        fname_stan = '/media/RESSPECT/data/PLAsTiCC/for_metrics/wfd/emille_samples' + str(k) + '/stan_summary/' \\\n",
    "                 + 'stan_summary_' + case[:-4] + '_lowz_withbias.dat'\n",
    "\n",
    "    \n",
    "    op2 = open(fname_stan, 'r')\n",
    "    lin2 = op2.readlines()\n",
    "    op2.close()\n",
    "    \n",
    "    for j in range(len(lin2)):\n",
    "        if lin2[j][:2] == 'om':\n",
    "            om = lin2[j].split()[1]\n",
    "            omsig = lin2[j].split()[3]\n",
    "            stan_om_all_lowz.append(om)\n",
    "            stan_omsig_all_lowz.append(omsig)\n",
    "            \n",
    "            #print('om = ', om)\n",
    "            #print('omsig = ', omsig)\n",
    "            \n",
    "            \n",
    "        elif lin2[j][0] == 'w':\n",
    "            w = lin2[j].split()[1]\n",
    "            wsig = lin2[j].split()[3]\n",
    "            stan_w_all_lowz.append(w)\n",
    "            stan_wsig_all_lowz.append(wsig)\n",
    "            \n",
    "    # classification metrics\n",
    "            \n",
    "    Ia_code = 11\n",
    "        \n",
    "    flag_Ia = np.array(stats[0]) == Ia_code\n",
    "    \n",
    "    pop[Ia_code] = stats[1][flag_Ia][0]\n",
    "    perc[Ia_code] = round(100 * stats[1][flag_Ia][0]/data.shape[0])\n",
    "    \n",
    "    if  len(stats[0]) == 2:\n",
    "        other_code = [item for item in stats[0] if item != Ia_code][0]\n",
    "        pop[other_code] = stats[1][~flag_Ia][0]\n",
    "        perc[other_code] = 100 * stats[1][~flag_Ia][0]/data.shape[0]\n",
    "            \n",
    "        pop_nIa_all.append(pop[other_code])\n",
    "        perc_nIa_all.append(perc[other_code])\n",
    "        other_index.append(other_code)\n",
    "        other_name.append(SNANA_names[other_code])\n",
    "        \n",
    "    elif len(stats[0]) > 2:\n",
    "        other_code = [item for item in stats[0] if item != Ia_code]\n",
    "        for item in range(flag_Ia.shape[0]):\n",
    "            if not flag_Ia[item]:\n",
    "                pop[stats[0][item]] = stats[1][item]\n",
    "                perc[stats[0][item]] = round(100 * stats[1][item]/data.shape[0])\n",
    "                \n",
    "        pop_nIa_all.append([pop[item] for item in other_code])\n",
    "        perc_nIa_all.append([perc[item] for item in other_code])\n",
    "        other_index.append(other_code)\n",
    "        other_name.append([SNANA_names[i] for i in other_code])\n",
    "        \n",
    "    elif len(stats[0]) == 1:\n",
    "        other_code = '--'\n",
    "        pop_nIa_all.append(None)\n",
    "        perc_nIa_all.append(None)\n",
    "        other_index.append(None)\n",
    "        other_name.append(None)\n",
    "\n",
    "    pop_Ia_all.append(pop[Ia_code])\n",
    "    perc_Ia_all.append(perc[Ia_code])\n",
    "    \n",
    "    \n",
    "#     if '6' in case:\n",
    "#         tot = 6000\n",
    "#     elif '5' in case:\n",
    "#         tot = 1500\n",
    "#     elif '1000' in case:\n",
    "#         tot = 1000\n",
    "#     else:\n",
    "    tot = 3000\n",
    "    \n",
    "# this is where number of contaminants gets converted to fraction\n",
    "    cont = tot - pop[Ia_code]\n",
    "    \n",
    "    if field == 'DDF':\n",
    "        for i in range(wdist_ddf.shape[0]):\n",
    "            if case[:-4] in wdist_ddf['FileA'].values[i] or \\\n",
    "                case[:-4] in wdist_ddf['FileB'].values[i]:\n",
    "                wdist_median.append(wdist_ddf['WassersteinDistanceMedian'].values[i])\n",
    "                break\n",
    "    elif field == 'WFD':\n",
    "        if k in [1, '', 2, 3,4,5]:\n",
    "            for i in range(wdist_wfd.shape[0]):\n",
    "                if case[:-4] in wdist_wfd['FileA'].values[i] or \\\n",
    "                    case[:-4] in wdist_wfd['FileB'].values[i]:\n",
    "                    wdist_median.append(wdist_wfd['WassersteinDistanceMedian'].values[i])\n",
    "                    break\n",
    "        else:\n",
    "            wdist_median.append(-99)\n",
    "    \n",
    "    if tot == 3000:\n",
    "        metrics = classification_metrics(cont)\n",
    "    else: \n",
    "        print((cont, case))\n",
    "        metrics = [np.nan, np.nan, np.nan, np.nan, np.nan]\n",
    "    if not metrics:\n",
    "        print((cont, case))\n",
    "    acc.append(metrics[0])\n",
    "    eff.append(metrics[1])\n",
    "    pur.append(metrics[2])\n",
    "    f1.append(metrics[3])\n",
    "    f3.append(metrics[4])\n",
    "        \n",
    "data_all = {}\n",
    "data_all['case'] = names\n",
    "data_all['other_name'] = other_name\n",
    "data_all['other_code'] = other_index\n",
    "data_all['nIa'] = pop_Ia_all\n",
    "data_all['nothers'] = pop_nIa_all\n",
    "data_all['perc_Ia'] = perc_Ia_all\n",
    "data_all['perc_others'] = perc_nIa_all\n",
    "data_all['accuracy'] = acc\n",
    "data_all['efficiency'] = eff\n",
    "data_all['purity'] = pur\n",
    "data_all['f1'] = f1\n",
    "data_all['fom3'] = f3\n",
    "data_all['wfit_w_lowz'] = wfit_w_all_lowz\n",
    "data_all['wfit_wsig_lowz'] = wfit_wsig_all_lowz\n",
    "data_all['wfit_om_lowz'] = wfit_om_all_lowz\n",
    "data_all['wfit_omsig_lowz'] = wfit_omsig_all_lowz\n",
    "data_all['stan_w_lowz'] = stan_w_all_lowz\n",
    "data_all['stan_wsig_lowz'] = stan_wsig_all_lowz\n",
    "data_all['stan_om_lowz'] = stan_om_all_lowz\n",
    "data_all['stan_omsig_lowz'] = stan_omsig_all_lowz\n",
    "data_all['WassersteinDistanceMedian'] = wdist_median\n",
    "\n",
    "data_all = pd.DataFrame(data_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate KLD \n",
    "\n",
    "This is the slowest step\n",
    "\n",
    "TODO: change to 1D posteriors in calculation, check that it doesn't affect ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # add KLD\n",
    "# if True:\n",
    "#     fname1 = '/media/RESSPECT/data/PLAsTiCC/for_metrics/ddf/posteriors/klds.csv'\n",
    "#     data_kld_ddf = pd.read_csv(fname1)\n",
    "\n",
    "#     kld_ddf_column = []\n",
    "#     for i in range(data_all.shape[0]):\n",
    "#         name = data_all.iloc[i]['case']\n",
    "    \n",
    "#         found = False\n",
    "#         for j in range(data_kld_ddf.shape[0]):\n",
    "#             if name in data_kld_ddf['path'].iloc[j]:\n",
    "#                 kld = data_kld_ddf['KLD'].iloc[j]\n",
    "#                 kld_ddf_column.append(kld)\n",
    "#                 found = True\n",
    "            \n",
    "#         if not found:\n",
    "#             kld_ddf_column.append(-99)\n",
    "        \n",
    "#     data_all['KLD'] = kld_ddf_column\n",
    "# else:\n",
    "#     data_all['KLD'] = -99\n",
    "\n",
    "path_pre = dirname[:46]\n",
    "path_post = 'posteriors/samples_emille'+str(k)+'/'\n",
    "refname = 'chains_perfect3000_lowz_withbias.csv.gz'\n",
    "\n",
    "# fieldstr = {'DDF': 'ddf', 'WFD': 'wfd'}\n",
    "\n",
    "fullpath = path_pre + path_post\n",
    "\n",
    "alloutputs = pd.DataFrame(columns=['case', 'KLD'])\n",
    "for case in cases:\n",
    "#     fullpath = dirname\n",
    "    # make reference sample\n",
    "    # make comparison samples\n",
    "#     allfn = os.scandir(fullpath)\n",
    "#     for entry in allfn:\n",
    "#         if entry.is_file() and entry.name[-4:] != '.csv':\n",
    "    samppath = fullpath+'chains_'+case[:-4]+'_lowz_withbias.csv.gz'\n",
    "    with gzip.open(samppath) as sampfile:\n",
    "        sampdata = pd.read_csv(sampfile)\n",
    "        [w_comp, Omm_comp] = [sampdata['w'], sampdata['om']]\n",
    "        kde_comp = make_kde(grid_ref['w'], grid_ref['Omm'], w_comp, Omm_comp)\n",
    "        the_kld = calculate_kld(kde_ref, kde_comp, np.array([d_ref['w'], d_ref['Omm']]))\n",
    "        newrow = {'case': case[:-4], 'KLD': the_kld}\n",
    "        print((newrow, type(newrow['KLD'])))\n",
    "        alloutputs = alloutputs.append(newrow, ignore_index=True)\n",
    "#     if case[:-4] == '90SNIa10SNIax':\n",
    "#         plt.hist2d(w_ref, Omm_ref, bins=[w_vec, Omm_vec], density=True, cmap=plt.cm.Blues, alpha=0.5)\n",
    "#         plt.hist2d(w_comp, Omm_comp, bins=[w_vec, Omm_vec], density=True, cmap=plt.cm.Reds, alpha=0.5)\n",
    "#         plt.show()\n",
    "#         data_all[data_all['case'] == case]['KLD'] = the_kld\n",
    "# alloutputs.to_csv(fullpath+'klds.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### put it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = data_all.join(alloutputs.set_index('case'), on='case')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save results!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if True:\n",
    "if field == 'DDF':\n",
    "    if alex:\n",
    "        data_all.to_csv('summary_cases_omprior_0.01_flat_alex.csv', index=False)\n",
    "        data_all.to_csv('/media2/RESSPECT2/data/posteriors_ddf/omprior_0.01_flat/summary_cases_alex.csv', index=False)\n",
    "    elif redo:\n",
    "        data_all.to_csv('summary_cases_omprior_0.01_flat_redone.csv', index=False)\n",
    "        data_all.to_csv('/media2/RESSPECT2/data/posteriors_ddf/omprior_0.01_flat/summary_cases_redone.csv', index=False)\n",
    "    else:\n",
    "        data_all.to_csv('summary_cases_omprior_0.01_flat_emille.csv', index=False)\n",
    "        data_all.to_csv('/media2/RESSPECT2/data/posteriors_ddf/omprior_0.01_flat/summary_cases_emille.csv', index=False)\n",
    "elif field == 'WFD':\n",
    "    if redo:\n",
    "        data_all.to_csv('summary_cases_omprior_0.01_flat_redone' + str(k) +'.csv', index=False)\n",
    "        data_all.to_csv('/media2/RESSPECT2/data/posteriors_wfd/omprior_0.01_flat/summary_cases_omprior_0.01_flat_redone' + \\\n",
    "                str(k) + '.csv', index=False)\n",
    "    else:\n",
    "        data_all.to_csv('summary_cases_omprior_0.01_flat_emille' + str(k) +'.csv', index=False)\n",
    "        data_all.to_csv('/media2/RESSPECT2/data/posteriors_wfd/omprior_0.01_flat/summary_cases_omprior_0.01_flat_emille' + \\\n",
    "                str(k) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot results\n",
    "\n",
    "see other notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sns.set(style='white')\n",
    "\n",
    "# #c=perc_Ia\n",
    "# #markershape=other_name\n",
    "\n",
    "# plt.title('DDF')\n",
    "# plt.scatter(data_all['fom3'], data_all['WassersteinDistanceMedian'], \n",
    "#             c=1.-data_all['perc_Ia'], marker='x', alpha=0.5)\n",
    "# # plt.xlim(0.725, 1.025)\n",
    "# plt.xlabel('fom', fontsize=20)\n",
    "# plt.ylabel('Wasserstein Distance')\n",
    "# plt.savefig('DDF_Wasserstein_v_FOM.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.title('DDF')\n",
    "# plt.scatter(data_all['fom3'], data_all['KLD'], \n",
    "#             c=1.-data_all['perc_Ia'], marker='x', alpha=0.5)\n",
    "# plt.semilogy()\n",
    "# # plt.xlim(0., 1.05)\n",
    "# plt.xlabel('fom', fontsize=20)\n",
    "# plt.ylabel('KLD')\n",
    "# plt.savefig('DDF_KLD_v_FOM.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# plt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(16,5))\n",
    "\n",
    "# plt.subplot(1,2,1)\n",
    "# plt.hist(data_all['stan_w_lowz'].values.astype(float), color='green', label='stan', alpha=0.5)\n",
    "# plt.hist(data_all['wfit_w_lowz'].values.astype(float), color='brown', label='wfit', alpha=0.3)\n",
    "# plt.xlabel('w', fontsize=14)\n",
    "# plt.ylabel('N', fontsize=14)\n",
    "# plt.legend()\n",
    "\n",
    "# plt.subplot(1,2,2)\n",
    "# plt.hist(data_all['stan_wsig_lowz'].values.astype(float), color='green', alpha=0.5, label='stan')\n",
    "# plt.hist(data_all['wfit_wsig_lowz'].values.astype(float), color='brown', label='wfit', alpha=0.3)\n",
    "# plt.xlabel('wsig_from_stan_with_lowz', fontsize=14)\n",
    "# plt.ylabel('N', fontsize=14)\n",
    "# plt.legend()\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from astropy.cosmology import FlatLambdaCDM\n",
    "    \n",
    "# cosmo = FlatLambdaCDM(H0=72, Om0=0.3)\n",
    "# theor_dist = [cosmo.distmod(z).value for z in np.arange(0.001,1.5,0.005)]\n",
    "\n",
    "# for name in cases[:2]:\n",
    "    \n",
    "#     fname_cosmo_lowz = '/media/RESSPECT/data/PLAsTiCC/for_metrics/ddf/M0DIF/omprior_0.01_flat/' + \\\n",
    "#                        'test_salt2mu_lowz_withbias_' + name[:-4] + '.M0DIF.cospar'\n",
    "    \n",
    "#     cosmofit = pd.read_csv(fname_cosmo_lowz, delim_whitespace=True,\n",
    "#                           comment='#', names=['w', 'wsig_marg',  'OM',  'OM_sig',  'chi2',  \n",
    "#                                               'Ndof',  'sigint', 'wran',  'OMran',  'label'])\n",
    "    \n",
    "#     fname_fitres = '/media/RESSPECT/data/PLAsTiCC/for_metrics/ddf/fitres/test_salt2mu_lowz_withbias_' + name[:-4] + '.fitres'\n",
    "#     fitres = pd.read_csv(fname_fitres, comment='#', delim_whitespace=True)\n",
    "    \n",
    "#     flag = np.logical_or(fitres['SIM_TYPE_INDEX'].values == 11, fitres['SIM_TYPE_INDEX'].values == 1)\n",
    "#     z = fitres['SIM_ZCMB'].values\n",
    "#     mu = fitres['MU'].values\n",
    "#     muerr = fitres['MUERR'].values\n",
    "    \n",
    "    \n",
    "#     fig = plt.figure(figsize=(10,8))\n",
    "        \n",
    "#     ax = fig.add_subplot(111)\n",
    "#     ax.set_title(name, fontsize=26)\n",
    "\n",
    "#     if sum(flag) > 0:\n",
    "#         plt.errorbar(z[flag], mu[flag], yerr=muerr[flag], fmt='o', alpha=0.1, label='spec-Ia', color='blue')\n",
    "    \n",
    "#     if sum(~flag) > 0:\n",
    "#         plt.errorbar(z[~flag], mu[~flag], yerr=muerr[~flag], fmt='^', alpha=0.1, label='photo-Ia', color='green')\n",
    "       \n",
    "#     plt.plot(np.arange(0.001, 1.5,0.005), theor_dist, label='w = -1', color='red')\n",
    "\n",
    "#     w = str(cosmofit['w'].values[0])\n",
    "#     if len(w) >= 6:\n",
    "#         w1 = w[:6]\n",
    "#     else:\n",
    "#         w1 = w.ljust(6, '0')\n",
    "            \n",
    "#     werr = str(cosmofit['wsig_marg'].values[0])\n",
    "#     if len(werr) >= 6:\n",
    "#         werr1 = werr[:5]\n",
    "#     else:\n",
    "#         werr1 = werr.ljust(5, '0')\n",
    "            \n",
    "#     flag_case = data_all['case'].values == name[:-4]\n",
    "#     ax.text(0.2, 32, 'stan = ' + str(data_all[flag_case]['stan_w_lowz'].values[0]) + r' $\\pm$ ' + \\\n",
    "#             str(data_all[flag_case]['stan_wsig_lowz'].values[0]), fontsize=20)\n",
    "#     ax.text(0.2, 30, r'wfit = ' + w1 + r' $\\pm$ ' + werr1 , fontsize=20)\n",
    "        \n",
    "#     ax.set_xlabel('redshift', fontsize=22)\n",
    "#     ax.set_ylabel('mu', fontsize=22)\n",
    "#     plt.legend(fontsize=22, loc='lower right')\n",
    "\n",
    "    \n",
    "#     #plt.savefig('plots/distances/omprior_0.01_flat/dist_' + name[:-4] + '.png')\n",
    "#     #plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save samples for Alberto"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "flist = os.listdir('.')\n",
    "flist.remove('summary_cases_omprior_0.01_flat_emille.csv')\n",
    "flist.remove('summary.ipynb')\n",
    "flist.remove('plots')\n",
    "flist.remove('.ipynb_checkpoints')\n",
    "flist.remove('summary_cases_omprior_0.01_flat.csv')\n",
    "\n",
    "for case in flist:\n",
    "\n",
    "    if os.path.isdir(case + '/test_mysamples/omprior_0.01_flat/'):\n",
    "        name = '/media/emille/git/COIN/RESSPECT_work/PLAsTiCC/metrics_paper/resspect_metric/posteriors/DDF/' + case + '/' + \\\n",
    "               'test_mysamples/omprior_0.01_flat/chains/chains_' + case + '_lowz_withbias.pkl'\n",
    "        data = pd.read_pickle(name)\n",
    "        data2 = pd.DataFrame(data)\n",
    "\n",
    "        data2.to_csv('/media/RESSPECT/data/PLAsTiCC/for_metrics/ddf/posteriors/samples_emille/' + \\\n",
    "                    'chains_'  + case + '_lowz_withbias.csv.gz', index=False)\n",
    "        \n",
    "    else:\n",
    "        print(case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Emille]: I would not consider the cell above because the only thing it does is to copy the files\n",
    "\n",
    "all files are accessible at `/media/RESSPECT/data/PLAsTiCC/for_metrics/ddf/posteriors/samples_emille/`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recidivator (Python 3)",
   "language": "python",
   "name": "recidivator_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
