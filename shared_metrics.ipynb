{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating and saving all metrics\n",
    "\n",
    "_Alex I. Malz (GCCL@RUB) & Emille Ishida (LPC@Clermont-Ferrand)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "# import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import gzip\n",
    "import scipy.stats as sps\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class naming scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNANA_names = {11: 'Ia', 3:'Ibc', 13: 'Ibc', 2:'II', 12:'II', 14:'II',\n",
    "               41: '91bg', 43:'Iax', 51:'KN', 60:'SLSN', 61:'PISN', 62:'ILOT',\n",
    "               63:'CART', 64:'TDE', 70:'AGN', 80:'RRL', 81:'M-dwarf', 83:'EB',\n",
    "               84:'Mira', 90:'MicroLB', 91:'MicroL', 93:'MicroL'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how to name saved files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # these are for what to name saved files\n",
    "# alex = False\n",
    "# redo = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which test cas to run (because one big loop would be awful, although this should really be a script that runs them in parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = '0'\n",
    "field = 'WFD'\n",
    "nobj = '3000'\n",
    "    \n",
    "dirname = '/media/RESSPECT/data/PLAsTiCC/for_metrics/final_data3/'+field+'/results/v'+k+'/'+nobj+'/'\n",
    "sampdir = dirname + 'samples/'\n",
    "cases = os.listdir(sampdir)\n",
    "print(cases)\n",
    "\n",
    "# cases.remove('random1500.csv')\n",
    "# cases.remove('random6000.csv')\n",
    "# cases.remove('fiducial1500.csv')\n",
    "# cases.remove('fiducial6000.csv')\n",
    "# cases.remove('perfect6000.csv')\n",
    "# cases.remove('perfect1500.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "manually remove cases that won't be in the same big plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## how to calculate classification metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import proclam\n",
    "from proclam.metrics.util import *\n",
    "from proclam.metrics.util import RateMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class for binary classification metrics\n",
    "\n",
    "TODO: add in checks for between 0 and 1\n",
    "\n",
    "TODO: put this in proclam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class det_mets(RateMatrix):\n",
    "    \"binary classification metrics\"\n",
    "    def __init__(self, **rates):\n",
    "        \"\"\"\n",
    "        Call like `thing = det_mets(**rates._asdict())`\n",
    "        \"\"\"\n",
    "#         self.rates = rates#.asdict()\n",
    "        self._get_tots()\n",
    "        self._from_rates()\n",
    "        self._sn_mets()\n",
    "        self._translate()\n",
    "    def _get_tots(self):\n",
    "        self.CP = self.TP + self.FN\n",
    "        self.CN = self.TN + self.FP\n",
    "        self.T = self.TP + self.TN\n",
    "        self.F = self.FP + self.FN\n",
    "        self.P = self.TP + self.FP\n",
    "        self.N = self.TN + self.FN\n",
    "    def _from_rates(self):\n",
    "        self.PPV = self.TP / (self.TP + self.FP)\n",
    "        self.NPV = self.TN / (self.TN + self.FN)\n",
    "        self.PT = (np.sqrt(self.TPR * (1. - self.TNR)) + self.TNR - 1.) / (self.TPR + self.TNR - 1.)\n",
    "        self.TS = self.TP / (self.TP + self.FN + self.FP)\n",
    "        self._derived()\n",
    "    def _derived(self):\n",
    "        self.ACC = (self.TP + self.TN) / (self.CP + self.CN)\n",
    "        self.BA = (self.TPR + self.TNR) / 2.,\n",
    "        self.F1S = 2. * self.PPV * self.TPR / (self.PPV + self.TPR)\n",
    "        self.MCC = (self.TP * self.TN - self.FP * self.FN) / (np.sqrt(self.P * self.CP * self.CN * self.N))\n",
    "        self.FM = np.sqrt(self.PPV * self.TPR)\n",
    "        self.BM = self.TPR + self.TNR - 1.\n",
    "        self.MK = self.PPV + self.NPV - 1.\n",
    "    def _translate(self):\n",
    "        self.positive = self.CP\n",
    "        self.negative = self.CN\n",
    "        self.sensitivity = self.TPR\n",
    "        self.recall = self.TPR\n",
    "        self.specificity = self.TNR\n",
    "        self.selectivity = self.TNR\n",
    "        self.precision = self.PPV\n",
    "        self.FDR = 1. - self.PPV\n",
    "        self.FOR = 1. - self.NPV\n",
    "        self.CSI = self.TS\n",
    "        self.accuracy = self.ACC\n",
    "        self.f1_score = self.F1S\n",
    "        self.informedness = self.BM\n",
    "        self.deltaP = self.MK\n",
    "    def _sn_mets(self):\n",
    "        self.get_efficiency()\n",
    "        self.get_purity()\n",
    "    def get_efficiency(self):\n",
    "        self.efficiency = self.TP / self.CP\n",
    "        return self.efficiency\n",
    "    def get_purity(self):\n",
    "        self.purity = self.TP / self.P\n",
    "        return self.purity\n",
    "    def get_fom(self, penalty):\n",
    "        self.pseudo_purity = self.TP / (self.TP + penalty * self.FP)\n",
    "        fom = self.pseudo_purity * self.efficiency\n",
    "        return fom\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make the proclam ratematrix from information we know about each test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nums_to_rate(tot, P, T, FP):\n",
    "    N = tot - P\n",
    "    TN = N - FP\n",
    "    F = tot - T\n",
    "    FN = F - TN\n",
    "    TP = P - FN\n",
    "    assert(FP + FN + TP + TN == tot)\n",
    "    \n",
    "    TPR = TP / P\n",
    "    FPR = FP / N\n",
    "    FNR = FN / P\n",
    "    TNR = TN / N\n",
    "    \n",
    "#     cm = np.array([[totIa - cont, totall - ], [, cont]])\n",
    "    rate = proclam.util.RateMatrix(TPR=TPR, FPR=FPR, FNR=FNR, TNR=TNR, TP=TP, FN=FN, TN=TN, FP=FP)\n",
    "    return rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define the true number of objects available in each sample before and after SALT2 fit as filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = {}\n",
    "total['DDF'] = {}                     # total number of objects in the sample\n",
    "# total['DDF']['before_salt2'] = 4335\n",
    "total['DDF']['after_salt2'] = int(9621 / 11236 * 3000) #3456\n",
    "total['WFD'] = {}\n",
    "# total['WFD']['before_salt2'] = 5588\n",
    "total['WFD']['after_salt2'] = int(999789 / 1094829 * 3000) #3306"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate the metrics from the available numbers of objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_metrics(cont, totIa=3000, before_salt2=False, field=field):\n",
    "    \"\"\"Classification metrics for a sample of 3k SNIa.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    cont: int < totIa\n",
    "        number of contaminant objects\n",
    "    totIa: int (optional)\n",
    "        Number of Ia in the sample. Default is 3000.\n",
    "    before_salt2: bool (optional)\n",
    "        If True use total sample number before SALT2 fit.\n",
    "        Default is False.\n",
    "    field: str (optional)\n",
    "        Cadence: 'DDF' or 'WFD'. Default is DDF.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    accuracy: float\n",
    "    efficiency: float\n",
    "    purity: float\n",
    "    figure of merit (W=1): float\n",
    "    figure of merit (W=3): float\n",
    "    \"\"\"\n",
    "    \n",
    "    if totIa != 3000:\n",
    "        raise ValueError('Numbers are hard coded for 3000 SNIa.')\n",
    "#         return(np.nan, np.nan, np.nan, np.nan, np.nan)\n",
    "    \n",
    "    if before_salt2:\n",
    "        totall = total[field]['before_salt2']\n",
    "    else:\n",
    "        totall = total[field]['after_salt2']\n",
    "    \n",
    "    rate = nums_to_rate(tot=totall, P=totIa, T=totIa, FP=cont)._asdict()\n",
    "    class_mets = det_mets(**rate)\n",
    "    \n",
    "    f3 = class_mets.get_fom(3.)\n",
    "\n",
    "    return class_mets.accuracy, class_mets.get_efficiency(), class_mets.get_purity(), class_mets.f1_score, f3#class_mets.get_fom(3.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## how to calculate KLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KLD is prone to numerical divergence when coverage between distributions is poor, so protect against overflow/underflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 2. * sys.float_info.min\n",
    "\n",
    "def safe_log(arr, threshold=eps):\n",
    "    \"\"\"\n",
    "    Takes the natural logarithm of an array that might contain zeros.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr: ndarray, float\n",
    "        array of values to be logged\n",
    "    threshold: float, optional\n",
    "        small, positive value to replace zeros and negative numbers\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    logged: ndarray\n",
    "        logged values, with small value replacing un-loggable values\n",
    "    \"\"\"\n",
    "    arr = np.asarray(arr)\n",
    "    arr[arr < threshold] = threshold\n",
    "    logged = np.log(arr)\n",
    "    return logged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how to make a grid and fit ~~multidimensional~~ KDE to posterior samples (of $w$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_grid(x, y, x_ngrid=100, y_ngrid=100):\n",
    "    x_min = x.min()#-1.2\n",
    "    x_max = x.max()#-0.8\n",
    "    y_min = y.min()#0.2\n",
    "    y_max = y.max()#0.4\n",
    "\n",
    "    x_grid, y_grid = np.mgrid[x_min:x_max:x_ngrid*1.j, y_min:y_max:y_ngrid*1.j]\n",
    "    x_vec, y_vec = x_grid[:, 0], y_grid[0, :]\n",
    "    dx = (x_max - x_min) / (x_ngrid - 1)\n",
    "    dy = (y_max - y_min) / (y_ngrid - 1)\n",
    "\n",
    "    return(((x_min, y_min), (x_max, y_max)), (x_grid, y_grid), (x_vec, y_vec), (dx, dy))\n",
    "\n",
    "def make_kde(Xgrid, Ygrid, Xsamps, Ysamps, to_log=False, save=None, one_d=True):\n",
    "    if not one_d:\n",
    "        positions = np.vstack([Xgrid.ravel(), Ygrid.ravel()])\n",
    "        values = np.vstack([Xsamps, Ysamps])\n",
    "        kernel = sps.gaussian_kde(values, bw_method='scott')\n",
    "        Z = np.reshape(kernel(positions).T, Xgrid.shape)\n",
    "    else:\n",
    "        positions = Xgrid.T[0]\n",
    "        values = Xsamps\n",
    "        kernel = sps.gaussian_kde(values, bw_method='scott')\n",
    "        Z = kernel(positions)\n",
    "    \n",
    "    if to_log:\n",
    "        return safe_log(Z)\n",
    "    else:\n",
    "        return Z\n",
    "#     if save is not None:\n",
    "# TODO: normalize up here before log!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use chippr implementation of KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stolen from chippr\n",
    "def calculate_kld(lpe, lqe, dx, from_log=False, vb=True):\n",
    "    \"\"\"\n",
    "    Calculates the Kullback-Leibler Divergence between two N-dimensional PDFs \n",
    "    evaluated on a shared, regular grid (sorry, too lazy to deal with irregular grid)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lpe: numpy.ndarray, float\n",
    "        log-probability distribution evaluated on a grid whose distance from `q`\n",
    "        will be calculated.\n",
    "    lqe: numpy.ndarray, float\n",
    "        log-probability distribution evaluated on a grid whose distance to `p` will\n",
    "        be calculated.\n",
    "    dx: numpy.ndarray, float\n",
    "        separation of grid values in each dimension\n",
    "    from_log: boolean, optional\n",
    "        if False, lpe, lqe are probability distributions, not log-probability distributions\n",
    "    vb: boolean, optional\n",
    "        report on progress to stdout?\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dpq: float\n",
    "        the value of the Kullback-Leibler Divergence from `q` to `p`\n",
    "    \"\"\"\n",
    "    # Normalize the evaluations, so that the integrals can be done\n",
    "    gridnorm = np.ones_like(lpe) * np.prod(dx)\n",
    "    if from_log:\n",
    "        pe = np.exp(lpe)\n",
    "        qe = np.exp(lqe)\n",
    "#     print(np.prod(dx))\n",
    "#     print(gridnorm)\n",
    "    else:\n",
    "        pe = lpe\n",
    "        qe = lqe\n",
    "    pi = np.sum(pe * gridnorm)\n",
    "    qi = np.sum(qe * gridnorm)\n",
    "    # (very approximately!) by simple summation:\n",
    "    pn = pe / pi\n",
    "    qn = qe / qi\n",
    "    # Compute the log of the normalized PDFs\n",
    "    logp = safe_log(pn)\n",
    "    logq = safe_log(qn)\n",
    "    # Calculate the KLD from q to p\n",
    "    Dpq = np.sum(pn * (logp - logq))\n",
    "#     if np.isnan(Dpq):\n",
    "#         return((lpe, lqe, dx))\n",
    "    return Dpq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep for the metrics calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set up for saving the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "\n",
    "pop_Ia_all = []\n",
    "pop_nIa_all = []\n",
    "perc_Ia_all = []\n",
    "perc_nIa_all = []\n",
    "\n",
    "wfit_w_all = []\n",
    "wfit_wsig_all = []\n",
    "wfit_om_all = []\n",
    "wfit_omsig_all = []\n",
    "wfit_w_all_lowz = []\n",
    "wfit_wsig_all_lowz = []\n",
    "wfit_om_all_lowz = []\n",
    "wfit_omsig_all_lowz = []\n",
    "\n",
    "stan_w_all = []\n",
    "stan_wsig_all = []\n",
    "stan_om_all = []\n",
    "stan_omsig_all = []\n",
    "\n",
    "stan_w_all_lowz = []\n",
    "stan_wsig_all_lowz = []\n",
    "stan_om_all_lowz = []\n",
    "stan_omsig_all_lowz = []\n",
    "\n",
    "other_index = []\n",
    "other_name = []\n",
    "\n",
    "wdist_median = []\n",
    "\n",
    "eff = []\n",
    "pur = []\n",
    "acc = []\n",
    "f1 = []\n",
    "f3 = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read Wasserstein distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if field == 'DDF':\n",
    "#     fname = dirname + 'Wasserstein/wassersteinDistances_ddf'+k+'.dat'\n",
    "# elif field == 'WFD':\n",
    "#     fname = dirname + 'Wasserstein/wassersteinDistances_WFD'+k+'.dat'\n",
    "\n",
    "# wdist = pd.read_csv(fname, index_col=1)\n",
    "# wdist['FileB'][wdist['FileB'] == 'chains_perfect3000_lowz_withbias.csv.gz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(wdist)\n",
    "# # name_flagA = np.array(['perfect3000' in name for name in wdist['FileA'].values])\n",
    "# name_flagB = np.array(['perfect3000' in name for name in wdist['FileB'].values])\n",
    "# # name_flag = np.logical_or(name_flagA, name_flagB)\n",
    "\n",
    "# wdist = wdist[name_flagB]\n",
    "# # if field == 'DDF':\n",
    "# #     wdist_ddf = wdist[name_flag]\n",
    "# # elif field == 'WFD':\n",
    "# #     wdist_wfd = wdist[name_flag]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prep KDE for reference (closest to truth) distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_pre = dirname#[:46]\n",
    "path_post = 'posteriors/csv/'\n",
    "refname = 'chains_perfect3000_lowz_withbias.csv.gz'\n",
    "\n",
    "# fieldstr = {'DDF': 'ddf', 'WFD': 'wfd'}\n",
    "\n",
    "fullpath = path_pre + path_post\n",
    "alloutputs = pd.DataFrame(columns=['path', 'KLD'])\n",
    "    # make reference sample\n",
    "with gzip.open(fullpath+refname) as reffn:\n",
    "    flatref = pd.read_csv(reffn)\n",
    "[w_ref, Omm_ref] = [flatref['w'], flatref['om']]\n",
    "ref_extrema, ref_grids, ref_vecs, ref_ds = make_grid(w_ref, Omm_ref)\n",
    "(w_vec, Omm_vec) = ref_vecs\n",
    "(dw, dOmm) = ref_ds\n",
    "((xmin, ymin), (xmax, ymax)) = ref_extrema\n",
    "(w_grid, Omm_grid) = ref_grids\n",
    "d_ref = {'w': dw, 'Omm': dOmm}\n",
    "grid_ref = {'w': w_grid, 'Omm': Omm_grid}\n",
    "kde_ref = make_kde(w_grid, Omm_grid, w_ref, Omm_ref, one_d=True, to_log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "diagnostic plots for the posterior samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# stolen from chippr\n",
    "def fancyplot(xdata, ydata, xgrid, ygrid, xlab='', ylab='', title=''):\n",
    "    # 2d KDE\n",
    "    f, scatplot = plt.subplots(figsize=(7.5, 7.5))\n",
    "    f.subplots_adjust(hspace=0)\n",
    "    scatplot.hist2d(xdata, ydata, bins=[xgrid, ygrid], alpha=0.5, density=True, cmap=plt.cm.Blues)\n",
    "    \n",
    "    scatplot.set_xlabel(xlab, fontsize=20)\n",
    "    scatplot.set_ylabel(ylab, fontsize=20)\n",
    "    divider = make_axes_locatable(scatplot)\n",
    "    histx = divider.append_axes('top', 1.2, pad=0., sharex=scatplot)\n",
    "    histy = divider.append_axes('right', 1.2, pad=0., sharey=scatplot)\n",
    "    histx.xaxis.set_tick_params(labelbottom=False)\n",
    "    histy.yaxis.set_tick_params(labelleft=False)\n",
    "    histx.hist(xdata, bins=xgrid, alpha=0.75, density=True)\n",
    "    histy.hist(ydata, bins=ygrid, alpha=0.75, density=True, orientation='horizontal')\n",
    "    histx.set_yticks([])\n",
    "    histy.set_xticks([])\n",
    "    scatplot.set_title(title, fontsize=24)\n",
    "    f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fancyplot(Omm_ref, w_ref, Omm_vec, w_vec, xlab=r'$\\Omega_{m}$', ylab=r'$w$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read in cosmology outputs and calculate classification metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for case in cases:\n",
    "\n",
    "    names.append(case[:-4])\n",
    "    \n",
    "    pop = {}\n",
    "    perc = {}\n",
    "\n",
    "    samples_dir = case[:-4] + '/'\n",
    "    \n",
    "    data = pd.read_csv(dirname + 'samples/' + case, header=0, delimiter=' ')   \n",
    "    stats = np.unique(data['SIM_TYPE_INDEX'].values, return_counts=True)\n",
    "    \n",
    "    fname_cosmo_lowz = dirname+'cospar/test_salt2mu_lowz_withbias_'+case[:-4]+'.M0DIF.cospar'\n",
    "    cosmofit_lowz = pd.read_csv(fname_cosmo_lowz, delim_whitespace=True,\n",
    "                          comment='#', names=['w', 'wsig_marg',  'OM',  'OM_sig',  'chi2',  \n",
    "                                              'Ndof',  'sigint', 'wran',  'OMran',  'label'])\n",
    "    wfit_w_all_lowz.append(cosmofit_lowz['w'].values[0])\n",
    "    wfit_wsig_all_lowz.append(cosmofit_lowz['wsig_marg'].values[0])\n",
    "    wfit_om_all_lowz.append(cosmofit_lowz['OM'].values[0])\n",
    "    wfit_omsig_all_lowz.append(cosmofit_lowz['OM_sig'].values[0])\n",
    "\n",
    "    fname_stan = dirname+'stan_summary/stan_summary_'+case[:-4]+'_lowz_withbias.dat'\n",
    "    op2 = open(fname_stan, 'r')\n",
    "    lin2 = op2.readlines()\n",
    "    op2.close()\n",
    "    \n",
    "    for j in range(len(lin2)):\n",
    "        if lin2[j][:2] == 'om':\n",
    "            om = lin2[j].split()[1]\n",
    "            omsig = lin2[j].split()[3]\n",
    "            stan_om_all_lowz.append(om)\n",
    "            stan_omsig_all_lowz.append(omsig)\n",
    "        elif lin2[j][0] == 'w':\n",
    "            w = lin2[j].split()[1]\n",
    "            wsig = lin2[j].split()[3]\n",
    "            stan_w_all_lowz.append(w)\n",
    "            stan_wsig_all_lowz.append(wsig)\n",
    "            \n",
    "    # classification metrics\n",
    "    Ia_code = 11\n",
    "    flag_Ia = np.array(stats[0]) == Ia_code\n",
    "    \n",
    "    pop[Ia_code] = stats[1][flag_Ia][0]\n",
    "    perc[Ia_code] = round(100 * stats[1][flag_Ia][0]/data.shape[0])\n",
    "    \n",
    "    if  len(stats[0]) == 2:\n",
    "        other_code = [item for item in stats[0] if item != Ia_code][0]\n",
    "        pop[other_code] = stats[1][~flag_Ia][0]\n",
    "        perc[other_code] = 100 * stats[1][~flag_Ia][0]/data.shape[0]\n",
    "            \n",
    "        pop_nIa_all.append(pop[other_code])\n",
    "        perc_nIa_all.append(perc[other_code])\n",
    "        other_index.append(other_code)\n",
    "        other_name.append(SNANA_names[other_code])\n",
    "        \n",
    "    elif len(stats[0]) > 2:\n",
    "        other_code = [item for item in stats[0] if item != Ia_code]\n",
    "        for item in range(flag_Ia.shape[0]):\n",
    "            if not flag_Ia[item]:\n",
    "                pop[stats[0][item]] = stats[1][item]\n",
    "                perc[stats[0][item]] = round(100 * stats[1][item]/data.shape[0])\n",
    "                \n",
    "        pop_nIa_all.append([pop[item] for item in other_code])\n",
    "        perc_nIa_all.append([perc[item] for item in other_code])\n",
    "        other_index.append(other_code)\n",
    "        other_name.append([SNANA_names[i] for i in other_code])\n",
    "        \n",
    "    elif len(stats[0]) == 1:\n",
    "        other_code = '--'\n",
    "        pop_nIa_all.append(None)\n",
    "        perc_nIa_all.append(None)\n",
    "        other_index.append(None)\n",
    "        other_name.append(None)\n",
    "\n",
    "    pop_Ia_all.append(pop[Ia_code])\n",
    "    perc_Ia_all.append(perc[Ia_code])\n",
    "    \n",
    "    tot = 3000\n",
    "    \n",
    "    cont = tot - pop[Ia_code]\n",
    "    \n",
    "#     if field == 'DDF':\n",
    "#         for i in range(wdist_ddf.shape[0]):\n",
    "#             if case[:-4] in wdist_ddf['FileA'].values[i] or \\\n",
    "#                 case[:-4] in wdist_ddf['FileB'].values[i]:\n",
    "#                 wdist_median.append(wdist_ddf['WassersteinDistanceMedian'].values[i])\n",
    "#                 break\n",
    "#     elif field == 'WFD':\n",
    "#         if k in [1, '', 2, 3,4,5]:\n",
    "#             for i in range(wdist_wfd.shape[0]):\n",
    "#                 if case[:-4] in wdist_wfd['FileA'].values[i] or \\\n",
    "#                     case[:-4] in wdist_wfd['FileB'].values[i]:\n",
    "#                     wdist_median.append(wdist_wfd['WassersteinDistanceMedian'].values[i])\n",
    "#                     break\n",
    "#         else:\n",
    "#             wdist_median.append(-99)\n",
    "#     for i in range(wdist.shape[0]):     \n",
    "#     if 'chains_'+case[:-4]+'_lowz_withbias.csv.gz' in wdist.index:\n",
    "#         wdist_median.append(wdist.loc['chains_'+case[:-4]+'_lowz_withbias.csv.gz']['WassersteinDistanceMedian'])\n",
    "#     else:\n",
    "#         print((case, 'b0rk!'))\n",
    "#         wdist_median.append(-99)\n",
    "    \n",
    "    if tot == 3000:\n",
    "        metrics = classification_metrics(cont)\n",
    "    else: \n",
    "        print((cont, case))\n",
    "        metrics = [np.nan, np.nan, np.nan, np.nan, np.nan]\n",
    "    if not metrics:\n",
    "        print((cont, case))\n",
    "    acc.append(metrics[0])\n",
    "    eff.append(metrics[1])\n",
    "    pur.append(metrics[2])\n",
    "    f1.append(metrics[3])\n",
    "    f3.append(metrics[4])\n",
    "   \n",
    "data_all = {}\n",
    "\n",
    "res = np.array(other_name)\n",
    "for i, contaminant in enumerate(res):\n",
    "    if type(contaminant) == list:\n",
    "        if len(contaminant) <= 3:\n",
    "            res[i] = contaminant[0]\n",
    "        else:\n",
    "            res[i] = 'mix'\n",
    "    elif contaminant is None:\n",
    "        res[i] = 'n/a'\n",
    "data_all['other_short'] = res\n",
    "        \n",
    "data_all['case'] = names\n",
    "data_all['nIa'] = pop_Ia_all\n",
    "data_all['perc_Ia'] = perc_Ia_all\n",
    "data_all['accuracy'] = acc\n",
    "data_all['efficiency'] = eff\n",
    "data_all['purity'] = pur\n",
    "data_all['f1'] = f1\n",
    "data_all['fom3'] = f3\n",
    "data_all['wfit_w_lowz'] = wfit_w_all_lowz\n",
    "data_all['wfit_wsig_lowz'] = wfit_wsig_all_lowz\n",
    "data_all['wfit_om_lowz'] = wfit_om_all_lowz\n",
    "data_all['wfit_omsig_lowz'] = wfit_omsig_all_lowz\n",
    "data_all['stan_w_lowz'] = stan_w_all_lowz\n",
    "data_all['stan_wsig_lowz'] = stan_wsig_all_lowz\n",
    "data_all['stan_om_lowz'] = stan_om_all_lowz\n",
    "data_all['stan_omsig_lowz'] = stan_omsig_all_lowz\n",
    "# data_all['WassersteinDistanceMedian'] = wdist_median\n",
    "\n",
    "# data_all['other_name'] = other_name\n",
    "# data_all['other_code'] = other_index\n",
    "# data_all['nothers'] = pop_nIa_all\n",
    "# data_all['perc_others'] = perc_nIa_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = np.array(data_all['other_name'].values)\n",
    "# for i, contaminant in enumerate(res):\n",
    "#     if type(contaminant) == list:\n",
    "#         if len(contaminant) <= 3:\n",
    "#             res[i] = contaminant[0]\n",
    "#         else:\n",
    "#             res[i] = 'mix'\n",
    "#     elif contaminant is None:\n",
    "#         res[i] = 'n/a'\n",
    "# data_all['other_short'] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = pd.DataFrame(data_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate KLD and EMD\n",
    "\n",
    "This is the slowest step\n",
    "\n",
    "TODO: add onto fancyplot overplotting each set of posteriors as contours (as in [chippr example](https://github.com/aimalz/chippr/blob/issue/83/referee/research/scripts/pedagogical-final.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colordict = {}\n",
    "for i, contaminant in enumerate(set(res)):\n",
    "    colordict[str(contaminant)] = i / 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_pre = dirname#[:46]\n",
    "path_post = 'posteriors/csv/'\n",
    "refname = 'chains_perfect3000_lowz_withbias.csv'\n",
    "\n",
    "fullpath = path_pre + path_post\n",
    "\n",
    "alloutputs = pd.DataFrame(columns=['case', 'KLD', 'EMD'])\n",
    "for case in cases:\n",
    "    samppath = fullpath+'chains_'+case[:-4]+'_lowz_withbias.csv.gz'\n",
    "    with gzip.open(samppath) as sampfile:\n",
    "        sampdata = pd.read_csv(sampfile)\n",
    "        [w_comp, Omm_comp] = [sampdata['w'], sampdata['om']]\n",
    "        wass = sps.wasserstein_distance(w_ref, w_comp)\n",
    "        \n",
    "        kde_comp = make_kde(grid_ref['w'], grid_ref['Omm'], w_comp, Omm_comp, one_d=True, to_log=True)\n",
    "        the_kld = calculate_kld(kde_ref, kde_comp, d_ref['w'], from_log=True)##np.array([d_ref['w'], d_ref['Omm']]))\n",
    "        newrow = {'case': case[:-4], 'KLD': the_kld, 'EMD': wass}\n",
    "        alloutputs = alloutputs.append(newrow, ignore_index=True)\n",
    "# alloutputs.to_csv(fullpath+'klds.csv')\n",
    "#         print(colordict[data_all['other_short'][data_all['case'] == newrow['case']][0]])\n",
    "#         colorval = data_all['other_short'].loc[data_all['case'] == newrow['case']].values\n",
    "# #         print(colorval)\n",
    "#         plt.plot(grid_ref['w'].T[0], np.exp(kde_comp),# label=newrow['case'], \n",
    "#                  c=plt.cm.tab10(colordict[colorval[0]]))\n",
    "# for key in colordict.keys():\n",
    "#     plt.plot([-1], [-1], c=plt.cm.tab10(colordict[key]), label=key)\n",
    "# plt.legend()\n",
    "# plt.title(field+k)\n",
    "# plt.xlabel(r'$w$')\n",
    "# # plt.yticklabels([])\n",
    "# plt.savefig('dists_'+field+k+'.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### put it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = data_all.join(alloutputs.set_index('case'), on='case')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save results!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all.to_csv('/media/RESSPECT/data/PLAsTiCC/for_metrics/final_data3/'+field+'/results/v'+k+'/'+nobj+\n",
    "#                 '/media/RESSPECT/data/PLAsTiCC/for_metrics/final_data/'+field+'/v'+str(k)+\n",
    "                '/summary_stats.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot results\n",
    "\n",
    "see other notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
