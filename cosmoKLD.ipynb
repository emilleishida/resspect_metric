{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the KLD from posterior samples of cosmological parameters\n",
    "\n",
    "_Alex I. Malz (GCCL@RUB)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import re\n",
    "from scipy import stats as sps\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import proclam\n",
    "from proclam.metrics.util import *\n",
    "from proclam.metrics.util import RateMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin with samples of $(w, \\Omega_{m})$ pairs, where one set of samples is defined as the reference sample corresponding to a best-case scenario of a 100% pure SN Ia data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # replace with reading in the data\n",
    "# def measure(n, w_bar, w_sig, Omm_bar,Omm_sig):\n",
    "#     \"Measurement model, return two coupled measurements.\"\n",
    "#     w = np.random.normal(loc=w_bar, scale=w_sig, size=n)\n",
    "#     Omm = np.random.normal(loc=Omm_bar, scale=Omm_sig, size=n)\n",
    "#     return w, Omm\n",
    "\n",
    "def measure(path, cols):\n",
    "    alldims = pkl.load(open(path, 'rb'))\n",
    "    return [alldims[col] for col in cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '/media/RESSPECT/data/PLAsTiCC/SALT2mu_posteriors/perfect_classifier/chains_plasticc_perfect.pkl'\n",
    "# '/media/RESSPECT/data/PLAsTiCC/SALT2mu_posteriors/static/DDF/train_10/batch_10/UncSampling/chains/chains_loop_99.pkl'\n",
    "# '/media/emille/git/COIN/RESSPECT_work/PLAsTiCC/metrics_paper/resspect_metric/posteriors/'\n",
    "# '/media/RESSPECT/data/PLAsTiCC/for_metrics/posteriors/'\n",
    "# '/media/emille/data/PLAsTiCC/posteriors/'\n",
    "# '/media2/RESSPECT2/data/posteriors'\n",
    "postpath = '/media2/RESSPECT2/data/posteriors/'\n",
    "\n",
    "refpath = postpath+'perfect/chains_perfect.pkl'\n",
    "comppath = postpath+'fiducial/chains_fiducial.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kinda slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w_ref, Omm_ref = measure(1000, -1., 0.1, 0.5, 0.1)\n",
    "# w_comp, Omm_comp = measure(1000, -1.1, 0.2, 0.25, 0.05)\n",
    "\n",
    "[w_ref, Omm_ref] = measure(refpath, ['w', 'om'])\n",
    "[w_comp, Omm_comp] = measure(comppath, ['w', 'om'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(w_ref, Omm_ref, s=1, alpha=0.2, label='best possible')\n",
    "# plt.scatter(w_comp, Omm_comp, s=1, alpha=0.2, label='approximation')\n",
    "# plt.legend(loc='lower left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`chippr`](https://github.com/aimalz/chippr/) contains code for calculating the KLD of PDFs evaluated on a grid, so we start by fitting a 2D KDE to the samples.\n",
    "The PDFs must be $\\geq0$ over the entire range of the grid, so we make a grid based on the reference sample's range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngrid_x = 100\n",
    "# ngrid_y = 100\n",
    "# xmin = w_ref.min()#-1.2\n",
    "# xmax = w_ref.max()#-0.8\n",
    "# ymin = Omm_ref.min()#0.2\n",
    "# ymax = Omm_ref.max()#0.4\n",
    "\n",
    "# w_grid, Omm_grid = np.mgrid[xmin:xmax:100*1.j, ymin:ymax:100*1.j]\n",
    "# w_vec, Omm_vec = w_grid[:, 0], Omm_grid[0, :]\n",
    "# dw = (xmax - xmin) / (ngrid_x - 1)\n",
    "# dOmm = (ymax - ymin) / (ngrid_y - 1)\n",
    "# # use meshgrid instead of mgrid\n",
    "\n",
    "def make_grid(x, y, x_ngrid=100, y_ngrid=100):\n",
    "    x_min = x.min()#-1.2\n",
    "    x_max = x.max()#-0.8\n",
    "    y_min = y.min()#0.2\n",
    "    y_max = y.max()#0.4\n",
    "\n",
    "    x_grid, y_grid = np.mgrid[x_min:x_max:x_ngrid*1.j, y_min:y_max:y_ngrid*1.j]\n",
    "    x_vec, y_vec = x_grid[:, 0], y_grid[0, :]\n",
    "    dx = (x_max - x_min) / (x_ngrid - 1)\n",
    "    dy = (y_max - y_min) / (y_ngrid - 1)\n",
    "\n",
    "    return(((x_min, y_min), (x_max, y_max)), (x_grid, y_grid), (x_vec, y_vec), (dx, dy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_extrema, ref_grids, ref_vecs, ref_ds = make_grid(w_ref, Omm_ref)\n",
    "(w_vec, Omm_vec) = ref_vecs\n",
    "(dw, dOmm) = ref_ds\n",
    "((xmin, ymin), (xmax, ymax)) = ref_extrema\n",
    "(w_grid, Omm_grid) = ref_grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist2d(w_ref, Omm_ref, bins=[w_vec, Omm_vec], density=True, cmap=plt.cm.Blues, alpha=0.5)\n",
    "# plt.hist2d(w_comp, Omm_comp, bins=[w_vec, Omm_vec], density=True, cmap=plt.cm.Reds, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist_ref, xgrid, ygrid = np.histogram2d(w_ref, Omm_ref, bins=[w_vec, Omm_vec], density=True)\n",
    "# hist_comp, xgrid, ygrid = np.histogram2d(w_comp, Omm_comp, bins=[w_vec, Omm_vec], density=True)\n",
    "# print(np.sum(hist_ref * ((np.ones_like(hist_ref) * dw).T * dOmm).T))\n",
    "# print(np.sum(hist_comp * ((np.ones_like(hist_comp) * dw).T * dOmm).T))\n",
    "# plt.imshow(hist_ref, origin='lower', extent=[xmin, xmax, ymin, ymax], cmap=plt.cm.Blues, alpha=0.5)\n",
    "# plt.imshow(hist_comp, origin='lower', extent=[xmin, xmax, ymin, ymax], cmap=plt.cm.Reds, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 2. * sys.float_info.min\n",
    "\n",
    "def safe_log(arr, threshold=eps):\n",
    "    \"\"\"\n",
    "    Takes the natural logarithm of an array that might contain zeros.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr: ndarray, float\n",
    "        array of values to be logged\n",
    "    threshold: float, optional\n",
    "        small, positive value to replace zeros and negative numbers\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    logged: ndarray\n",
    "        logged values, with small value replacing un-loggable values\n",
    "    \"\"\"\n",
    "    arr = np.asarray(arr)\n",
    "    arr[arr < threshold] = threshold\n",
    "    logged = np.log(arr)\n",
    "    return logged\n",
    "\n",
    "def make_kde(Xgrid, Ygrid, Xsamps, Ysamps, to_log=False):\n",
    "    positions = np.vstack([Xgrid.ravel(), Ygrid.ravel()])\n",
    "    values = np.vstack([Xsamps, Ysamps])\n",
    "    kernel = sps.gaussian_kde(values, bw_method='scott')#'scott'\n",
    "    Z = np.reshape(kernel(positions).T, Xgrid.shape)\n",
    "    if to_log:\n",
    "        return save_log(Z)\n",
    "    else:\n",
    "        return Z\n",
    "# TODO: normalize up here before log!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde_ref = make_kde(w_grid, Omm_grid, w_ref, Omm_ref)\n",
    "# plt.imshow(kde_ref, extent=[xmin, xmax, ymin, ymax], origin='lower', cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace with reading in other sets of posteriors\n",
    "kde_comp = make_kde(w_grid, Omm_grid, w_comp, Omm_comp)\n",
    "# plt.imshow(kde_comp, extent=[xmin, xmax, ymin, ymax], origin='lower', cmap=plt.cm.Reds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the 2D PDFs, let's define the KLD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stolen from chippr\n",
    "def calculate_kld(lpe, lqe, dx, from_log=False, vb=True):\n",
    "    \"\"\"\n",
    "    Calculates the Kullback-Leibler Divergence between two N-dimensional PDFs \n",
    "    evaluated on a shared, regular grid (sorry, too lazy to deal with irregular grid)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lpe: numpy.ndarray, float\n",
    "        log-probability distribution evaluated on a grid whose distance from `q`\n",
    "        will be calculated.\n",
    "    lqe: numpy.ndarray, float\n",
    "        log-probability distribution evaluated on a grid whose distance to `p` will\n",
    "        be calculated.\n",
    "    dx: numpy.ndarray, float\n",
    "        separation of grid values in each dimension\n",
    "    from_log: boolean, optional\n",
    "        if False, lpe, lqe are probability distributions, not log-probability distributions\n",
    "    vb: boolean, optional\n",
    "        report on progress to stdout?\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dpq: float\n",
    "        the value of the Kullback-Leibler Divergence from `q` to `p`\n",
    "    \"\"\"\n",
    "    # Normalize the evaluations, so that the integrals can be done\n",
    "    gridnorm = np.ones_like(lpe) * np.prod(dx)\n",
    "    if from_log:\n",
    "        pe = np.exp(lpe)\n",
    "        qe = np.exp(lqe)\n",
    "#     print(np.prod(dx))\n",
    "#     print(gridnorm)\n",
    "    else:\n",
    "        pe = lpe\n",
    "        qe = lqe\n",
    "    pi = np.sum(pe * gridnorm)\n",
    "    qi = np.sum(qe * gridnorm)\n",
    "    # (very approximately!) by simple summation:\n",
    "    pn = pe / pi\n",
    "    qn = qe / qi\n",
    "    # Compute the log of the normalized PDFs\n",
    "    logp = safe_log(pn)\n",
    "    logq = safe_log(qn)\n",
    "    # Calculate the KLD from q to p\n",
    "    Dpq = np.sum(pn * (logp - logq))\n",
    "    return Dpq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can evaluate it for our reference sample and a comparison sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_kld(kde_ref, kde_comp, np.array([dw, dOmm]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "could also do this as a function of chain iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### also some deterministic metrics for comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wouldn't need this if I'd been smart enough to put it in `proclam` already. . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class det_mets(RateMatrix):\n",
    "    \"binary classification metrics\"\n",
    "    def __init__(self, **rates):\n",
    "        \"\"\"\n",
    "        Call like `thing = det_mets(**rates._asdict())`\n",
    "        \"\"\"\n",
    "#         self.rates = rates#.asdict()\n",
    "        self._get_tots()\n",
    "        self._from_rates()\n",
    "        self._sn_mets()\n",
    "        self._translate()\n",
    "    def _get_tots(self):\n",
    "        self.CP = self.TP + self.FN\n",
    "        self.CN = self.TN + self.FP\n",
    "        self.T = self.TP + self.TN\n",
    "        self.F = self.FP + self.FN\n",
    "        self.P = self.TP + self.FP\n",
    "        self.N = self.TN + self.FN\n",
    "    def _from_rates(self):\n",
    "        self.PPV = self.TP / (self.TP + self.FP)\n",
    "        self.NPV = self.TN / (self.TN + self.FN)\n",
    "        self.PT = (np.sqrt(self.TPR * (1. - self.TNR)) + self.TNR - 1.) / (self.TPR + self.TNR - 1.)\n",
    "        self.TS = self.TP / (self.TP + self.FN + self.FP)\n",
    "        self._derived()\n",
    "    def _derived(self):\n",
    "        self.ACC = (self.TP + self.TN) / (self.CP + self.CN)\n",
    "        self.BA = (self.TPR + self.TNR) / 2,\n",
    "        self.F1S = 2. * self.PPV * self.TPR / (self.PPV + self.TPR)\n",
    "        self.MCC = (self.TP * self.TN - self.FP * self.FN) / (np.sqrt(self.P * self.CP * self.CN * self.N))\n",
    "        self.FM = np.sqrt(self.PPV * self.TPR)\n",
    "        self.BM = self.TPR + self.TNR - 1.\n",
    "        self.MK = self.PPV + self.NPV - 1.\n",
    "    def _translate(self):\n",
    "        self.positive = self.CP\n",
    "        self.negative = self.CN\n",
    "        self.sensitivity = self.TPR\n",
    "        self.recall = self.TPR\n",
    "        self.specificity = self.TNR\n",
    "        self.selectivity = self.TNR\n",
    "        self.precision = self.PPV\n",
    "        self.FDR = 1. - self.PPV\n",
    "        self.FOR = 1. - self.NPV\n",
    "        self.CSI = self.TS\n",
    "        self.accuracy = self.ACC\n",
    "        self.f1_score = self.F1S\n",
    "        self.informedness = self.BM\n",
    "        self.deltaP = self.MK\n",
    "    def _sn_mets(self):\n",
    "        self.get_efficiency()\n",
    "        self.get_purity()\n",
    "    def get_efficiency(self):\n",
    "        self.efficiency = self.TP / self.CP\n",
    "        return self.efficiency\n",
    "    def get_purity(self):\n",
    "        self.purity = self.TP / self.P\n",
    "        return self.purity\n",
    "    def get_fom(self, penalty):\n",
    "        self.pseudo_purity = self.TP / (self.TP + penalty * self.FP)\n",
    "        return self.pseudo_purity * self.efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with \"real\" contaminated samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savedpath = '/media/RESSPECT/data/PLAsTiCC/for_metrics/'\n",
    "metpaths = {field: savedpath+field+'/metrics/' for field in ['ddf', 'wfd']}\n",
    "# metpaths = {field: savedpath+'metrics/' for field in ['ddf']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maybe_sn_classes = {90: 'SNIa', \n",
    "                    67: 'SNIa-91bg', \n",
    "                    52: 'SNIax', \n",
    "                    42: 'SNII', \n",
    "                    62: 'SNIbc', \n",
    "                    95: 'SLSN-I', \n",
    "                    88: 'AGN'}\n",
    "maybe_sn_classes[15] = 'TDE'\n",
    "maybe_sn_classes[64] = 'KN'\n",
    "\n",
    "sel_class = 90\n",
    "\n",
    "# ia_percents = np.array([50, 68, 75, 90, 95, 98, 99])\n",
    "# mix_percents = 100 - ia_percents\n",
    "contaminants = maybe_sn_classes.copy()\n",
    "contaminants.pop(sel_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate on the grid for the perfect samples as reference\n",
    "\n",
    "The KDEs are the slow step here. . . don't run this more than once\n",
    "\n",
    "TODO: save the KDEs, just in case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ref, grid_ref, kde_ref = {}, {}, {}\n",
    "for field in ['ddf', 'wfd']:\n",
    "    if field == 'wfd':\n",
    "        prepend = 'WFD/'\n",
    "    else:\n",
    "        prepend = ''\n",
    "    [w_ref, Omm_ref] = measure(postpath+prepend+'perfect/chains_perfect.pkl', ['w', 'om'])\n",
    "    ref_extrema, ref_grids, ref_vecs, ref_ds = make_grid(w_ref, Omm_ref)\n",
    "    (w_vec, Omm_vec) = ref_vecs\n",
    "    (dw, dOmm) = ref_ds\n",
    "    ((xmin, ymin), (xmax, ymax)) = ref_extrema\n",
    "    (w_grid, Omm_grid) = ref_grids\n",
    "    d_ref[field] = {'w': dw, 'Omm': dOmm}\n",
    "    grid_ref[field] = {'w': w_grid, 'Omm': Omm_grid}\n",
    "    kde_ref[field] = make_kde(w_grid, Omm_grid, w_ref, Omm_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "allmets = pd.read_csv(savedpath+'directory.csv')\n",
    "allmets['KLD'] = None\n",
    "allmets = allmets.drop_duplicates(ignore_index=True)\n",
    "\n",
    "\n",
    "for ind in allmets.index:\n",
    "    row = allmets.loc[ind]\n",
    "    testname = str(100-row['percent'])+str(maybe_sn_classes[sel_class])+str(row['percent'])+row['contaminant']\n",
    "    if row['field'] == 'wfd':\n",
    "        comppath = postpath+'WFD/'+testname\n",
    "    else:\n",
    "        comppath = postpath+testname\n",
    "    compfn = comppath+'/chains_'+testname+'_lowz_withbias.pkl'\n",
    "    print(compfn)\n",
    "    if os.path.exists(compfn):\n",
    "        [w_comp, Omm_comp] = measure(compfn, ['w', 'om'])\n",
    "        kde_comp = make_kde(grid_ref[row['field']]['w'], grid_ref[row['field']]['Omm'], w_comp, Omm_comp)\n",
    "        allmets['KLD'].loc[ind] = calculate_kld(kde_ref[row['field']], kde_comp, \n",
    "                                                np.array([d_ref[row['field']]['w'], d_ref[row['field']]['Omm']]))\n",
    "allmets.to_csv(savedpath+'KLD.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run me only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allmets = pd.read_csv(savedpath+'KLD.csv')\n",
    "allmets['fom1'] = None\n",
    "allmets['fom3'] = None\n",
    "allmets['purity'] = None\n",
    "allmets['efficiency'] = None\n",
    "allmets['f1'] = None\n",
    "for ind in allmets.index:\n",
    "    row = allmets.loc[ind]\n",
    "    concode = list(contaminants.keys())[list(contaminants.values()).index(row['contaminant'])]\n",
    "    testname = f\"{100-row['percent']}_{sel_class}_{row['percent']}_{concode}\"\n",
    "    metfn = metpaths[row['field']]+testname+'.pkl'#f'{100-perc}_{sel_class}_{perc}_{key}.pkl'\n",
    "    with open(metfn, 'rb') as metfile:\n",
    "        rates = proclam.util.RateMatrix(**pkl.load(metfile))\n",
    "        ratedict = rates._asdict()\n",
    "        mets = det_mets(**ratedict)\n",
    "    allmets['purity'].loc[ind] = mets.purity\n",
    "    allmets['efficiency'].loc[ind] = mets.efficiency\n",
    "    allmets['f1'].loc[ind] = mets.f1_score\n",
    "    allmets['fom1'].loc[ind] = mets.get_fom(1.)\n",
    "    allmets['fom3'].loc[ind] = mets.get_fom(3.)\n",
    "allmets.to_csv(savedpath+'FOM.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allmets = pd.read_csv(savedpath+'FOM.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allmets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plotting cosmo/principled vs. traditional/deterministic metrics\n",
    "\n",
    "~~TODO: actually make these~~\n",
    "~~- [X] by contaminant (markershape)~~\n",
    "~~- [X] by contamination rate (continuous colors? or markersize?)~~\n",
    "~~- [X] by field (markersize? or discrete colors?)~~\n",
    "\n",
    "TODO:\n",
    "- [X] hardcode the shapes\n",
    "- [X] open/closed for field\n",
    "- [ ] logscale colors\n",
    "- [X] check WFD directory for posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sizes = {'ddf': 50, 'wfd': 150}\n",
    "all_shapes = {}\n",
    "# for i, (k, v) in enumerate(maybe_sn_classes.items()):\n",
    "#     shapes[v] = (np.mod(i, 3)+3, int(i / 3), np.mod(i, 4)*45)\n",
    "shape_pairs = [('.', 'o'), ('1', 'v'), ('2', '^'), ('3', '<'), ('4', '>'), ('+', 'P'), ('x', 'X'), ('*', 'p')]\n",
    "for i, field in enumerate(['ddf', 'wfd']):\n",
    "    shapes = {}\n",
    "    for j, contaminant in enumerate(allmets['contaminant'].unique()):\n",
    "        shapes[contaminant] = shape_pairs[j][i]\n",
    "    all_shapes[field] = shapes\n",
    "\n",
    "colors = {i: plt.get_cmap('plasma_r')(i) for i in allmets['percent'].unique()}\n",
    "\n",
    "alldets = ['f1', 'fom1', 'fom3', 'purity', 'efficiency']\n",
    "dim = len(alldets)\n",
    "\n",
    "fave_cmap = 'viridis_r'#'plasma_r'#'cool'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: add in 1D histograms for metric value for each contaminant, for percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = list(itertools.combinations(range(dim), 2))\n",
    "fig, axs = plt.subplots(dim-1, dim-1, figsize=(5*(dim-1), 5*(dim-1)))\n",
    "norm = mpl.colors.Normalize(vmin=0., vmax=50.)\n",
    "fig.colorbar(mpl.cm.ScalarMappable(cmap=plt.get_cmap(fave_cmap), norm=norm), cax=axs[-1][0], \n",
    "             ticks=allmets['percent'].unique())\n",
    "\n",
    "for i, c in enumerate(allmets['contaminant'].unique()):\n",
    "    for f in ['ddf', 'wfd']:\n",
    "        axs[-1][0].scatter(-1, -1, marker=all_shapes[f][c], color='k', label=f+':'+c)\n",
    "        one_c = allmets[(allmets['contaminant'] == c) & (allmets['field'] == f)]\n",
    "        for pair in pairs:\n",
    "            axs[pair[0]][pair[1]-1].scatter(one_c[alldets[pair[0]]], one_c[alldets[pair[1]]],\n",
    "                                   alpha=0.5, s=100,#[sizes[f] for f in one_c['field']], \n",
    "                                   marker=all_shapes[f][c], \n",
    "                                   color=plt.get_cmap(fave_cmap)(one_c['percent']/50.))\n",
    "for pair in pairs:\n",
    "    axs[pair[0]][pair[1]-1].set_xlabel(alldets[pair[0]])\n",
    "    axs[pair[0]][pair[1]-1].set_ylabel(alldets[pair[1]])\n",
    "axs[-1][0].legend()\n",
    "fig.savefig('draft_dets.png', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dets_to_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dets_to_plot = ['fom3']\n",
    "dim = len(dets_to_plot)\n",
    "\n",
    "fig, axs = plt.subplots(1, dim+1, figsize=(5*(dim+1), 5))#,\n",
    "#                         gridspec_kw={'width_ratios': [10]*dim+[1]})\n",
    "norm = mpl.colors.Normalize(vmin=0., vmax=50.)\n",
    "fig.colorbar(mpl.cm.ScalarMappable(cmap=plt.get_cmap(fave_cmap), norm=norm), cax=axs[-1], \n",
    "             ticks=allmets['percent'].unique())\n",
    "for i, metric in enumerate(dets_to_plot):\n",
    "    for field in sizes.keys():\n",
    "        for contaminant in shapes.keys():\n",
    "            plotmask = allmets[(allmets['field'] == field) & (allmets['contaminant'] == contaminant)]\n",
    "            axs[i].scatter(plotmask[metric], plotmask['KLD'], alpha=0.75, s=100,\n",
    "                           marker=all_shapes[field][contaminant], \n",
    "                           color=plt.get_cmap(fave_cmap)(plotmask['percent']/50.))\n",
    "    axs[i].semilogy()\n",
    "    axs[i].set_xlabel(metric)\n",
    "    axs[i].set_ylabel('KLD')\n",
    "    axs[i].set_xlim(-0.1, 1.1)\n",
    "for field in sizes.keys():\n",
    "#         axs[i].scatter(-1, -1, s=100, marker='o', color='k', \n",
    "#                        label=field)\n",
    "    for contaminant in contaminants.values():\n",
    "        axs[-1].scatter(-1, -1, s=50, marker=all_shapes[field][contaminant], color='k', \n",
    "                       label=field+':'+contaminant)\n",
    "axs[-1].legend(loc='lower left', fontsize='small')\n",
    "fig.savefig('draft_kld.png', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: thought re: histograms of w\\_est, sigma\\_w, make two versions: color by which contaminant (discrete colors) and by % contamninant (continuous colors)\n",
    "\n",
    "these live in postpath = '/media2/RESSPECT2/data/posteriors/' stan\\_summary .dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allmets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recidivator (Python 3)",
   "language": "python",
   "name": "recidivator_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
