{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the KLD from posterior samples of cosmological parameters\n",
    "\n",
    "_Alex I. Malz (GCCL@RUB)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import re\n",
    "from scipy import stats as sps\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cmx\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# Color map\n",
    "rainbow = cm = plt.get_cmap('plasma_r')\n",
    "cNorm  = colors.LogNorm(vmin=1, vmax=50) #colors.Normalize(vmin=0, vmax=50)\n",
    "scalarMap = cmx.ScalarMappable(norm=cNorm, cmap=rainbow)\n",
    "color_map = scalarMap.to_rgba(np.arange(1, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import proclam\n",
    "from proclam.metrics.util import *\n",
    "from proclam.metrics.util import RateMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin with samples of $(w, \\Omega_{m})$ pairs, where one set of samples is defined as the reference sample corresponding to a best-case scenario of a 100% pure SN Ia data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # replace with reading in the data\n",
    "# def measure(n, w_bar, w_sig, Omm_bar,Omm_sig):\n",
    "#     \"Measurement model, return two coupled measurements.\"\n",
    "#     w = np.random.normal(loc=w_bar, scale=w_sig, size=n)\n",
    "#     Omm = np.random.normal(loc=Omm_bar, scale=Omm_sig, size=n)\n",
    "#     return w, Omm\n",
    "\n",
    "def measure(path, cols):\n",
    "    alldims = pkl.load(open(path, 'rb'))\n",
    "    return [alldims[col] for col in cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: also show for chain subsamples in loop for evolution (see `/media/RESSPECT/data/PLAsTiCC/SALT2mu_posteriors/static/DDF/train_10/batch_10/UncSampling/chains/chains_loop_99.pkl` and more numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: also flat prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '/media/RESSPECT/data/PLAsTiCC/SALT2mu_posteriors/perfect_classifier/chains_plasticc_perfect.pkl'\n",
    "# '/media/RESSPECT/data/PLAsTiCC/SALT2mu_posteriors/static/DDF/train_10/batch_10/UncSampling/chains/chains_loop_99.pkl'\n",
    "# '/media/emille/git/COIN/RESSPECT_work/PLAsTiCC/metrics_paper/resspect_metric/posteriors/'\n",
    "# '/media/RESSPECT/data/PLAsTiCC/for_metrics/posteriors/'\n",
    "# '/media/emille/data/PLAsTiCC/posteriors/'\n",
    "# '/media2/RESSPECT2/data/posteriors'\n",
    "postpath = '/media2/RESSPECT2/data/posteriors'\n",
    "fields = ['ddf']#, 'wfd']\n",
    "priors = [1, 5]#omprior_0.01_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listdirs(rootdir):\n",
    "    outfns = {}\n",
    "    for it in os.scandir(rootdir):\n",
    "        if it.is_dir():\n",
    "            fn = it.path[len(rootdir):]\n",
    "            lookfor = it.path+'/chains_'+fn+'_lowz_withbias.pkl'\n",
    "            if os.path.exists(lookfor):\n",
    "                outfns[fn] = lookfor\n",
    "    return outfns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postpaths, testcases = {}, {}\n",
    "for field in fields:\n",
    "    postpaths[field], testcases[field] = {}, {}\n",
    "    for prior in priors:\n",
    "        postpaths[field][prior] = postpath+'_'+field+'/omprior_0.0'+str(prior)+'/'\n",
    "        testcases[field][prior] = listdirs(postpaths[field][prior])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kinda slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refpath = testcases['perfect3000']\n",
    "# [w_ref, Omm_ref] = measure(refpath, ['w', 'om'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w_ref, Omm_ref = measure(1000, -1., 0.1, 0.5, 0.1)\n",
    "# w_comp, Omm_comp = measure(1000, -1.1, 0.2, 0.25, 0.05)\n",
    "\n",
    "# comppath = testcasespostpath+'fiducial3000/chains_fiducial3000.pkl'\n",
    "# [w_comp, Omm_comp] = measure(comppath, ['w', 'om'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(w_ref, Omm_ref, s=1, alpha=0.2, label='best possible')\n",
    "# plt.scatter(w_comp, Omm_comp, s=1, alpha=0.2, label='approximation')\n",
    "# plt.legend(loc='lower left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`chippr`](https://github.com/aimalz/chippr/) contains code for calculating the KLD of PDFs evaluated on a grid, so we start by fitting a 2D KDE to the samples.\n",
    "The PDFs must be $\\geq0$ over the entire range of the grid, so we make a grid based on the reference sample's range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngrid_x = 100\n",
    "# ngrid_y = 100\n",
    "# xmin = w_ref.min()#-1.2\n",
    "# xmax = w_ref.max()#-0.8\n",
    "# ymin = Omm_ref.min()#0.2\n",
    "# ymax = Omm_ref.max()#0.4\n",
    "\n",
    "# w_grid, Omm_grid = np.mgrid[xmin:xmax:100*1.j, ymin:ymax:100*1.j]\n",
    "# w_vec, Omm_vec = w_grid[:, 0], Omm_grid[0, :]\n",
    "# dw = (xmax - xmin) / (ngrid_x - 1)\n",
    "# dOmm = (ymax - ymin) / (ngrid_y - 1)\n",
    "# # use meshgrid instead of mgrid\n",
    "\n",
    "def make_grid(x, y, x_ngrid=100, y_ngrid=100):\n",
    "    x_min = x.min()#-1.2\n",
    "    x_max = x.max()#-0.8\n",
    "    y_min = y.min()#0.2\n",
    "    y_max = y.max()#0.4\n",
    "\n",
    "    x_grid, y_grid = np.mgrid[x_min:x_max:x_ngrid*1.j, y_min:y_max:y_ngrid*1.j]\n",
    "    x_vec, y_vec = x_grid[:, 0], y_grid[0, :]\n",
    "    dx = (x_max - x_min) / (x_ngrid - 1)\n",
    "    dy = (y_max - y_min) / (y_ngrid - 1)\n",
    "\n",
    "    return(((x_min, y_min), (x_max, y_max)), (x_grid, y_grid), (x_vec, y_vec), (dx, dy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref_extrema, ref_grids, ref_vecs, ref_ds = make_grid(w_ref, Omm_ref)\n",
    "# (w_vec, Omm_vec) = ref_vecs\n",
    "# (dw, dOmm) = ref_ds\n",
    "# ((xmin, ymin), (xmax, ymax)) = ref_extrema\n",
    "# (w_grid, Omm_grid) = ref_grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop this up later, just one for now\n",
    "which_field_prior = ('ddf', 1)\n",
    "onetest = testcases[which_field_prior[0]][which_field_prior[1]]\n",
    "\n",
    "# perfcases, randcases, fidcases = {}, {}, {}\n",
    "# perfposts, randposts, fidposts = {}, {}, {}\n",
    "# perfgrids = {}\n",
    "datatemplate = {'paths': {}, 'posteriors': {}, 'grid_info': {}}\n",
    "allperf, allrand, allfid = {'paths': {}, 'posteriors': {}, 'grid_info': {}}, {'paths': {}, 'posteriors': {}}, {'paths': {}, 'posteriors': {}}\n",
    "for i in onetest.keys():\n",
    "    newkey = i[-4:]\n",
    "    if 'perfect' in i:\n",
    "        allperf['paths'][newkey] = onetest[i]\n",
    "        allperf['posteriors'][newkey] = measure(allperf['paths'][newkey], ['w', 'om'])\n",
    "        allperf['grid_info'][newkey] = make_grid(allperf['posteriors'][newkey][0], allperf['posteriors'][newkey][1])\n",
    "        allperf['color'] = plt.cm.Blues\n",
    "    elif 'fiducial' in i:\n",
    "        allfid['paths'][newkey] = onetest[i]\n",
    "        allfid['posteriors'][newkey] = measure(allfid['paths'][newkey], ['w', 'om'])\n",
    "        allfid['color'] = plt.cm.Greens\n",
    "    elif 'random' in i:\n",
    "        allrand['paths'][newkey] = onetest[i]\n",
    "        allrand['posteriors'][newkey] = measure(allrand['paths'][newkey], ['w', 'om'])\n",
    "        allrand['color'] = plt.cm.Reds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist2d(w_ref, Omm_ref, bins=[w_vec, Omm_vec], density=True, cmap=plt.cm.Blues, alpha=0.5)\n",
    "# plt.hist2d(w_comp, Omm_comp, bins=[w_vec, Omm_vec], density=True, cmap=plt.cm.Reds, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: keep propagating for cosmological contours as a function of number of lightcurves, classifier, field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "for i, n_sn in enumerate(allperf['posteriors'].keys()):\n",
    "    ((x_min, y_min), (x_max, y_max)), (x_grid, y_grid), (x_vec, y_vec), (dx, dy) = allperf['grid_info'][n_sn]\n",
    "    \n",
    "    # these don't work because ranges really don't overlap!\n",
    "#     axs[i].hist2d(allperf['posteriors'][n_sn][0], allperf['posteriors'][n_sn][1], bins=[x_vec, y_vec], density=True, cmap=allperf['color'], alpha=0.3)\n",
    "#     axs[i].hist2d(allfid['posteriors'][n_sn][0], allfid['posteriors'][n_sn][1], bins=[x_vec, y_vec], density=True, cmap=allfid['color'], alpha=0.3)\n",
    "#     axs[i].hist2d(allrand['posteriors'][n_sn][0], allrand['posteriors'][n_sn][1], bins=[x_vec, y_vec], density=True, cmap=allrand['color'], alpha=0.3)\n",
    "    \n",
    "# # these work only without density and bin specs\n",
    "    hist_perf, xgrid, ygrid = np.histogram2d(allperf['posteriors'][n_sn][0], allperf['posteriors'][n_sn][1], bins=[x_vec, y_vec], density=True)\n",
    "    hist_fid, xgrid, ygrid = np.histogram2d(allfid['posteriors'][n_sn][0], allfid['posteriors'][n_sn][1])#, bins=[x_vec, y_vec], density=True)\n",
    "    hist_rand, xgrid, ygrid = np.histogram2d(allrand['posteriors'][n_sn][0], allrand['posteriors'][n_sn][1])#, bins=[x_vec, y_vec])#, density=True)\n",
    "    axs[i].imshow(hist_perf, origin='lower', extent=[x_min, x_max, y_min, y_max], cmap=allperf['color'], alpha=0.3)\n",
    "#     axs[i].imshow(hist_fid, origin='lower', extent=[x_min, x_max, y_min, y_max], cmap=allfid['color'], alpha=0.3)\n",
    "#     axs[i].imshow(hist_rand, origin='lower', extent=[x_min, x_max, y_min, y_max], cmap=allrand['color'], alpha=0.3)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 2. * sys.float_info.min\n",
    "\n",
    "def safe_log(arr, threshold=eps):\n",
    "    \"\"\"\n",
    "    Takes the natural logarithm of an array that might contain zeros.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr: ndarray, float\n",
    "        array of values to be logged\n",
    "    threshold: float, optional\n",
    "        small, positive value to replace zeros and negative numbers\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    logged: ndarray\n",
    "        logged values, with small value replacing un-loggable values\n",
    "    \"\"\"\n",
    "    arr = np.asarray(arr)\n",
    "    arr[arr < threshold] = threshold\n",
    "    logged = np.log(arr)\n",
    "    return logged\n",
    "\n",
    "def make_kde(Xgrid, Ygrid, Xsamps, Ysamps, to_log=False, save=None):\n",
    "    positions = np.vstack([Xgrid.ravel(), Ygrid.ravel()])\n",
    "    values = np.vstack([Xsamps, Ysamps])\n",
    "    kernel = sps.gaussian_kde(values, bw_method='scott')\n",
    "    Z = np.reshape(kernel(positions).T, Xgrid.shape)\n",
    "    if to_log:\n",
    "        return safe_log(Z)\n",
    "    else:\n",
    "        return Z\n",
    "    \n",
    "#     if save is not None:\n",
    "        \n",
    "# TODO: normalize up here before log!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde_ref = make_kde(w_grid, Omm_grid, w_ref, Omm_ref)\n",
    "# plt.imshow(kde_ref, extent=[xmin, xmax, ymin, ymax], origin='lower', cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace with reading in other sets of posteriors\n",
    "kde_comp = make_kde(w_grid, Omm_grid, w_comp, Omm_comp)\n",
    "# plt.imshow(kde_comp, extent=[xmin, xmax, ymin, ymax], origin='lower', cmap=plt.cm.Reds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(kde_ref, origin='lower', extent=[xmin, xmax, ymin, ymax], cmap=plt.cm.Blues, alpha=0.5)\n",
    "plt.imshow(kde_comp, origin='lower', extent=[xmin, xmax, ymin, ymax], cmap=plt.cm.Reds, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the 2D PDFs, let's define the KLD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stolen from chippr\n",
    "def calculate_kld(lpe, lqe, dx, from_log=False, vb=True):\n",
    "    \"\"\"\n",
    "    Calculates the Kullback-Leibler Divergence between two N-dimensional PDFs \n",
    "    evaluated on a shared, regular grid (sorry, too lazy to deal with irregular grid)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lpe: numpy.ndarray, float\n",
    "        log-probability distribution evaluated on a grid whose distance from `q`\n",
    "        will be calculated.\n",
    "    lqe: numpy.ndarray, float\n",
    "        log-probability distribution evaluated on a grid whose distance to `p` will\n",
    "        be calculated.\n",
    "    dx: numpy.ndarray, float\n",
    "        separation of grid values in each dimension\n",
    "    from_log: boolean, optional\n",
    "        if False, lpe, lqe are probability distributions, not log-probability distributions\n",
    "    vb: boolean, optional\n",
    "        report on progress to stdout?\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dpq: float\n",
    "        the value of the Kullback-Leibler Divergence from `q` to `p`\n",
    "    \"\"\"\n",
    "    # Normalize the evaluations, so that the integrals can be done\n",
    "    gridnorm = np.ones_like(lpe) * np.prod(dx)\n",
    "    if from_log:\n",
    "        pe = np.exp(lpe)\n",
    "        qe = np.exp(lqe)\n",
    "#     print(np.prod(dx))\n",
    "#     print(gridnorm)\n",
    "    else:\n",
    "        pe = lpe\n",
    "        qe = lqe\n",
    "    pi = np.sum(pe * gridnorm)\n",
    "    qi = np.sum(qe * gridnorm)\n",
    "    # (very approximately!) by simple summation:\n",
    "    pn = pe / pi\n",
    "    qn = qe / qi\n",
    "    # Compute the log of the normalized PDFs\n",
    "    logp = safe_log(pn)\n",
    "    logq = safe_log(qn)\n",
    "    # Calculate the KLD from q to p\n",
    "    Dpq = np.sum(pn * (logp - logq))\n",
    "#     if np.isnan(Dpq):\n",
    "#         return((lpe, lqe, dx))\n",
    "    return Dpq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can evaluate it for our reference sample and a comparison sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_kld(kde_ref, kde_comp, np.array([dw, dOmm]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "could also do this as a function of chain iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with \"real\" contaminated samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savedpath = '/media/RESSPECT/data/PLAsTiCC/for_metrics/'\n",
    "metpaths = {field: {prior: savedpath+field+'/metrics/'+str(prior)+'/' for prior in priors} for field in fields}\n",
    "# metpaths = {field: savedpath+'metrics/' for field in ['ddf']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maybe_sn_classes = {90: 'SNIa', \n",
    "                    67: 'SNIa-91bg', \n",
    "                    52: 'SNIax', \n",
    "                    42: 'SNII', \n",
    "                    62: 'SNIbc', \n",
    "                    95: 'SLSN-I', \n",
    "                    88: 'AGN'}\n",
    "maybe_sn_classes[15] = 'TDE'\n",
    "maybe_sn_classes[64] = 'KN'\n",
    "\n",
    "shapes = {'ddf': {'SNIa-91bg': 'o',\n",
    " 'SNIax': 's',\n",
    " 'SNII': 'd',\n",
    " 'SNIbc': 'X',\n",
    " 'SLSN-I': 'v',\n",
    " 'AGN': '^',\n",
    " 'TDE': '<',\n",
    " 'KN': '>',\n",
    " 'CART': 'v'},\n",
    "          'wfd': {'SNIa-91bg': '.',\n",
    " 'SNIax': '+',\n",
    " 'SNII': 'P',\n",
    " 'SNIbc': 'x',\n",
    " 'SLSN-I': '1',\n",
    " 'AGN': '2',\n",
    " 'TDE': '3',\n",
    " 'KN': '4',\n",
    " 'CART': 'v'}}\n",
    "\n",
    "sel_class = 90\n",
    "\n",
    "# ia_percents = np.array([50, 68, 75, 90, 95, 98, 99])\n",
    "# mix_percents = 100 - ia_percents\n",
    "contaminants = maybe_sn_classes.copy()\n",
    "contaminants.pop(sel_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### also some deterministic metrics for comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wouldn't need this if I'd been smart enough to put it in `proclam` already. . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class det_mets(RateMatrix):\n",
    "    \"binary classification metrics\"\n",
    "    def __init__(self, **rates):\n",
    "        \"\"\"\n",
    "        Call like `thing = det_mets(**rates._asdict())`\n",
    "        \"\"\"\n",
    "#         self.rates = rates#.asdict()\n",
    "        self._get_tots()\n",
    "        self._from_rates()\n",
    "        self._sn_mets()\n",
    "        self._translate()\n",
    "    def _get_tots(self):\n",
    "        self.CP = self.TP + self.FN\n",
    "        self.CN = self.TN + self.FP\n",
    "        self.T = self.TP + self.TN\n",
    "        self.F = self.FP + self.FNhttps://cosmostatistics.slack.com/archives/GSECX131T/p1622754012019100\n",
    "        self.P = self.TP + self.FP\n",
    "        self.N = self.TN + self.FN\n",
    "    def _from_rates(self):\n",
    "        self.PPV = self.TP / (self.TP + self.FP)\n",
    "        self.NPV = self.TN / (self.TN + self.FN)\n",
    "        self.PT = (np.sqrt(self.TPR * (1. - self.TNR)) + self.TNR - 1.) / (self.TPR + self.TNR - 1.)\n",
    "        self.TS = self.TP / (self.TP + self.FN + self.FP)\n",
    "        self._derived()\n",
    "    def _derived(self):\n",
    "        self.ACC = (self.TP + self.TN) / (self.CP + self.CN)\n",
    "        self.BA = (self.TPR + self.TNR) / 2,\n",
    "        self.F1S = 2. * self.PPV * self.TPR / (self.PPV + self.TPR)\n",
    "        self.MCC = (self.TP * self.TN - self.FP * self.FN) / (np.sqrt(self.P * self.CP * self.CN * self.N))\n",
    "        self.FM = np.sqrt(self.PPV * self.TPR)\n",
    "        self.BM = self.TPR + self.TNR - 1.\n",
    "        self.MK = self.PPV + self.NPV - 1.\n",
    "    def _translate(self):\n",
    "        self.positive = self.CP\n",
    "        self.negative = self.CN\n",
    "        self.sensitivity = self.TPR\n",
    "        self.recall = self.TPR\n",
    "        self.specificity = self.TNR\n",
    "        self.selectivity = self.TNR\n",
    "        self.precision = self.PPV\n",
    "        self.FDR = 1. - self.PPV\n",
    "        self.FOhttps://us02web.zoom.us/j/867187256?pwd=akRrVFU2VGlpR2ZXN1NVSUVUdE41dz09#successR = 1. - self.NPV\n",
    "        self.CSI = self.TS\n",
    "        self.accuracy = self.ACC\n",
    "        self.f1_score = self.F1S\n",
    "        self.informedness = self.BM\n",
    "        self.deltaP = self.MK\n",
    "    def _sn_mets(self):\n",
    "        self.get_efficiency()\n",
    "        self.get_purity()\n",
    "    def get_efficiency(self):\n",
    "        self.efficiency = self.TP / self.CP\n",
    "        return self.efficiency\n",
    "    def get_purity(self):\n",
    "        self.purity = self.TP / self.P\n",
    "        return self.purity\n",
    "    def get_fom(self, penalty):\n",
    "        self.pseudo_purity = self.TP / (self.TP + penalty * self.FP)\n",
    "        return self.pseudo_purity * self.efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just deterministic metrics first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap_rate(ratemat):\n",
    "    temp = proclam.util.RateMatrix(TPR=ratemat.TNR,\n",
    "                                  FPR=ratemat.FNR,\n",
    "                                  FNR=ratemat.FPR,\n",
    "                                  TNR=ratemat.TPR,\n",
    "                                  TP=ratemat.TN,\n",
    "                                  FP=ratemat.FN,\n",
    "                                  FN=ratemat.FP,\n",
    "                                  TN=ratemat.TP)\n",
    "    return(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allmets = pd.read_csv(savedpath+'directory.csv')\n",
    "allmets = allmets.drop_duplicates(ignore_index=True)\n",
    "for met in ['purity', 'efficiency', 'f1', 'fom1', 'fom3']:\n",
    "    allmets[met] = None\n",
    "for ind in allmets.index:\n",
    "    row = allmets.loc[ind]\n",
    "#     testname = str(100-row['percent'])+str(maybe_sn_classes[sel_class])+str(row['percent'])+row['contaminant']\n",
    "#     allmets['name'].loc[ind] = testname\n",
    "    testname = row['name']\n",
    "#     concode = list(contaminants.keys())[list(contaminants.values()).index(row['contaminant'])]\n",
    "#     testname = row['name']#f\"{100-row['percent']}_{sel_class}_{row['percent']}_{concode}\"\n",
    "    compfn = row['inloc']+'.pkl'\n",
    "#     if testname in testcases[field][prior].keys():\n",
    "#     compfn = testcases[field][prior][testname]\n",
    "#         metfn = metpaths[row['field']]+testname+'.pkl'#f'{100-perc}_{sel_class}_{perc}_{key}.pkl'\n",
    "    if os.path.exists(compfn):\n",
    "#         print(compfn)\n",
    "        with open(compfn, 'rb') as metfile:\n",
    "            rates = proclam.util.RateMatrix(**pkl.load(metfile))\n",
    "            rates = remap_rate(rates)\n",
    "            ratedict = rates._asdict()\n",
    "            mets = det_mets(**ratedict)\n",
    "        allmets['purity'].loc[ind] = mets.purity\n",
    "        allmets['efficiency'].loc[ind] = mets.efficiency\n",
    "        allmets['f1'].loc[ind] = mets.f1_score\n",
    "        allmets['fom1'].loc[ind] = mets.get_fom(1.)\n",
    "        allmets['fom3'].loc[ind] = mets.get_fom(3.)\n",
    "allmets.to_csv(savedpath+'FOM.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sizes = {'ddf': 50, 'wfd': 150}\n",
    "# all_shapes = {}\n",
    "# # for i, (k, v) in enumerate(maybe_sn_classes.items()):\n",
    "# #     shapes[v] = (np.mod(i, 3)+3, int(i / 3), np.mod(i, 4)*45)\n",
    "# shape_pairs = [('.', 'o'), ('1', 'v'), ('2', '^'), ('3', '<'), ('4', '>'), ('+', 'P'), ('x', 'X'), ('*', 'p')]\n",
    "# for i, field in enumerate(['ddf', 'wfd']):\n",
    "#     shapes = {}\n",
    "#     for j, contaminant in enumerate(allmets['contaminant'].unique()):\n",
    "#         shapes[contaminant] = shape_pairs[j][i]\n",
    "#     all_shapes[field] = shapes\n",
    "\n",
    "(uq, ind, inv, cts) = np.unique(allmets['percent'], return_index=True, return_inverse=True, return_counts=True)\n",
    "linear_colors = {uq[i]: i for i in range(len(uq))}\n",
    "cNorm  = colors.LogNorm(vmin=1, vmax=len(uq)) #colors.Normalize(vmin=0, vmax=50)\n",
    "scalarMap = cmx.ScalarMappable(norm=cNorm, cmap=rainbow)\n",
    "color_map = scalarMap.to_rgba(np.arange(0, len(uq)))\n",
    "\n",
    "# fave_cmap = 'viridis_r'#'plasma_r'#'cool'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allmets = pd.read_csv(savedpath+'FOM.csv')\n",
    "allmets = allmets[allmets['percent'] > 0]\n",
    "\n",
    "alldets = ['f1', 'fom1', 'fom3', 'purity', 'efficiency', 'percent']\n",
    "dim = len(alldets)\n",
    "\n",
    "pairs = list(itertools.combinations(range(dim), 2))\n",
    "fig, axs = plt.subplots(dim-1, dim-1, figsize=(5*(dim-1), 5*(dim-1)))\n",
    "# norm = mpl.colors.Normalize(vmin=0., vmax=50.)\n",
    "cbar = fig.colorbar(scalarMap, cax=axs[-1][0], ticks=range(len(uq)))#, ticklabels=str(uq))\n",
    "cbar.set_ticklabels(uq)\n",
    "\n",
    "for i, c in enumerate(allmets['contaminant'].unique()):\n",
    "#     axs[-1][0].scatter(-1, -1, marker=shapes[c], color='k', label='ddf: '+c)\n",
    "    for f in fields:\n",
    "        axs[-1][0].scatter(-1, -1, marker=shapes[f][c], color='k', label=f+':'+c)\n",
    "        one_c = allmets[(allmets['contaminant'] == c) & (allmets['fieldgzip.open'] == f)]\n",
    "        plotcolors = [linear_colors[c] for c in one_c['percent'].values]\n",
    "    for pair in pairs:\n",
    "        axs[pair[0]][pair[1]-1].scatter(one_c[alldets[pair[0]]], one_c[alldets[pair[1]]],\n",
    "                                   alpha=0.5, s=100,#[sizes[f] for f in one_c['field']], \n",
    "                                   marker=shapes[f][c], \n",
    "                                   color=color_map[plotcolors])\n",
    "# for row in allmets:\n",
    "#     f = row['field']\n",
    "#     c = row['contaminant']\n",
    "# #     axs[-1][0].scatter(-1, -1, marker=all_shapes[f][c], color='k', label=f+':'+c)\n",
    "# #     one_c = allmets[(allmets['contaminant'] == c) & (allmets['field'] == f)]\n",
    "#     for pair in pairs:\n",
    "#         axs[pair[0]][pair[1]-1].scatter(row[alldets[pair[0]]], row[alldets[pair[1]]],\n",
    "#                                    alpha=0.5, s=100,#[sizes[f] for f in one_c['field']], \n",
    "#                                    marker=all_shapes[f][c], \n",
    "#                                    color=plt.get_cmap(fave_cmap)(row['percent']/50.))\n",
    "            \n",
    "for pair in pairs:\n",
    "    axs[pair[0]][pair[1]-1].set_xlabel(alldets[pair[0]])\n",
    "    axs[pair[0]][pair[1]-1].se encountered an overflow error; the KLD is sensitive to the tails, and t_ylabel(alldets[pair[1]])\n",
    "#     axs[pair[0]][pair[1]-1].set_xlim(0., 1.)\n",
    "#     axs[pair[0]][pair[1]-1].set_ylim(0., 1.)\n",
    "axs[-1][0].legend()\n",
    "fig.savefig('draft_dets.png', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate on the grid for the perfect samples as reference\n",
    "\n",
    "The KDEs are the slow step here. . . don't run this more than once\n",
    "\n",
    "TODO: save the KDEs, just in case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ref, grid_ref, kde_ref = {}, {}, {}\n",
    "for field in fields:\n",
    "    d_ref[field], grid_ref[field], kde_ref[field] = {}, {}, {}\n",
    "    for prior in priors:\n",
    "        perfpath = testcases[field][prior]['perfect3000']\n",
    "        [w_ref, Omm_ref] = measure(perfpath, ['w', 'om'])\n",
    "        ref_extrema, ref_grids, ref_vecs, ref_ds = make_grid(w_ref, Omm_ref)\n",
    "        (w_vec, Omm_vec) = ref_vecs\n",
    "        (dw, dOmm) = ref_ds\n",
    "        ((xmin, ymin), (xmax, ymax)) = ref_extrema\n",
    "        (w_grid, Omm_grid) = ref_grids\n",
    "        d_ref[field][prior] = {'w': dw, 'Omm': dOmm}\n",
    "        grid_ref[field][prior] = {'w': w_grid, 'Omm': Omm_grid}\n",
    "        kde_ref[field][prior] = make_kde(w_grid, Omm_grid, w_ref, Omm_ref)\n",
    "#     with open(metpaths[field]+'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: make plot for fiducial, random, perfect as a function of sample size for WFD and DDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "# for i, field in enumerate(['ddf', 'wfd']):\n",
    "#     for conditions in ['perfect', 'random', 'fiducial']:\n",
    "#         postpath = '/media2/RESSPECT2/data/posteriors/'\n",
    "\n",
    "# refpath = postpath+'perfect/chains_perfect.pkl'\n",
    "# comppath = postpath+'fiducial/chains_fiducial.pkl'\n",
    "#         axs[i].imshow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEXT: save cosmology metrics corresponding to `testcases`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: propagate to WFD!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# allmets = pd.read_csv(savedpath+'FOM.csv', )\n",
    "eachprior = []\n",
    "for prior in priors:\n",
    "    allmets = pd.read_csv(savedpath+'FOM.csv')\n",
    "    allmets['prior'] = prior\n",
    "    allmets['outfile'] = None\n",
    "    for field in fields:\n",
    "        for ind in allmets.index:\n",
    "            row = allmets.loc[ind]\n",
    "            if row['name'] in testcases[field][prior].keys():\n",
    "            \n",
    "#     for testcase in testcases\n",
    "#             print(row['name'])\n",
    "                allmets['outfile'].loc[ind] = testcases[field][prior][row['name']]\n",
    "#         else:\n",
    "#             allmets\n",
    "    eachprior += [allmets.copy()]\n",
    "allmets = pd.concat(eachprior, ignore_index=True)\n",
    "# allmets.to_csv(savedpath+'FOM.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: try entropy and KLD of posteriors about origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for field in fields:\n",
    "#     for prior in priors:\n",
    "#         for postname, postpath in testcases[field][prior].items():\n",
    "#             if postpath not in allmets['outfile']:\n",
    "#                 print((field, prior, postname))\n",
    "#                 allmets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "allmets['KLD'] = None\n",
    "# allmets = allmets.drop_duplicates(ignore_index=True)\n",
    "\n",
    "# for row in allmets[allmets['outfile'] is not None]:\n",
    "for ind in allmets[allmets['outfile'].notnull()].index:\n",
    "    compfn = allmets.loc[ind]['outfile']\n",
    "    if os.path.exists(compfn):\n",
    "#     row = allmets.loc[ind]\n",
    "#     if row['outfile'] is not None:\n",
    "# #     row = allmets.loc[ind]\n",
    "# #     print(row)\n",
    "# #     testname = str(100-row['percent'])+str(maybe_sn_classes[sel_class])+str(row['percent'])+row['contaminant']\n",
    "#         testname = row['name']\n",
    "#         print(testname)\n",
    "# #     if row['field'] == 'wfd':\n",
    "# #         comppath = postpath+'WFD/'\n",
    "# #     else:\n",
    "# #         comppath = postpath\n",
    "#     for field in fields:\n",
    "#         for prior in priors:\n",
    "        field = allmets.loc[ind]['field']\n",
    "        prior = allmets.loc[ind]['prior']\n",
    "# #             if testname in [key in testcases[field][prior].keys()]:\n",
    "#                 compfn = testcases[field][prior][testname]#comppath+testname+'/chains_'+testname+'_lowz_withbias.pkl'\n",
    "#                 print(compfn)\n",
    "        [w_comp, Omm_comp] = measure(compfn, ['w', 'om'])\n",
    "        kde_comp = make_kde(grid_ref[field][prior]['w'], grid_ref[field][prior]['Omm'], w_comp, Omm_comp)\n",
    "        allmets['KLD'].loc[ind] = calculate_kld(kde_ref[field][prior], kde_comp, \n",
    "                                                np.array([d_ref[field][prior]['w'], d_ref[field][prior]['Omm']]))\n",
    "#                     allmets['name'].loc[ind] = testname\n",
    "\n",
    "    \n",
    "allmets.to_csv(savedpath+'KLD.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### shortcut\n",
    "\n",
    "1. make lists of all files in `/media/RESSPECT/data/PLAsTiCC/for_metrics/ddf/posteriors/samples_emille` and `/media/RESSPECT/data/PLAsTiCC/for_metrics/wfd/posteriors/samples_emille`\n",
    "2. loop over them calculating KLD relative to `chains_perfect3000_lowz_withbias.csv.gz`\n",
    "3. save in flat files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_pre = '/media/RESSPECT/data/PLAsTiCC/for_metrics/'\n",
    "path_post = '/posteriors/samples_emille/'\n",
    "refname = 'chains_perfect3000_lowz_withbias.csv.gz'\n",
    "\n",
    "for field in ['ddf', 'wfd']:\n",
    "    fullpath = path_pre+field+path_post\n",
    "    alloutputs = pd.DataFrame(columns=['path', 'KLD'])\n",
    "    # make reference sample\n",
    "    with gzip.open(fullpath+refname) as reffn:\n",
    "        flatref = pd.read_csv(reffn)\n",
    "        [w_ref, Omm_ref] = [flatref['w'], flatref['om']]\n",
    "        ref_extrema, ref_grids, ref_vecs, ref_ds = make_grid(w_ref, Omm_ref)\n",
    "        (w_vec, Omm_vec) = ref_vecs\n",
    "        (dw, dOmm) = ref_ds\n",
    "        ((xmin, ymin), (xmax, ymax)) = ref_extrema\n",
    "        (w_grid, Omm_grid) = ref_grids\n",
    "        d_ref = {'w': dw, 'Omm': dOmm}\n",
    "        grid_ref = {'w': w_grid, 'Omm': Omm_grid}\n",
    "        kde_ref = make_kde(w_grid, Omm_grid, w_ref, Omm_ref)\n",
    "    # make comparison samples\n",
    "    allfn = os.scandir(fullpath)\n",
    "    for entry in allfn:\n",
    "        if entry.is_file() and entry.name[-4:] != '.csv':\n",
    "            samppath = fullpath+entry.name\n",
    "            with gzip.open(samppath) as sampfile:\n",
    "                sampdata = pd.read_csv(sampfile)\n",
    "                [w_comp, Omm_comp] = [sampdata['w'], sampdata['om']]\n",
    "                kde_comp = make_kde(grid_ref['w'], grid_ref['Omm'], w_comp, Omm_comp)\n",
    "                the_kld = calculate_kld(kde_ref, kde_comp, np.array([d_ref['w'], d_ref['Omm']]))\n",
    "                newrow = {'path': samppath, 'KLD': the_kld}\n",
    "#                 print((newrow, type(newrow['KLD'])))\n",
    "                alloutputs = alloutputs.append(newrow, ignore_index=True)\n",
    "    alloutputs.to_csv(fullpath+'klds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thing = pd.read_csv(path_pre+'ddf'+path_post+'klds.csv')\n",
    "for p in thing[thing['KLD'].isnull()]['path']:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run me only once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: also calculate fisher matrix metric below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allmets = pd.read_csv(savedpath+'KLD.csv')\n",
    "fitmets = ['name', 'wfit_w_lowz', 'wfit_wsig_lowz', 'wfit_om_lowz', 'wfit_omsig_lowz', \n",
    "            'stan_w_lowz', 'stan_wsig_lowz', 'stan_om_lowz', 'stan_omsig_lowz']\n",
    "\n",
    "for field in fields:\n",
    "    if field == 'wfd':\n",
    "        comppath = postpath+'WFD/'\n",
    "    else:\n",
    "        comppath = postpath\n",
    "    moremets = pd.read_csv(comppath+'summary_cases.csv')\n",
    "    moremets = moremets.rename(columns={\"case\": \"name\"})[fitmets]\n",
    "    moremets['field'] = field\n",
    "    allmets = pd.merge(allmets, moremets, how=\"outer\", on=['field', 'name'])\n",
    "\n",
    "allmets.to_csv(savedpath+'FIT.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allmets = pd.read_csv(savedpath+'FIT.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plotting cosmo/principled vs. traditional/deterministic metrics\n",
    "\n",
    "~~TODO: actually make these~~\n",
    "~~- [X] by contaminant (markershape)~~\n",
    "~~- [X] by contamination rate (continuous colors? or markersize?)~~\n",
    "~~- [X] by field (markersize? or discrete colors?)~~\n",
    "\n",
    "TODO:\n",
    "- [X] hardcode the shapes\n",
    "- [X] open/closed for field\n",
    "- [ ] logscale colors\n",
    "- [X] check WFD directory for posteriors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: add in 1D histograms for metric value for each contaminant, for percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allmets = pd.read_csv(savedpath+'KLD.csv')\n",
    "\n",
    "dets_to_plot = ['f1']\n",
    "dim = len(dets_to_plot)\n",
    "\n",
    "fig, axs = plt.subplots(1, dim+1, figsize=(5*(dim+1), 5))#,\n",
    "#                         gridspec_kw={'width_ratios': [10]*dim+[1]})\n",
    "# norm = mpl.colors.Normalize(vmin=0., vmax=50.)\n",
    "# fig.colorbar(mpl.cm.ScalarMappable(cmap=plt.get_cmap(fave_cmap), norm=norm), cax=axs[-1], \n",
    "#              ticks=allmets['percent'].unique())\n",
    "fig.colorbar(scalarMap, cax=axs[-1], ticks=range(len(uq)))#, ticklabels=uq)\n",
    "\n",
    "for i, metric in enumerate(dets_to_plot):\n",
    "    for field in fields:\n",
    "        for contaminant in contaminants.values():\n",
    "            plotmask = allmets[(allmets['field'] == field) & (allmets['contaminant'] == contaminant)]\n",
    "            plotcolors = [linear_colors[c] for c in plotmask['percent']]\n",
    "            axs[i].scatter(plotmask[metric], plotmask['KLD'], alpha=0.75, s=100,\n",
    "                           marker=shapes[f][contaminant], \n",
    "                           color=color_map[plotcolors])\n",
    "    axs[i].semilogy()\n",
    "    axs[i].set_xlabel(metric)\n",
    "    axs[i].set_ylabel('KLD')\n",
    "    axs[i].set_xlim(0.74, 1.01)\n",
    "for field in fields:\n",
    "#         axs[i].scatter(-1, -1, s=100, marker='o', color='k', \n",
    "#                        label=field)\n",
    "    for contaminant in contaminants.values():\n",
    "        axs[-1].scatter(-1, -1, s=50, marker=shapes[field][contaminant], color='k', \n",
    "                       label=f+': '+contaminant)\n",
    "axs[-1].legend(loc='lower left', fontsize='small')\n",
    "fig.savefig('draft_kld.png', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: thought re: histograms of w\\_est, sigma\\_w, make two versions: color by which contaminant (discrete colors) and by % contamninant (continuous colors)\n",
    "\n",
    "these live in postpath = '/media2/RESSPECT2/data/posteriors/' stan\\_summary .dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('/media2/RESSPECT2/data/posteriors/summary_cases.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: plot posteriors for most extreme subsamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
