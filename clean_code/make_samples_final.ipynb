{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate SNANA types\n",
    "types_names = {90:'Ia', 67: '91bg', 52:'Iax', 42:'II', 62:'Ibc', \n",
    "               95: 'SLSN', 15:'TDE', 64:'KN', 88:'AGN', 92:'RRL', 65:'M-dwarf',\n",
    "               16:'EB',53:'Mira', 6:'MicroL', 991:'MicroLB', 992:'ILOT', \n",
    "               993:'CART', 994:'PISN',995:'MLString'}\n",
    "\n",
    "SNANA_types = {90:11, 62:{1:3, 2:13}, 42:{1:2, 2:12, 3:14},\n",
    "               67:41, 52:43, 64:51, 95:60, 994:61, 992:62,\n",
    "               993:63, 15:64, 88:70, 92:80, 65:81, 16:83,\n",
    "               53:84, 991:90, 6:{1:91, 2:93}}\n",
    "\n",
    "SNANA_names = {11: 'Ia', 3:'Ibc', 13: 'Ibc', 2:'II', 12:'II', 14:'II',\n",
    "               41: '91bg', 43:'Iax', 51:'KN', 60:'SLSN', 61:'PISN', 62:'ILOT',\n",
    "               63:'CART', 64:'TDE', 70:'AGN', 80:'RRL', 81:'M-dwarf', 83:'EB',\n",
    "               84:'Mira', 90:'MicroLB', 91:'MicroL', 93:'MicroL'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_root = '/media2/RESSPECT2/clean_output/'\n",
    "\n",
    "for field in ['DDF', 'WFD']:\n",
    "    for version in range(6):\n",
    "\n",
    "        # create directory structure\n",
    "        dir_list = [output_root + field + '/',\n",
    "            output_root + field + '/v' + str(version) + '/',\n",
    "            output_root + field + '/v' + str(version) + '/cospar/',\n",
    "            output_root + field + '/v' + str(version) + '/fitres/', \n",
    "            output_root + field + '/v' + str(version) + '/M0DIF/',\n",
    "            output_root + field + '/v' + str(version) + '/posteriors/',\n",
    "            output_root + field + '/v' + str(version) + '/posteriors/csv/',\n",
    "            output_root + field + '/v' + str(version) + '/posteriors/pkl',\n",
    "            output_root + field + '/v' + str(version) + '/samples/',\n",
    "            output_root + field + '/v' + str(version) + '/stan_input/',\n",
    "            output_root + field + '/v' + str(version) + '/stan_summary/',\n",
    "           ]\n",
    "\n",
    "        for name in dir_list:\n",
    "            if not os.path.isdir(name):\n",
    "                os.makedirs(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read zenodo metadata\n",
    "fname = '/media/RESSPECT/data/PLAsTiCC/PLAsTiCC_zenodo/plasticc_test_metadata.csv'\n",
    "test_metadata = pd.read_csv(fname)\n",
    "\n",
    "# separate fields\n",
    "ddf_flag = test_metadata['ddf_bool'].values == 1\n",
    "ids_ddf = test_metadata['object_id'].values[ddf_flag]\n",
    "ids_wfd = test_metadata['object_id'].values[~ddf_flag]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create perfect samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For DDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all Ias in DDF\n",
    "salt2_Ia_DDF = pd.read_csv('../SALT2_fit/Ia/results/master_fitres.fitres', comment='#', delim_whitespace=True)\n",
    "salt2_Ia_DDF['zHD'] = salt2_Ia_DDF['SIM_ZCMB']          # requirement of so SALT2mu can work\n",
    "\n",
    "# choose sample size\n",
    "nobjs = 6000\n",
    "\n",
    "# choose number of versions of the same sample to generate\n",
    "v = 0\n",
    "\n",
    "for i in range(1, v):\n",
    "    perfect_Ia_DDF = salt2_Ia_DDF.sample(n=nobjs, replace=False)\n",
    "    #perfect_Ia_DDF.to_csv(data_dir + 'DDF/v' + str(i) +  '/samples/perfect' + \\\n",
    "    #                      str(nobjs) + '.csv', sep=' ', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For WFD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File Ia/results/master_fitres_77.fitres does not exist: 'Ia/results/master_fitres_77.fitres'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-5667ba90ace2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfnames_Ia\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     fitres_temp = pd.read_csv('Ia/results/' + name, delim_whitespace=True, \n\u001b[0;32m---> 13\u001b[0;31m                               comment='#')\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mfitres_temp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'zHD'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfitres_temp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SIM_ZCMB'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0msalt2_WFD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfitres_temp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/emille/git/COIN/RESSPECT_repo/venv/resspect_main/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/emille/git/COIN/RESSPECT_repo/venv/resspect_main/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/emille/git/COIN/RESSPECT_repo/venv/resspect_main/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/emille/git/COIN/RESSPECT_repo/venv/resspect_main/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/emille/git/COIN/RESSPECT_repo/venv/resspect_main/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File Ia/results/master_fitres_77.fitres does not exist: 'Ia/results/master_fitres_77.fitres'"
     ]
    }
   ],
   "source": [
    "# read all Ias in WFD\n",
    "fnames_Ia = os.listdir('../SALT2_fit/Ia/results/')\n",
    "\n",
    "# remove unecessary files and folders\n",
    "fnames_Ia.remove('master_fitres.fitres')\n",
    "fnames_Ia.remove('salt3')\n",
    "fnames_Ia.remove('.ipynb_checkpoints')\n",
    "\n",
    "salt2_WFD = []\n",
    "\n",
    "for name in fnames_Ia:\n",
    "    fitres_temp = pd.read_csv('../SALT2_fit/Ia/results/' + name, delim_whitespace=True, \n",
    "                              comment='#')\n",
    "    fitres_temp['zHD'] = fitres_temp['SIM_ZCMB']\n",
    "    salt2_WFD.append(fitres_temp)\n",
    "\n",
    "salt2_Ia_WFD = pd.concat(salt2_WFD, ignore_index=True)\n",
    "\n",
    "# choose sample size\n",
    "nobjs = 6000\n",
    "\n",
    "# choose number of versions of the same sample to generate\n",
    "v = 6\n",
    "\n",
    "for i in range(v):\n",
    "    perfect_Ia_WFD = salt2_Ia_WFD.sample(n=nobjs, replace=False)\n",
    "    #perfect_Ia_WFD.to_csv(data_dir + 'WFD/v' + str(i) + '/samples/perfect' + \\\n",
    "    #                      str(nobjs) + '.csv', sep=' ', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Random samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For DDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of classes surviving SALT2 fit\n",
    "surv_class_DDF = ['91bg', 'AGN', 'CART', 'Ia', 'Iax', 'Ibc', 'II', 'TDE']\n",
    "\n",
    "# read all SALT2 fit results for DDF\n",
    "all_DDF = []\n",
    "for obj_type in surv_class_DDF:\n",
    "    data_temp = pd.read_csv(obj_type + '/results/master_fitres.fitres', comment='#', delim_whitespace=True)\n",
    "    data_temp['zHD'] = data_temp['SIM_ZCMB']\n",
    "    data_temp.fillna(-99, inplace=True)\n",
    "    all_DDF.append(data_temp)\n",
    "    \n",
    "all_surv_DDF = pd.concat(all_DDF, ignore_index=True)\n",
    "all_surv_DDF.fillna(-99, inplace=True)\n",
    "\n",
    "# choose sample size\n",
    "nobjs = 6000\n",
    "\n",
    "# choose number of versions of the same sample to generate\n",
    "v = 2\n",
    "\n",
    "for i in range(1, v):\n",
    "    random_DDF = all_surv_DDF.sample(n=nobjs, replace=False)\n",
    "    #random_DDF.to_csv(data_dir + 'DDF/v' + str(i) + '/samples/random' + \\\n",
    "    #                  str(nobjs) + '.csv', sep=' ', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For WFD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of classes surviving SALT2 fit\n",
    "surv_class_WFD = ['91bg', 'AGN', 'CART', 'Ia', 'Iax', 'Ibc', 'II', 'TDE', 'ILOT', 'PISN', 'SLSN']\n",
    "\n",
    "# read all SALT2 fit results for WFD\n",
    "all_WFD = []\n",
    "for obj_type in surv_class_WFD:\n",
    "    flist = os.listdir(obj_type + '/results/')\n",
    "    flist.remove('salt3')                  # remove directory of temporary SALT files\n",
    "    flist.remove('master_fitres.fitres')   # remove DDF file\n",
    "    if '.ipynb_checkpoints' in flist:\n",
    "        flist.remove('.ipynb_checkpoints')\n",
    "    \n",
    "    for name in flist:\n",
    "        data_temp = pd.read_csv(obj_type + '/results/' + name, comment='#', delim_whitespace=True)\n",
    "        data_temp['zHD'] = data_temp['SIM_ZCMB']\n",
    "        data_temp.fillna(-99, inplace=True)\n",
    "        all_WFD.append(data_temp)\n",
    "        \n",
    "    \n",
    "all_surv_WFD = pd.concat(all_WFD, ignore_index=True)\n",
    "all_surv_WFD.fillna(-99, inplace=True)\n",
    "\n",
    "# choose sample size\n",
    "nobjs = 6000\n",
    "\n",
    "# choose number of versions of the same sample to generate\n",
    "v = 7\n",
    "\n",
    "for i in range(6, v):\n",
    "    random_WFD = all_surv_WFD.sample(n=nobjs, replace=False)\n",
    "    #random_WFD.to_csv(data_dir + 'WFD/v' + str(i) + '/samples/perfect' + \\\n",
    "    #                  str(nobjs) + '.csv', sep=' ', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Fiducial samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For DDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read results from avocado\n",
    "fname_DDF = data_dir + 'DDF/avocado/avocado_DDF.csv'\n",
    "avocado_DDF = pd.read_csv(fname_DDF, names=['object_id','6','15','16','42','52','53','62','64','65','67','88',\n",
    "                                           '90','92','95'], skiprows=1)\n",
    "\n",
    "# determine final classification\n",
    "class_final_DDF = []\n",
    "for i in range(avocado_DDF.shape[0]):\n",
    "    indx = np.argsort(avocado_DDF.iloc[i].values[1:])[-1]\n",
    "    code = int(avocado_DDF.keys()[indx + 1])\n",
    "    class_final_DDF.append(types_names[code])\n",
    "class_final_DDF = np.array(class_final_DDF)\n",
    "\n",
    "# get photometrically classified Ia\n",
    "flag_class_Ia_DDF = class_final_DDF == 'Ia'\n",
    "avocado_DDF_Ia = avocado_DDF[flag_class_Ia_DDF]\n",
    "\n",
    "# get SALT2 fit for objs photometrically classified as Ia\n",
    "avocado_DDF_Ia_fitres_flag = np.array([item in avocado_DDF_Ia['object_id'].values for item in all_surv_DDF['CID'].values])\n",
    "all_avocado_DDF_Ia = all_surv_DDF[avocado_DDF_Ia_fitres_flag]\n",
    "\n",
    "# choose sample size\n",
    "nobjs = 6000\n",
    "\n",
    "# choose number of versions of the same sample to generate\n",
    "v = 2\n",
    "\n",
    "for i in range(1, v):\n",
    "    fiducial_DDF = all_avocado_DDF_Ia.sample(n=nobjs, replace=False)\n",
    "    #fiducial_DDF.to_csv(data_dir + 'DDF/v' + str(i) + '/samples/fiducial' + \\\n",
    "    #                  str(nobjs) + '.csv', sep=' ', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For WFD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read results from avocado\n",
    "fname_WFD =  data_dir + 'WFD/avocado/avocado_WFD.csv'\n",
    "avocado_WFD = pd.read_csv(fname_WFD, names=['object_id','6','15','16','42','52','53','62','64','65','67','88',\n",
    "                                           '90','92','95'], skiprows=1)\n",
    "\n",
    "# determine final classification\n",
    "class_final_WFD = []\n",
    "for i in range(avocado_WFD.shape[0]):\n",
    "    indx = np.argsort(avocado_WFD.iloc[i].values[1:])[-1]\n",
    "    code = int(avocado_WFD.keys()[indx + 1])\n",
    "    class_final_WFD.append(types_names[code])\n",
    "    \n",
    "# get photometrically classified Ia\n",
    "class_final_WFD = np.array(class_final_WFD)\n",
    "flag_class_Ia_WFD = class_final_WFD == 'Ia'\n",
    "avocado_WFD_Ia = avocado_WFD[flag_class_Ia_WFD]\n",
    "\n",
    "# get SALT2 fit for objs photometrically classified as Ia\n",
    "avocado_WFD_Ia_fitres_flag = np.array([item in avocado_WFD_Ia['object_id'].values \n",
    "                                       for item in all_surv_WFD['CID'].values])\n",
    "all_avocado_WFD_Ia = all_surv_WFD[avocado_WFD_Ia_fitres_flag]\n",
    "\n",
    "# choose sample size\n",
    "nobjs = 3000\n",
    "\n",
    "# choose number of versions of the same sample to generate\n",
    "v = 7\n",
    "\n",
    "for i in range(6, v):\n",
    "    fiducial_WFD = all_avocado_WFD_Ia.sample(n=nobjs, replace=False)\n",
    "    #fiducial_WFD.to_csv(data_dir + 'WFD/v' + str(i) + '/samples/perfect' + \\\n",
    "    #                  str(nobjs) + '.csv', sep=' ', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create single contaminant samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For DDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# levels of contamination\n",
    "cont_DDF = {'II': [0.28, 0.25, 0.1, 0.05, 0.02, 0.01],\n",
    "            'Ibc': [0.05, 0.02, 0.01],\n",
    "            'Iax': [0.14, 0.1, 0.05, 0.02, 0.01],\n",
    "            'CART': [0.009], \n",
    "            '91bg': [0.002],\n",
    "            'AGN': [0.001]}\n",
    "\n",
    "complete_names ={'II': 'SNII', 'Ibc': 'SNIbc', 'Iax': 'SNIax', 'CART':'CART',\n",
    "                 '91bg':'SNIa-91bg', 'AGN':'AGN'}\n",
    "\n",
    "# choose sample size\n",
    "nobjs = 3000\n",
    "\n",
    "# choose number of versions of the same sample to generate\n",
    "v = 6\n",
    "\n",
    "for i in range(1, v):\n",
    "    for obj_class in list(cont_DDF.keys()):\n",
    "        # read all contaminants surviving SALT2 fit\n",
    "        sample_cont = pd.read_csv(obj_class + '/results/master_fitres.fitres', comment='#',\n",
    "                              delim_whitespace=True)\n",
    "        sample_cont['zHD'] = sample_cont['SIM_ZCMB']\n",
    "    \n",
    "        for perc in cont_DDF[obj_class]:\n",
    "            Ia_temp = salt2_Ia_DDF.sample(n=int((1 - perc) * nobjs), replace=False)\n",
    "            cont_temp = sample_cont.sample(n=int(perc * nobjs), replace = False)\n",
    "            sample_final = pd.concat([Ia_temp, cont_temp], ignore_index=True)\n",
    "            sample_final.fillna(-99, inplace=True)\n",
    "        \n",
    "            if obj_class not in ['CART', '91bg', 'AGN']:\n",
    "                sample_final.to_csv(data_dir + 'DDF/v' + str(i) + '/samples/' + str(int(100 - 100 * perc)) + \\\n",
    "                                    'SNIa' + str(int(100 * perc)) + complete_names[obj_class] + '.csv', \n",
    "                                     sep=' ', index=False)\n",
    "            else:\n",
    "                sample_final.to_csv(data_dir + 'DDF/v' + str(i) + '/samples/' + str(round(100 - 100 * perc, 1)) + \\\n",
    "                                    'SNIa' + str(round(100 * perc, 1)) + complete_names[obj_class] + '.csv', \n",
    "                                     sep=' ', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For WFD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# levels of contamination\n",
    "cont_WFD = {'II': [0.28, 0.25, 0.1, 0.05, 0.02, 0.01],\n",
    "            'Ibc': [0.1, 0.05, 0.02, 0.01],\n",
    "            'Iax': [0.25, 0.1, 0.05, 0.02, 0.01],\n",
    "            '91bg': [0.05, 0.02, 0.01],\n",
    "            'AGN': [0.05, 0.02, 0.01],\n",
    "            'TDE': [0.004],\n",
    "            'CART': [0.003]}\n",
    "\n",
    "complete_names ={'II': 'SNII', 'Ibc': 'SNIbc', 'Iax': 'SNIax', 'CART':'CART',\n",
    "                 '91bg':'SNIa-91bg', 'AGN':'AGN', 'TDE':'TDE'}\n",
    "\n",
    "# choose sample size\n",
    "nobjs = 3000\n",
    "\n",
    "# choose number of versions of the same sample to generate\n",
    "v = 7\n",
    "\n",
    "for i in range(6, v):\n",
    "    for obj_class in list(cont_WFD.keys()):\n",
    "        # read all contaminants surviving SALT2 fit\n",
    "        flist = os.listdir(obj_class + '/results/')\n",
    "        flist.remove('salt3')\n",
    "        flist.remove('master_fitres.fitres')\n",
    "        if '.ipynb_checkpoints' in flist:\n",
    "            flist.remove('.ipynb_checkpoints')\n",
    "        \n",
    "        sample_cont = []\n",
    "        for name in flist:\n",
    "            temp_cont = pd.read_csv(obj_class + '/results/' + name, comment='#',\n",
    "                                  delim_whitespace=True)\n",
    "            temp_cont['zHD'] = temp_cont['SIM_ZCMB']\n",
    "            sample_cont.append(temp_cont)\n",
    "            \n",
    "        sample_cont2 = pd.concat(sample_cont, ignore_index=True)\n",
    "    \n",
    "        for perc in cont_WFD[obj_class]:\n",
    "            Ia_temp2 = salt2_Ia_WFD.sample(n=int((1-perc)*ntot), replace=False)\n",
    "            cont_temp2 = sample_cont2.sample(n=int(perc*ntot), replace = False)\n",
    "            sample_final = pd.concat([Ia_temp2, cont_temp2], ignore_index=True)\n",
    "            sample_final.fillna(-99, inplace=True)\n",
    "        \n",
    "            #sample_final.to_csv(data_dir + 'WFD/v' + str(i) + '/samples/' + str(int(100 - 100 * perc)) + \\\n",
    "            #              'SNIa' + str(int(100 * perc)) + complete_names[obj_class] + '.csv', \n",
    "            #              sep=' ', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
